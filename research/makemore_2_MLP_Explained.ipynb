{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrej Karpathy's /makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://www.youtube.com/watch?v=TCH_1BHY58I\n",
    "\n",
    "https://github.com/karpathy/makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bigram char level model, we only used two chars which created a 27x27 data space. if we move deeper in this approach to enhance the loss function and the model itself, the only avenue was to explor adding more dimensions, i.e. 27x27x27. however this path suddenly explodes in terms of data and parameters that we want to use for this model.\n",
    "\n",
    "Therefore, we need to explore a better model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)\n",
    "#### Bengio et al. 2003\n",
    "\n",
    "This is another char level model to predict the next char, however the paper is based on word predictions. \n",
    "\n",
    "The proposed approach is to take 'w' number of words, and associate to each word, 'm' number of feature vectors. Meaning that, each word is embedded in a 'm' dimensional feature space. Initially these words are initialized randomly but later we'll tune them using backpropagation. \n",
    "\n",
    "To imagine this approach, think about words that are similar or synonyms. They will end up in the same part of the space. And those that are different will be separated. \n",
    "\n",
    "The modeling approach is similar to the NN approach for Bigram. They use multi-layer NN to predict the next words, given the previous words. To train the NN, they ```maximize the log-likelihood of the training data```.\n",
    "\n",
    "Let's look at an ```example``` for this approach. Assume, we are not given the sentence \"A dog was running in a room\". But now for testing the model we are providing it with \"A dog was running in a ...\" and expecting the model to fill in the blank. Since it hasn't seen this exact sentence, we call it, ```out of distribution```. However, MLP doesn't need to have seen the exact words to predict 'room' for the blank. Because it might have seen \"The dog was running in a room\" and based on the learnings, it has put the embeddings of 'The' and 'A' near by each other in the space. So now that we are asking it to fill the blank based on \"A dog was running in a ...\", it will match it up with \"The dog was running in a room\". This is called ```knowledge transfer```.\n",
    "\n",
    "Let's look at the ```architecture``` of this approach. \n",
    "\n",
    "Assume the NN's input, takes 3 previous-words. And the output is the fourth word. Each of the incoming words, will go through a look-up table, to match up the corresponding embedding ('m' feature vector) for that word. So there will be $3 \\times m$ neurons holding the 3 words. \n",
    "\n",
    "Then we need to build a hidden layer. The size is a ```hyper-parameter```. Meaning that, we need to come up with the right size based on try-error. So all the input neurons goes into the hidden layer. And there will be a ```tanh``` function applied for non-linearity. \n",
    "\n",
    "The output layer is a huge one, because the number of neurons is equivalent to $w$, the number of words in our data set. All the neurons in the hidden layer are connected to the output neurons. That's why there will be lots of params in between these two layers, and therefore, it's going to be computationally expensive. On top of the output layer we have ```softmax``` (exponentiate the logits and normalize, so that it will sum up to 1). This way, we'll get a nice probability distribution for the next word in the sequence. \n",
    "\n",
    "During training, because we have xs and ys, we will get the probability for each x and minimize the NN's loss by improving the parameters. The optimization used here is also ```backpropagation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# read from another package while we are in a separate package\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "file_path = os.path.join(parent_directory, 'opensource/makemore', 'names.txt')\n",
    "\n",
    "words = open(file_path, 'r').read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of chars and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s2i = {s:i+1 for i,s in enumerate(chars)}\n",
    "s2i['.'] = 0\n",
    "i2s = {i:s for s,i in s2i.items()}\n",
    "print(i2s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words[:5]: # the examples we can generate from the first 5 words\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(i2s[i] for i in context), '--->', i2s[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Embeddings\n",
    "\n",
    "the paper used 70000 words with 30 embeddings. we have 27 chars, so we'll go with 2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5356, -1.1602])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5356, -1.1602],\n",
       "        [-1.9636, -1.2327],\n",
       "        [-0.3581,  0.4690]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve embeddings with a list of lookups\n",
    "C[[5,6,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-1.5356, -1.1602]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-1.5356, -1.1602],\n",
       "         [-0.4779,  0.7886]],\n",
       "\n",
       "        [[-1.5356, -1.1602],\n",
       "         [-0.4779,  0.7886],\n",
       "         [-0.4779,  0.7886]],\n",
       "\n",
       "        [[-0.4779,  0.7886],\n",
       "         [-0.4779,  0.7886],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.8766, -0.7644]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.8766, -0.7644],\n",
       "         [-0.3467,  1.6859]],\n",
       "\n",
       "        [[-0.8766, -0.7644],\n",
       "         [-0.3467,  1.6859],\n",
       "         [ 0.3724, -0.7340]],\n",
       "\n",
       "        [[-0.3467,  1.6859],\n",
       "         [ 0.3724, -0.7340],\n",
       "         [-0.1864,  0.4035]],\n",
       "\n",
       "        [[ 0.3724, -0.7340],\n",
       "         [-0.1864,  0.4035],\n",
       "         [ 0.3724, -0.7340]],\n",
       "\n",
       "        [[-0.1864,  0.4035],\n",
       "         [ 0.3724, -0.7340],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [ 3.1316, -0.3591],\n",
       "         [-0.1864,  0.4035]],\n",
       "\n",
       "        [[ 3.1316, -0.3591],\n",
       "         [-0.1864,  0.4035],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [ 0.3724, -0.7340]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [ 0.3724, -0.7340],\n",
       "         [ 0.0934, -0.2708]],\n",
       "\n",
       "        [[ 0.3724, -0.7340],\n",
       "         [ 0.0934, -0.2708],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[ 0.0934, -0.2708],\n",
       "         [ 3.1316, -0.3591],\n",
       "         [ 0.1287,  2.0604]],\n",
       "\n",
       "        [[ 3.1316, -0.3591],\n",
       "         [ 0.1287,  2.0604],\n",
       "         [-1.5356, -1.1602]],\n",
       "\n",
       "        [[ 0.1287,  2.0604],\n",
       "         [-1.5356, -1.1602],\n",
       "         [-0.3467,  1.6859]],\n",
       "\n",
       "        [[-1.5356, -1.1602],\n",
       "         [-0.3467,  1.6859],\n",
       "         [-0.3467,  1.6859]],\n",
       "\n",
       "        [[-0.3467,  1.6859],\n",
       "         [-0.3467,  1.6859],\n",
       "         [ 3.1316, -0.3591]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [-0.7441, -1.3213],\n",
       "         [ 0.0934, -0.2708]],\n",
       "\n",
       "        [[-0.7441, -1.3213],\n",
       "         [ 0.0934, -0.2708],\n",
       "         [-0.8766, -0.7644]],\n",
       "\n",
       "        [[ 0.0934, -0.2708],\n",
       "         [-0.8766, -0.7644],\n",
       "         [ 1.1004, -0.2633]],\n",
       "\n",
       "        [[-0.8766, -0.7644],\n",
       "         [ 1.1004, -0.2633],\n",
       "         [-0.3581,  0.4690]],\n",
       "\n",
       "        [[ 1.1004, -0.2633],\n",
       "         [-0.3581,  0.4690],\n",
       "         [ 0.3724, -0.7340]],\n",
       "\n",
       "        [[-0.3581,  0.4690],\n",
       "         [ 0.3724, -0.7340],\n",
       "         [ 3.1316, -0.3591]]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# therefore this works\n",
    "emb = C[X]\n",
    "emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_hyperparameter_size = 100\n",
    "num_of_words = 3\n",
    "num_of_embeddings = 2\n",
    "num_of_inputs = num_of_words * num_of_embeddings\n",
    "\n",
    "w1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size))\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wee want to setup the tensor's shapes in such a way that ```emb @ w1 + b1``` would work.\n",
    "\n",
    "http://blog.ezyang.com/2019/05/pytorch-internals/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063, -0.1956,  1.0527,  ...,  3.4857, -2.3596, -0.0303],\n",
       "        [-1.6774, -0.0384,  0.9194,  ...,  4.7999, -1.2727,  0.3032],\n",
       "        [-2.6512, -0.2633,  1.5187,  ...,  4.3099, -1.5200,  0.9739],\n",
       "        ...,\n",
       "        [ 0.3280,  1.0885,  2.9605,  ..., -0.9545, -0.8164,  0.6726],\n",
       "        [ 2.0355, -0.5030, -0.1003,  ...,  0.7610, -2.7609, -2.6052],\n",
       "        [ 1.9825, -0.9363,  2.6258,  ..., -7.4045, -5.4526, -1.3877]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_size = emb.shape[0] # or use -1 for pytorch to figure it out\n",
    "emb.view(x_size, num_of_inputs) @ w1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8027, -0.1932,  0.7828,  ...,  0.9981, -0.9823, -0.0303],\n",
       "        [-0.9325, -0.0384,  0.7256,  ...,  0.9999, -0.8545,  0.2942],\n",
       "        [-0.9901, -0.2573,  0.9085,  ...,  0.9996, -0.9087,  0.7504],\n",
       "        ...,\n",
       "        [ 0.3167,  0.7963,  0.9946,  ..., -0.7418, -0.6731,  0.5867],\n",
       "        [ 0.9664, -0.4645, -0.0999,  ...,  0.6417, -0.9920, -0.9891],\n",
       "        [ 0.9628, -0.7335,  0.9896,  ..., -1.0000, -1.0000, -0.8827]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ w1 + b1) # added tanh to bring all the values between -1 and 1 for non-linearity\n",
    "h "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output layer\n",
    "w2 = torch.randn((hidden_layer_hyperparameter_size, 27))\n",
    "b2 = torch.randn((27))\n",
    "logits = h @ w2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # get fake counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize to get the probabilities\n",
    "probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proof of normalized probs is to check if every row sums up to =1\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0731e-06, 2.2220e-02, 4.7492e-13, 4.4023e-05, 1.0357e-08, 2.3559e-06,\n",
       "        8.3836e-01, 5.9869e-13, 4.3647e-05, 5.8108e-06, 3.7307e-08, 8.4064e-07,\n",
       "        8.9202e-08, 7.5333e-10, 2.5995e-08, 5.3484e-06, 3.4731e-02, 6.3238e-07,\n",
       "        8.3018e-09, 2.3675e-09, 5.5572e-07, 4.0001e-12, 1.1031e-09, 3.7042e-02,\n",
       "        1.3796e-11, 6.1066e-05, 1.4449e-08, 6.8229e-05, 2.3223e-07, 1.3255e-02,\n",
       "        3.0096e-09, 4.0769e-07])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Small Dataset (over-fitting)\n",
    "\n",
    "we haven't trained the NN yet so the probabilities are far from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7486)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to minimize this loss\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp() # (32, 27)\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of using the above code we can also use pytorch library to arrive at the same loss, which is more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's setup the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  17.769712448120117\n",
      "loss ->  13.656402587890625\n",
      "loss ->  11.298768997192383\n",
      "loss ->  9.4524564743042\n",
      "loss ->  7.984262943267822\n",
      "loss ->  6.891320705413818\n",
      "loss ->  6.100014686584473\n",
      "loss ->  5.452036380767822\n",
      "loss ->  4.898151874542236\n",
      "loss ->  4.414663791656494\n",
      "loss ->  3.985849380493164\n",
      "loss ->  3.6028308868408203\n",
      "loss ->  3.2621421813964844\n",
      "loss ->  2.961381435394287\n",
      "loss ->  2.6982975006103516\n",
      "loss ->  2.469712734222412\n",
      "loss ->  2.271660566329956\n",
      "loss ->  2.101283550262451\n",
      "loss ->  1.9571772813796997\n",
      "loss ->  1.8374857902526855\n",
      "loss ->  1.7380967140197754\n",
      "loss ->  1.653511643409729\n",
      "loss ->  1.5790899991989136\n",
      "loss ->  1.5117664337158203\n",
      "loss ->  1.449604868888855\n",
      "loss ->  1.3913118839263916\n",
      "loss ->  1.3359922170639038\n",
      "loss ->  1.2830528020858765\n",
      "loss ->  1.2321910858154297\n",
      "loss ->  1.18338143825531\n",
      "loss ->  1.136798620223999\n",
      "loss ->  1.092664122581482\n",
      "loss ->  1.051092267036438\n",
      "loss ->  1.0120269060134888\n",
      "loss ->  0.9752703309059143\n",
      "loss ->  0.940556526184082\n",
      "loss ->  0.9076125621795654\n",
      "loss ->  0.8761920928955078\n",
      "loss ->  0.8460890650749207\n",
      "loss ->  0.8171356916427612\n",
      "loss ->  0.7891989946365356\n",
      "loss ->  0.7621745467185974\n",
      "loss ->  0.7359813451766968\n",
      "loss ->  0.7105579972267151\n",
      "loss ->  0.6858609914779663\n",
      "loss ->  0.6618651747703552\n",
      "loss ->  0.6385655403137207\n",
      "loss ->  0.6159816980361938\n",
      "loss ->  0.5941658616065979\n",
      "loss ->  0.5732104182243347\n",
      "loss ->  0.5532563924789429\n",
      "loss ->  0.5344882011413574\n",
      "loss ->  0.5171165466308594\n",
      "loss ->  0.5013313293457031\n",
      "loss ->  0.48724260926246643\n",
      "loss ->  0.4748404622077942\n",
      "loss ->  0.4639977812767029\n",
      "loss ->  0.45451444387435913\n",
      "loss ->  0.44617098569869995\n",
      "loss ->  0.4387663006782532\n",
      "loss ->  0.4321332573890686\n",
      "loss ->  0.42613884806632996\n",
      "loss ->  0.42067989706993103\n",
      "loss ->  0.4156752824783325\n",
      "loss ->  0.4110615849494934\n",
      "loss ->  0.4067871868610382\n",
      "loss ->  0.402810662984848\n",
      "loss ->  0.3990972936153412\n",
      "loss ->  0.3956180214881897\n",
      "loss ->  0.39234787225723267\n",
      "loss ->  0.3892652988433838\n",
      "loss ->  0.38635194301605225\n",
      "loss ->  0.38359174132347107\n",
      "loss ->  0.38096994161605835\n",
      "loss ->  0.37847423553466797\n",
      "loss ->  0.37609291076660156\n",
      "loss ->  0.3738164007663727\n",
      "loss ->  0.3716350197792053\n",
      "loss ->  0.3695409893989563\n",
      "loss ->  0.3675268888473511\n",
      "loss ->  0.3655855357646942\n",
      "loss ->  0.3637113869190216\n",
      "loss ->  0.3618983030319214\n",
      "loss ->  0.36014166474342346\n",
      "loss ->  0.3584362864494324\n",
      "loss ->  0.3567781150341034\n",
      "loss ->  0.3551627993583679\n",
      "loss ->  0.35358694195747375\n",
      "loss ->  0.35204702615737915\n",
      "loss ->  0.3505397439002991\n",
      "loss ->  0.3490622937679291\n",
      "loss ->  0.3476122319698334\n",
      "loss ->  0.34618666768074036\n",
      "loss ->  0.34478360414505005\n",
      "loss ->  0.3434009850025177\n",
      "loss ->  0.3420368432998657\n",
      "loss ->  0.34068992733955383\n",
      "loss ->  0.33935868740081787\n",
      "loss ->  0.33804193139076233\n",
      "loss ->  0.3367388844490051\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we reached above optimized loss with 5 words and 32 examples. so let's pull in all the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training On Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words: # the examples we can generate from the first 5 words\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  19.505229949951172\n",
      "loss ->  17.084484100341797\n",
      "loss ->  15.776531219482422\n",
      "loss ->  14.833340644836426\n",
      "loss ->  14.002605438232422\n",
      "loss ->  13.253263473510742\n",
      "loss ->  12.57991886138916\n",
      "loss ->  11.983102798461914\n",
      "loss ->  11.47049331665039\n",
      "loss ->  11.05185604095459\n",
      "loss ->  10.709586143493652\n",
      "loss ->  10.407631874084473\n",
      "loss ->  10.127808570861816\n",
      "loss ->  9.864364624023438\n",
      "loss ->  9.614501953125\n",
      "loss ->  9.376439094543457\n",
      "loss ->  9.148944854736328\n",
      "loss ->  8.931110382080078\n",
      "loss ->  8.722230911254883\n",
      "loss ->  8.521748542785645\n",
      "loss ->  8.329227447509766\n",
      "loss ->  8.144325256347656\n",
      "loss ->  7.966791152954102\n",
      "loss ->  7.796450614929199\n",
      "loss ->  7.633184909820557\n",
      "loss ->  7.476907730102539\n",
      "loss ->  7.327521800994873\n",
      "loss ->  7.184885025024414\n",
      "loss ->  7.04879093170166\n",
      "loss ->  6.918952465057373\n",
      "loss ->  6.795018196105957\n",
      "loss ->  6.676603317260742\n",
      "loss ->  6.563317775726318\n",
      "loss ->  6.454790115356445\n",
      "loss ->  6.350668907165527\n",
      "loss ->  6.250643253326416\n",
      "loss ->  6.15443229675293\n",
      "loss ->  6.061785697937012\n",
      "loss ->  5.972482681274414\n",
      "loss ->  5.886328220367432\n",
      "loss ->  5.803147315979004\n",
      "loss ->  5.722784519195557\n",
      "loss ->  5.645094394683838\n",
      "loss ->  5.5699462890625\n",
      "loss ->  5.497213840484619\n",
      "loss ->  5.426781177520752\n",
      "loss ->  5.358536243438721\n",
      "loss ->  5.292376518249512\n",
      "loss ->  5.228204727172852\n",
      "loss ->  5.165928840637207\n",
      "loss ->  5.10546875\n",
      "loss ->  5.046748638153076\n",
      "loss ->  4.989699363708496\n",
      "loss ->  4.934262275695801\n",
      "loss ->  4.880381107330322\n",
      "loss ->  4.828006744384766\n",
      "loss ->  4.777096271514893\n",
      "loss ->  4.727610111236572\n",
      "loss ->  4.6795148849487305\n",
      "loss ->  4.6327805519104\n",
      "loss ->  4.587379455566406\n",
      "loss ->  4.543290138244629\n",
      "loss ->  4.500492095947266\n",
      "loss ->  4.458967685699463\n",
      "loss ->  4.418701648712158\n",
      "loss ->  4.379676818847656\n",
      "loss ->  4.341879367828369\n",
      "loss ->  4.305293083190918\n",
      "loss ->  4.269900798797607\n",
      "loss ->  4.235681533813477\n",
      "loss ->  4.202613830566406\n",
      "loss ->  4.170671463012695\n",
      "loss ->  4.139825820922852\n",
      "loss ->  4.110045433044434\n",
      "loss ->  4.08129358291626\n",
      "loss ->  4.053532600402832\n",
      "loss ->  4.026721954345703\n",
      "loss ->  4.000818729400635\n",
      "loss ->  3.975781202316284\n",
      "loss ->  3.9515655040740967\n",
      "loss ->  3.9281294345855713\n",
      "loss ->  3.9054319858551025\n",
      "loss ->  3.883434534072876\n",
      "loss ->  3.8620996475219727\n",
      "loss ->  3.841392755508423\n",
      "loss ->  3.8212807178497314\n",
      "loss ->  3.8017354011535645\n",
      "loss ->  3.7827277183532715\n",
      "loss ->  3.7642338275909424\n",
      "loss ->  3.7462310791015625\n",
      "loss ->  3.728696823120117\n",
      "loss ->  3.7116124629974365\n",
      "loss ->  3.694960355758667\n",
      "loss ->  3.6787240505218506\n",
      "loss ->  3.662888288497925\n",
      "loss ->  3.6474387645721436\n",
      "loss ->  3.6323626041412354\n",
      "loss ->  3.6176466941833496\n",
      "loss ->  3.6032798290252686\n",
      "loss ->  3.589250326156616\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] \n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch\n",
    "\n",
    "The reason why above training takes a lot of time is that, it's doing a forward and backward pass on a large dataset. To optimize the training process we introduce mini-batches. \n",
    "\n",
    "With mini-batch, we do the forward and backward passes on a smaller dataset. Once optimized we move to another batch for training and optimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  3.3262014389038086\n",
      "loss ->  3.990469455718994\n",
      "loss ->  4.568592071533203\n",
      "loss ->  3.4572391510009766\n",
      "loss ->  3.057582139968872\n",
      "loss ->  2.968867540359497\n",
      "loss ->  4.13862943649292\n",
      "loss ->  3.8914408683776855\n",
      "loss ->  4.303944110870361\n",
      "loss ->  3.335280179977417\n",
      "loss ->  3.6771888732910156\n",
      "loss ->  3.1159186363220215\n",
      "loss ->  3.814709424972534\n",
      "loss ->  3.5316100120544434\n",
      "loss ->  3.5149285793304443\n",
      "loss ->  2.8092126846313477\n",
      "loss ->  3.787062883377075\n",
      "loss ->  3.6012444496154785\n",
      "loss ->  3.673198699951172\n",
      "loss ->  3.9197616577148438\n",
      "loss ->  4.388669490814209\n",
      "loss ->  3.146528959274292\n",
      "loss ->  3.1732144355773926\n",
      "loss ->  3.5235331058502197\n",
      "loss ->  3.263209819793701\n",
      "loss ->  3.5374042987823486\n",
      "loss ->  3.11478853225708\n",
      "loss ->  3.938927173614502\n",
      "loss ->  4.150063991546631\n",
      "loss ->  2.880949020385742\n",
      "loss ->  3.6969046592712402\n",
      "loss ->  3.147365093231201\n",
      "loss ->  3.9225196838378906\n",
      "loss ->  3.6787753105163574\n",
      "loss ->  3.236668586730957\n",
      "loss ->  3.5371813774108887\n",
      "loss ->  3.903244972229004\n",
      "loss ->  3.2392003536224365\n",
      "loss ->  3.5782012939453125\n",
      "loss ->  3.029801845550537\n",
      "loss ->  3.7204091548919678\n",
      "loss ->  3.6255314350128174\n",
      "loss ->  3.4678220748901367\n",
      "loss ->  3.887538433074951\n",
      "loss ->  4.324337482452393\n",
      "loss ->  3.0221805572509766\n",
      "loss ->  3.480983018875122\n",
      "loss ->  3.453226327896118\n",
      "loss ->  2.4027440547943115\n",
      "loss ->  4.0787529945373535\n",
      "loss ->  3.500005006790161\n",
      "loss ->  3.2898337841033936\n",
      "loss ->  3.7904162406921387\n",
      "loss ->  3.031040906906128\n",
      "loss ->  3.107012987136841\n",
      "loss ->  2.949817419052124\n",
      "loss ->  2.9427599906921387\n",
      "loss ->  4.802146911621094\n",
      "loss ->  2.767101287841797\n",
      "loss ->  3.2254419326782227\n",
      "loss ->  3.371626138687134\n",
      "loss ->  3.554943561553955\n",
      "loss ->  3.0595974922180176\n",
      "loss ->  3.6152403354644775\n",
      "loss ->  3.1893060207366943\n",
      "loss ->  3.0356273651123047\n",
      "loss ->  3.2109174728393555\n",
      "loss ->  2.80183482170105\n",
      "loss ->  3.467437982559204\n",
      "loss ->  3.265791893005371\n",
      "loss ->  3.1458942890167236\n",
      "loss ->  3.69720721244812\n",
      "loss ->  2.8810276985168457\n",
      "loss ->  3.4770917892456055\n",
      "loss ->  2.721588373184204\n",
      "loss ->  3.8057491779327393\n",
      "loss ->  2.7209935188293457\n",
      "loss ->  3.0538854598999023\n",
      "loss ->  2.7124574184417725\n",
      "loss ->  3.120622158050537\n",
      "loss ->  2.716887950897217\n",
      "loss ->  3.204951524734497\n",
      "loss ->  2.4412715435028076\n",
      "loss ->  2.824753761291504\n",
      "loss ->  2.7856147289276123\n",
      "loss ->  3.181116819381714\n",
      "loss ->  2.9146928787231445\n",
      "loss ->  3.2978336811065674\n",
      "loss ->  2.992945909500122\n",
      "loss ->  4.014272689819336\n",
      "loss ->  3.246028423309326\n",
      "loss ->  3.092923164367676\n",
      "loss ->  2.6927132606506348\n",
      "loss ->  3.730992078781128\n",
      "loss ->  2.788017988204956\n",
      "loss ->  2.4544572830200195\n",
      "loss ->  3.2549819946289062\n",
      "loss ->  3.124001979827881\n",
      "loss ->  3.6820144653320312\n",
      "loss ->  2.9954423904418945\n",
      "loss ->  2.721302032470703\n",
      "loss ->  3.272015333175659\n",
      "loss ->  3.162796974182129\n",
      "loss ->  2.9939491748809814\n",
      "loss ->  3.3596527576446533\n",
      "loss ->  2.9073307514190674\n",
      "loss ->  2.8450121879577637\n",
      "loss ->  2.728278160095215\n",
      "loss ->  3.2566113471984863\n",
      "loss ->  3.5871851444244385\n",
      "loss ->  3.152841091156006\n",
      "loss ->  3.0289971828460693\n",
      "loss ->  2.553462505340576\n",
      "loss ->  2.900568962097168\n",
      "loss ->  2.4335474967956543\n",
      "loss ->  2.711153984069824\n",
      "loss ->  2.6481471061706543\n",
      "loss ->  3.426367998123169\n",
      "loss ->  2.953432083129883\n",
      "loss ->  3.2609145641326904\n",
      "loss ->  3.1768510341644287\n",
      "loss ->  2.5745747089385986\n",
      "loss ->  2.7922050952911377\n",
      "loss ->  3.483098268508911\n",
      "loss ->  3.021073818206787\n",
      "loss ->  2.726083993911743\n",
      "loss ->  3.561627149581909\n",
      "loss ->  2.9006845951080322\n",
      "loss ->  3.0429158210754395\n",
      "loss ->  2.955512046813965\n",
      "loss ->  2.9940245151519775\n",
      "loss ->  2.884819984436035\n",
      "loss ->  2.7719340324401855\n",
      "loss ->  3.768479108810425\n",
      "loss ->  3.048100471496582\n",
      "loss ->  2.697650909423828\n",
      "loss ->  2.6896982192993164\n",
      "loss ->  2.9961650371551514\n",
      "loss ->  3.5351369380950928\n",
      "loss ->  2.876641273498535\n",
      "loss ->  2.646649122238159\n",
      "loss ->  3.041429281234741\n",
      "loss ->  3.0604968070983887\n",
      "loss ->  3.016766309738159\n",
      "loss ->  3.0072832107543945\n",
      "loss ->  2.8389101028442383\n",
      "loss ->  3.316971778869629\n",
      "loss ->  2.5607211589813232\n",
      "loss ->  2.8631811141967773\n",
      "loss ->  3.0218818187713623\n",
      "loss ->  2.794189214706421\n",
      "loss ->  2.851170539855957\n",
      "loss ->  2.9788761138916016\n",
      "loss ->  2.893791437149048\n",
      "loss ->  2.7165849208831787\n",
      "loss ->  3.4558355808258057\n",
      "loss ->  2.9318292140960693\n",
      "loss ->  3.014824151992798\n",
      "loss ->  3.5264768600463867\n",
      "loss ->  4.039315223693848\n",
      "loss ->  2.874238967895508\n",
      "loss ->  2.754237651824951\n",
      "loss ->  2.516305923461914\n",
      "loss ->  2.6788086891174316\n",
      "loss ->  3.219935178756714\n",
      "loss ->  2.5463781356811523\n",
      "loss ->  3.126967668533325\n",
      "loss ->  3.0648980140686035\n",
      "loss ->  3.017442226409912\n",
      "loss ->  3.2736852169036865\n",
      "loss ->  3.2482383251190186\n",
      "loss ->  3.410243272781372\n",
      "loss ->  2.794464588165283\n",
      "loss ->  3.134913444519043\n",
      "loss ->  2.4594154357910156\n",
      "loss ->  3.3115532398223877\n",
      "loss ->  2.792055130004883\n",
      "loss ->  2.7009987831115723\n",
      "loss ->  2.7948269844055176\n",
      "loss ->  3.092027425765991\n",
      "loss ->  2.7581334114074707\n",
      "loss ->  3.506584644317627\n",
      "loss ->  2.9308815002441406\n",
      "loss ->  2.7571299076080322\n",
      "loss ->  3.34287166595459\n",
      "loss ->  3.5177669525146484\n",
      "loss ->  3.125967025756836\n",
      "loss ->  2.910677909851074\n",
      "loss ->  3.0644350051879883\n",
      "loss ->  2.6479909420013428\n",
      "loss ->  2.512868881225586\n",
      "loss ->  3.11226487159729\n",
      "loss ->  3.7874648571014404\n",
      "loss ->  2.9421331882476807\n",
      "loss ->  3.1279664039611816\n",
      "loss ->  2.9711203575134277\n",
      "loss ->  2.9928808212280273\n",
      "loss ->  2.7704896926879883\n",
      "loss ->  2.675450086593628\n",
      "loss ->  2.9487085342407227\n",
      "loss ->  3.0267248153686523\n",
      "loss ->  3.2421834468841553\n",
      "loss ->  2.988173007965088\n",
      "loss ->  2.9313347339630127\n",
      "loss ->  2.72642183303833\n",
      "loss ->  3.2720682621002197\n",
      "loss ->  3.240021228790283\n",
      "loss ->  2.645267963409424\n",
      "loss ->  3.685309648513794\n",
      "loss ->  2.8340811729431152\n",
      "loss ->  3.1689038276672363\n",
      "loss ->  2.389162063598633\n",
      "loss ->  2.579357862472534\n",
      "loss ->  2.7050178050994873\n",
      "loss ->  2.826784372329712\n",
      "loss ->  2.743486166000366\n",
      "loss ->  2.912797689437866\n",
      "loss ->  3.386338949203491\n",
      "loss ->  2.6212518215179443\n",
      "loss ->  2.755166530609131\n",
      "loss ->  3.401000499725342\n",
      "loss ->  2.772975444793701\n",
      "loss ->  2.7118051052093506\n",
      "loss ->  2.9120662212371826\n",
      "loss ->  2.8184173107147217\n",
      "loss ->  2.975541830062866\n",
      "loss ->  2.612109899520874\n",
      "loss ->  2.7654871940612793\n",
      "loss ->  2.683476448059082\n",
      "loss ->  2.9378957748413086\n",
      "loss ->  2.8758134841918945\n",
      "loss ->  3.3950979709625244\n",
      "loss ->  2.765723705291748\n",
      "loss ->  2.758354902267456\n",
      "loss ->  2.765780210494995\n",
      "loss ->  2.768315315246582\n",
      "loss ->  2.7037580013275146\n",
      "loss ->  2.871230363845825\n",
      "loss ->  2.8625564575195312\n",
      "loss ->  3.0946178436279297\n",
      "loss ->  2.6752970218658447\n",
      "loss ->  3.1673550605773926\n",
      "loss ->  2.9119207859039307\n",
      "loss ->  2.5353810787200928\n",
      "loss ->  3.1546900272369385\n",
      "loss ->  2.968168020248413\n",
      "loss ->  2.707733631134033\n",
      "loss ->  2.7881572246551514\n",
      "loss ->  2.860830783843994\n",
      "loss ->  3.106078624725342\n",
      "loss ->  2.5722532272338867\n",
      "loss ->  2.998426675796509\n",
      "loss ->  2.8325369358062744\n",
      "loss ->  3.412182331085205\n",
      "loss ->  2.7072606086730957\n",
      "loss ->  2.8171067237854004\n",
      "loss ->  3.1419284343719482\n",
      "loss ->  3.481598138809204\n",
      "loss ->  2.8927440643310547\n",
      "loss ->  2.4842939376831055\n",
      "loss ->  2.600337505340576\n",
      "loss ->  3.24334454536438\n",
      "loss ->  2.7314043045043945\n",
      "loss ->  2.693507432937622\n",
      "loss ->  3.2982840538024902\n",
      "loss ->  2.877199172973633\n",
      "loss ->  2.944159984588623\n",
      "loss ->  2.668611764907837\n",
      "loss ->  2.8547475337982178\n",
      "loss ->  2.8234143257141113\n",
      "loss ->  3.281507730484009\n",
      "loss ->  2.851747512817383\n",
      "loss ->  2.9020659923553467\n",
      "loss ->  2.936093330383301\n",
      "loss ->  2.9189016819000244\n",
      "loss ->  2.361156940460205\n",
      "loss ->  2.8384323120117188\n",
      "loss ->  2.697460651397705\n",
      "loss ->  2.3576884269714355\n",
      "loss ->  3.1691408157348633\n",
      "loss ->  2.539177894592285\n",
      "loss ->  2.5746853351593018\n",
      "loss ->  3.1173183917999268\n",
      "loss ->  2.6704940795898438\n",
      "loss ->  2.745300054550171\n",
      "loss ->  2.82210373878479\n",
      "loss ->  3.3902201652526855\n",
      "loss ->  2.7999966144561768\n",
      "loss ->  2.6422529220581055\n",
      "loss ->  2.7903592586517334\n",
      "loss ->  2.921536684036255\n",
      "loss ->  3.2191162109375\n",
      "loss ->  3.1441094875335693\n",
      "loss ->  2.7967529296875\n",
      "loss ->  3.542248010635376\n",
      "loss ->  2.9614930152893066\n",
      "loss ->  3.1047468185424805\n",
      "loss ->  2.8400025367736816\n",
      "loss ->  3.139058828353882\n",
      "loss ->  2.960371971130371\n",
      "loss ->  2.8282053470611572\n",
      "loss ->  2.6876165866851807\n",
      "loss ->  2.967653274536133\n",
      "loss ->  2.4540648460388184\n",
      "loss ->  2.780790090560913\n",
      "loss ->  3.186049461364746\n",
      "loss ->  2.867392063140869\n",
      "loss ->  2.3079516887664795\n",
      "loss ->  3.218494176864624\n",
      "loss ->  2.6401774883270264\n",
      "loss ->  3.0036227703094482\n",
      "loss ->  3.158416986465454\n",
      "loss ->  2.264268398284912\n",
      "loss ->  2.4735891819000244\n",
      "loss ->  2.6606414318084717\n",
      "loss ->  2.4395930767059326\n",
      "loss ->  2.46142578125\n",
      "loss ->  2.7294082641601562\n",
      "loss ->  3.021697998046875\n",
      "loss ->  2.9937381744384766\n",
      "loss ->  3.683115243911743\n",
      "loss ->  2.6844141483306885\n",
      "loss ->  2.6901936531066895\n",
      "loss ->  3.08347225189209\n",
      "loss ->  2.7145309448242188\n",
      "loss ->  2.6694228649139404\n",
      "loss ->  2.7944564819335938\n",
      "loss ->  2.557300329208374\n",
      "loss ->  2.6088621616363525\n",
      "loss ->  2.2273244857788086\n",
      "loss ->  2.6817426681518555\n",
      "loss ->  2.6127443313598633\n",
      "loss ->  2.8627235889434814\n",
      "loss ->  2.596351385116577\n",
      "loss ->  2.9596757888793945\n",
      "loss ->  2.7991220951080322\n",
      "loss ->  3.1715996265411377\n",
      "loss ->  3.185356855392456\n",
      "loss ->  3.3681960105895996\n",
      "loss ->  2.5584871768951416\n",
      "loss ->  2.9259915351867676\n",
      "loss ->  2.9237985610961914\n",
      "loss ->  2.9614598751068115\n",
      "loss ->  2.3878087997436523\n",
      "loss ->  2.3674919605255127\n",
      "loss ->  3.0166232585906982\n",
      "loss ->  2.53462290763855\n",
      "loss ->  2.7764298915863037\n",
      "loss ->  2.680572271347046\n",
      "loss ->  2.847151279449463\n",
      "loss ->  3.0300159454345703\n",
      "loss ->  2.9810564517974854\n",
      "loss ->  2.8863754272460938\n",
      "loss ->  2.967524290084839\n",
      "loss ->  2.6846015453338623\n",
      "loss ->  3.0977087020874023\n",
      "loss ->  2.521806001663208\n",
      "loss ->  2.9133119583129883\n",
      "loss ->  3.2178118228912354\n",
      "loss ->  3.294790744781494\n",
      "loss ->  2.6231513023376465\n",
      "loss ->  3.213336944580078\n",
      "loss ->  2.6338720321655273\n",
      "loss ->  3.001267910003662\n",
      "loss ->  2.639723062515259\n",
      "loss ->  3.415952444076538\n",
      "loss ->  2.888441801071167\n",
      "loss ->  2.8749892711639404\n",
      "loss ->  2.5823023319244385\n",
      "loss ->  2.6956169605255127\n",
      "loss ->  2.948195219039917\n",
      "loss ->  2.907013416290283\n",
      "loss ->  3.077849864959717\n",
      "loss ->  2.8660829067230225\n",
      "loss ->  2.4781816005706787\n",
      "loss ->  2.9882702827453613\n",
      "loss ->  2.686598062515259\n",
      "loss ->  2.8053131103515625\n",
      "loss ->  2.824671745300293\n",
      "loss ->  2.859809398651123\n",
      "loss ->  2.653087854385376\n",
      "loss ->  3.2334179878234863\n",
      "loss ->  2.899118185043335\n",
      "loss ->  2.5145983695983887\n",
      "loss ->  2.8579647541046143\n",
      "loss ->  2.7064547538757324\n",
      "loss ->  2.4511802196502686\n",
      "loss ->  2.973255157470703\n",
      "loss ->  2.9156992435455322\n",
      "loss ->  3.2819032669067383\n",
      "loss ->  2.89323353767395\n",
      "loss ->  3.057279109954834\n",
      "loss ->  2.919353723526001\n",
      "loss ->  2.6256091594696045\n",
      "loss ->  2.8313701152801514\n",
      "loss ->  2.4768407344818115\n",
      "loss ->  3.867105007171631\n",
      "loss ->  2.871514320373535\n",
      "loss ->  3.480785846710205\n",
      "loss ->  2.71105694770813\n",
      "loss ->  2.619783401489258\n",
      "loss ->  2.9409236907958984\n",
      "loss ->  3.0596914291381836\n",
      "loss ->  2.921677350997925\n",
      "loss ->  2.945363998413086\n",
      "loss ->  3.343158721923828\n",
      "loss ->  2.6886816024780273\n",
      "loss ->  2.8434152603149414\n",
      "loss ->  2.5576703548431396\n",
      "loss ->  2.987543821334839\n",
      "loss ->  3.1256535053253174\n",
      "loss ->  2.6452715396881104\n",
      "loss ->  2.730475902557373\n",
      "loss ->  2.5904176235198975\n",
      "loss ->  2.867786169052124\n",
      "loss ->  2.9252235889434814\n",
      "loss ->  2.8066396713256836\n",
      "loss ->  2.8117051124572754\n",
      "loss ->  2.7603440284729004\n",
      "loss ->  2.7788379192352295\n",
      "loss ->  2.6537740230560303\n",
      "loss ->  2.4197418689727783\n",
      "loss ->  2.3823533058166504\n",
      "loss ->  2.2923905849456787\n",
      "loss ->  2.8144431114196777\n",
      "loss ->  3.0108845233917236\n",
      "loss ->  2.9298269748687744\n",
      "loss ->  2.5151710510253906\n",
      "loss ->  2.756613254547119\n",
      "loss ->  2.96524977684021\n",
      "loss ->  2.875401258468628\n",
      "loss ->  2.445286273956299\n",
      "loss ->  2.915956497192383\n",
      "loss ->  3.0202059745788574\n",
      "loss ->  2.7174577713012695\n",
      "loss ->  2.7809054851531982\n",
      "loss ->  3.0819320678710938\n",
      "loss ->  2.9045231342315674\n",
      "loss ->  2.5922605991363525\n",
      "loss ->  2.751763105392456\n",
      "loss ->  3.007812261581421\n",
      "loss ->  2.5093507766723633\n",
      "loss ->  2.952387809753418\n",
      "loss ->  3.457040309906006\n",
      "loss ->  2.667637348175049\n",
      "loss ->  2.679715633392334\n",
      "loss ->  2.809000015258789\n",
      "loss ->  2.639075994491577\n",
      "loss ->  2.8763821125030518\n",
      "loss ->  3.151998996734619\n",
      "loss ->  3.055551052093506\n",
      "loss ->  3.1963024139404297\n",
      "loss ->  2.818821430206299\n",
      "loss ->  2.608609437942505\n",
      "loss ->  2.809783458709717\n",
      "loss ->  3.181123733520508\n",
      "loss ->  3.266387462615967\n",
      "loss ->  2.3708596229553223\n",
      "loss ->  2.7202725410461426\n",
      "loss ->  2.586050033569336\n",
      "loss ->  3.017587423324585\n",
      "loss ->  2.3354251384735107\n",
      "loss ->  2.9892945289611816\n",
      "loss ->  3.1952452659606934\n",
      "loss ->  2.43723464012146\n",
      "loss ->  2.9183945655822754\n",
      "loss ->  2.7907488346099854\n",
      "loss ->  2.7956595420837402\n",
      "loss ->  2.7962372303009033\n",
      "loss ->  2.816009044647217\n",
      "loss ->  2.8102784156799316\n",
      "loss ->  2.8161087036132812\n",
      "loss ->  2.7891223430633545\n",
      "loss ->  3.024646043777466\n",
      "loss ->  2.3809831142425537\n",
      "loss ->  2.7424700260162354\n",
      "loss ->  3.0757877826690674\n",
      "loss ->  2.721961259841919\n",
      "loss ->  2.8670785427093506\n",
      "loss ->  2.7874507904052734\n",
      "loss ->  2.4605393409729004\n",
      "loss ->  3.059302806854248\n",
      "loss ->  3.372911214828491\n",
      "loss ->  2.7065646648406982\n",
      "loss ->  3.0398716926574707\n",
      "loss ->  2.4338090419769287\n",
      "loss ->  2.953948974609375\n",
      "loss ->  2.5977823734283447\n",
      "loss ->  2.7735819816589355\n",
      "loss ->  2.708096981048584\n",
      "loss ->  2.4942829608917236\n",
      "loss ->  3.1186485290527344\n",
      "loss ->  2.677140235900879\n",
      "loss ->  2.9064414501190186\n",
      "loss ->  3.0959584712982178\n",
      "loss ->  2.597003221511841\n",
      "loss ->  2.7890312671661377\n",
      "loss ->  2.4630508422851562\n",
      "loss ->  2.462618589401245\n",
      "loss ->  3.1702964305877686\n",
      "loss ->  2.53330135345459\n",
      "loss ->  2.6116268634796143\n",
      "loss ->  2.1393465995788574\n",
      "loss ->  2.409696340560913\n",
      "loss ->  3.1136770248413086\n",
      "loss ->  2.627004861831665\n",
      "loss ->  2.7074263095855713\n",
      "loss ->  2.9408698081970215\n",
      "loss ->  2.917055130004883\n",
      "loss ->  2.9153404235839844\n",
      "loss ->  2.5284194946289062\n",
      "loss ->  3.2776899337768555\n",
      "loss ->  2.781534194946289\n",
      "loss ->  2.513338088989258\n",
      "loss ->  2.633742332458496\n",
      "loss ->  2.606264352798462\n",
      "loss ->  2.5409510135650635\n",
      "loss ->  2.4930427074432373\n",
      "loss ->  2.6257638931274414\n",
      "loss ->  2.6408255100250244\n",
      "loss ->  2.3692924976348877\n",
      "loss ->  2.6613240242004395\n",
      "loss ->  2.7226614952087402\n",
      "loss ->  2.751250743865967\n",
      "loss ->  2.928053855895996\n",
      "loss ->  3.271366834640503\n",
      "loss ->  3.0093703269958496\n",
      "loss ->  2.4783899784088135\n",
      "loss ->  2.6793713569641113\n",
      "loss ->  2.9860150814056396\n",
      "loss ->  2.506831169128418\n",
      "loss ->  2.581831216812134\n",
      "loss ->  2.810535192489624\n",
      "loss ->  2.654498338699341\n",
      "loss ->  2.6515233516693115\n",
      "loss ->  2.4745893478393555\n",
      "loss ->  2.493860960006714\n",
      "loss ->  2.997800827026367\n",
      "loss ->  2.6402034759521484\n",
      "loss ->  2.6565487384796143\n",
      "loss ->  2.621912717819214\n",
      "loss ->  2.6735432147979736\n",
      "loss ->  2.6906392574310303\n",
      "loss ->  2.80753493309021\n",
      "loss ->  2.600344181060791\n",
      "loss ->  2.633847951889038\n",
      "loss ->  2.7649354934692383\n",
      "loss ->  2.8443961143493652\n",
      "loss ->  2.8178908824920654\n",
      "loss ->  2.9300882816314697\n",
      "loss ->  2.678370714187622\n",
      "loss ->  2.8250820636749268\n",
      "loss ->  2.6428699493408203\n",
      "loss ->  2.4868814945220947\n",
      "loss ->  3.095531940460205\n",
      "loss ->  2.626598358154297\n",
      "loss ->  2.7577571868896484\n",
      "loss ->  2.7299282550811768\n",
      "loss ->  2.6334645748138428\n",
      "loss ->  2.6391544342041016\n",
      "loss ->  2.6869430541992188\n",
      "loss ->  2.430178165435791\n",
      "loss ->  2.7829465866088867\n",
      "loss ->  2.6683857440948486\n",
      "loss ->  2.608236312866211\n",
      "loss ->  2.8718831539154053\n",
      "loss ->  2.6072611808776855\n",
      "loss ->  2.908292770385742\n",
      "loss ->  2.4190313816070557\n",
      "loss ->  2.4810690879821777\n",
      "loss ->  2.483294725418091\n",
      "loss ->  2.652242422103882\n",
      "loss ->  2.774062156677246\n",
      "loss ->  2.9580225944519043\n",
      "loss ->  2.5305023193359375\n",
      "loss ->  2.9921493530273438\n",
      "loss ->  2.7730813026428223\n",
      "loss ->  2.805262565612793\n",
      "loss ->  2.8035061359405518\n",
      "loss ->  2.5055313110351562\n",
      "loss ->  2.629457473754883\n",
      "loss ->  2.8818554878234863\n",
      "loss ->  2.453096389770508\n",
      "loss ->  2.5179033279418945\n",
      "loss ->  2.5723936557769775\n",
      "loss ->  2.7147088050842285\n",
      "loss ->  2.888824701309204\n",
      "loss ->  3.0287868976593018\n",
      "loss ->  2.3896615505218506\n",
      "loss ->  2.5225813388824463\n",
      "loss ->  2.4026026725769043\n",
      "loss ->  2.178013563156128\n",
      "loss ->  2.6445446014404297\n",
      "loss ->  2.556407928466797\n",
      "loss ->  2.9697091579437256\n",
      "loss ->  2.745495080947876\n",
      "loss ->  2.7500553131103516\n",
      "loss ->  2.839582920074463\n",
      "loss ->  2.8787224292755127\n",
      "loss ->  2.5608999729156494\n",
      "loss ->  2.5535686016082764\n",
      "loss ->  2.5481042861938477\n",
      "loss ->  2.3438620567321777\n",
      "loss ->  2.862229347229004\n",
      "loss ->  2.950800657272339\n",
      "loss ->  2.5752811431884766\n",
      "loss ->  3.071554660797119\n",
      "loss ->  2.4230329990386963\n",
      "loss ->  2.425527572631836\n",
      "loss ->  2.7400946617126465\n",
      "loss ->  2.6789710521698\n",
      "loss ->  2.5240020751953125\n",
      "loss ->  2.6521778106689453\n",
      "loss ->  2.8287553787231445\n",
      "loss ->  2.903073310852051\n",
      "loss ->  2.707216501235962\n",
      "loss ->  2.5886149406433105\n",
      "loss ->  2.177706003189087\n",
      "loss ->  2.858694076538086\n",
      "loss ->  2.882556438446045\n",
      "loss ->  2.7254090309143066\n",
      "loss ->  2.7277657985687256\n",
      "loss ->  2.751429557800293\n",
      "loss ->  2.9689674377441406\n",
      "loss ->  2.8457467555999756\n",
      "loss ->  2.910362958908081\n",
      "loss ->  2.5519144535064697\n",
      "loss ->  3.0328609943389893\n",
      "loss ->  2.4777169227600098\n",
      "loss ->  2.7883827686309814\n",
      "loss ->  2.8477396965026855\n",
      "loss ->  2.4081175327301025\n",
      "loss ->  2.7557692527770996\n",
      "loss ->  2.739873170852661\n",
      "loss ->  2.7381796836853027\n",
      "loss ->  2.6898257732391357\n",
      "loss ->  3.333271026611328\n",
      "loss ->  2.9238264560699463\n",
      "loss ->  2.9568216800689697\n",
      "loss ->  2.6417760848999023\n",
      "loss ->  2.2269749641418457\n",
      "loss ->  2.6509487628936768\n",
      "loss ->  2.9854860305786133\n",
      "loss ->  2.725842237472534\n",
      "loss ->  2.8485803604125977\n",
      "loss ->  2.7922146320343018\n",
      "loss ->  2.2702138423919678\n",
      "loss ->  2.9618430137634277\n",
      "loss ->  2.5105245113372803\n",
      "loss ->  2.4932138919830322\n",
      "loss ->  2.3784120082855225\n",
      "loss ->  2.7228240966796875\n",
      "loss ->  3.1456613540649414\n",
      "loss ->  3.03792405128479\n",
      "loss ->  2.9366812705993652\n",
      "loss ->  2.5926380157470703\n",
      "loss ->  2.6521778106689453\n",
      "loss ->  3.1826369762420654\n",
      "loss ->  2.747114896774292\n",
      "loss ->  2.55395770072937\n",
      "loss ->  2.39094877243042\n",
      "loss ->  2.6701552867889404\n",
      "loss ->  2.9270246028900146\n",
      "loss ->  2.4761502742767334\n",
      "loss ->  2.6662683486938477\n",
      "loss ->  2.9820034503936768\n",
      "loss ->  3.0618584156036377\n",
      "loss ->  2.8314049243927\n",
      "loss ->  2.4278364181518555\n",
      "loss ->  2.8548123836517334\n",
      "loss ->  2.556195020675659\n",
      "loss ->  2.9392013549804688\n",
      "loss ->  3.1085448265075684\n",
      "loss ->  2.5974040031433105\n",
      "loss ->  2.5811681747436523\n",
      "loss ->  3.0283899307250977\n",
      "loss ->  2.7439656257629395\n",
      "loss ->  2.8454902172088623\n",
      "loss ->  2.214254379272461\n",
      "loss ->  2.909001350402832\n",
      "loss ->  2.3750059604644775\n",
      "loss ->  2.437636137008667\n",
      "loss ->  2.6714980602264404\n",
      "loss ->  2.6964097023010254\n",
      "loss ->  2.7821600437164307\n",
      "loss ->  3.0146565437316895\n",
      "loss ->  2.942753314971924\n",
      "loss ->  2.373673677444458\n",
      "loss ->  2.676591157913208\n",
      "loss ->  3.532935619354248\n",
      "loss ->  2.790214776992798\n",
      "loss ->  2.6366684436798096\n",
      "loss ->  2.7258567810058594\n",
      "loss ->  2.2005980014801025\n",
      "loss ->  3.2249395847320557\n",
      "loss ->  2.492520809173584\n",
      "loss ->  2.3437652587890625\n",
      "loss ->  3.044323444366455\n",
      "loss ->  2.530219316482544\n",
      "loss ->  3.0485939979553223\n",
      "loss ->  3.0314557552337646\n",
      "loss ->  2.637568950653076\n",
      "loss ->  3.0525405406951904\n",
      "loss ->  2.346240997314453\n",
      "loss ->  2.8435239791870117\n",
      "loss ->  2.3432183265686035\n",
      "loss ->  2.943368434906006\n",
      "loss ->  3.0154659748077393\n",
      "loss ->  2.459855556488037\n",
      "loss ->  2.5398826599121094\n",
      "loss ->  3.1948187351226807\n",
      "loss ->  2.524170160293579\n",
      "loss ->  2.453341245651245\n",
      "loss ->  2.596048593521118\n",
      "loss ->  2.7690176963806152\n",
      "loss ->  2.6597139835357666\n",
      "loss ->  2.5526959896087646\n",
      "loss ->  2.8091845512390137\n",
      "loss ->  2.793048858642578\n",
      "loss ->  2.603036403656006\n",
      "loss ->  2.5537269115448\n",
      "loss ->  2.8296706676483154\n",
      "loss ->  2.6690778732299805\n",
      "loss ->  3.1484546661376953\n",
      "loss ->  2.558063507080078\n",
      "loss ->  2.9473235607147217\n",
      "loss ->  2.6845896244049072\n",
      "loss ->  2.7028162479400635\n",
      "loss ->  2.7263245582580566\n",
      "loss ->  2.627080202102661\n",
      "loss ->  2.7324564456939697\n",
      "loss ->  2.7971510887145996\n",
      "loss ->  2.511843204498291\n",
      "loss ->  2.850208044052124\n",
      "loss ->  2.68110728263855\n",
      "loss ->  2.9729275703430176\n",
      "loss ->  2.592066526412964\n",
      "loss ->  2.833900213241577\n",
      "loss ->  2.9408810138702393\n",
      "loss ->  2.8547580242156982\n",
      "loss ->  2.471389055252075\n",
      "loss ->  2.7120776176452637\n",
      "loss ->  2.5760152339935303\n",
      "loss ->  3.0552704334259033\n",
      "loss ->  2.734570264816284\n",
      "loss ->  2.9760262966156006\n",
      "loss ->  2.676576852798462\n",
      "loss ->  2.5153613090515137\n",
      "loss ->  2.9378502368927\n",
      "loss ->  2.806318998336792\n",
      "loss ->  2.5367865562438965\n",
      "loss ->  2.7060980796813965\n",
      "loss ->  2.4946236610412598\n",
      "loss ->  2.526735544204712\n",
      "loss ->  2.372608184814453\n",
      "loss ->  2.9129409790039062\n",
      "loss ->  3.048450469970703\n",
      "loss ->  2.741363286972046\n",
      "loss ->  3.1511030197143555\n",
      "loss ->  2.4751107692718506\n",
      "loss ->  2.8063442707061768\n",
      "loss ->  2.626511812210083\n",
      "loss ->  2.844510316848755\n",
      "loss ->  2.863844871520996\n",
      "loss ->  2.5870087146759033\n",
      "loss ->  2.668738842010498\n",
      "loss ->  2.2270760536193848\n",
      "loss ->  2.4079089164733887\n",
      "loss ->  2.737417459487915\n",
      "loss ->  2.558008909225464\n",
      "loss ->  2.9232263565063477\n",
      "loss ->  2.3661537170410156\n",
      "loss ->  3.392439842224121\n",
      "loss ->  2.7242791652679443\n",
      "loss ->  3.0793538093566895\n",
      "loss ->  2.6673505306243896\n",
      "loss ->  2.7173657417297363\n",
      "loss ->  2.6133031845092773\n",
      "loss ->  2.5016939640045166\n",
      "loss ->  2.6342151165008545\n",
      "loss ->  2.478809118270874\n",
      "loss ->  2.6461212635040283\n",
      "loss ->  2.853285074234009\n",
      "loss ->  2.5798118114471436\n",
      "loss ->  3.0260708332061768\n",
      "loss ->  2.856700897216797\n",
      "loss ->  2.388607978820801\n",
      "loss ->  2.689788341522217\n",
      "loss ->  2.4824461936950684\n",
      "loss ->  2.9905104637145996\n",
      "loss ->  2.4721667766571045\n",
      "loss ->  2.510340452194214\n",
      "loss ->  2.6561648845672607\n",
      "loss ->  2.805682420730591\n",
      "loss ->  3.1732735633850098\n",
      "loss ->  2.321016788482666\n",
      "loss ->  2.640155076980591\n",
      "loss ->  2.7138330936431885\n",
      "loss ->  2.479034900665283\n",
      "loss ->  2.893345594406128\n",
      "loss ->  2.77001953125\n",
      "loss ->  2.6244194507598877\n",
      "loss ->  2.8521904945373535\n",
      "loss ->  2.5672717094421387\n",
      "loss ->  2.6014163494110107\n",
      "loss ->  2.6276516914367676\n",
      "loss ->  2.5424726009368896\n",
      "loss ->  2.7619428634643555\n",
      "loss ->  2.9434661865234375\n",
      "loss ->  2.864818572998047\n",
      "loss ->  2.626471996307373\n",
      "loss ->  2.436359405517578\n",
      "loss ->  2.4442996978759766\n",
      "loss ->  2.576981782913208\n",
      "loss ->  2.8809797763824463\n",
      "loss ->  2.9160995483398438\n",
      "loss ->  3.150106430053711\n",
      "loss ->  2.4105758666992188\n",
      "loss ->  2.921393632888794\n",
      "loss ->  2.417970657348633\n",
      "loss ->  2.9349167346954346\n",
      "loss ->  2.6286849975585938\n",
      "loss ->  2.893095016479492\n",
      "loss ->  2.8930788040161133\n",
      "loss ->  2.77374529838562\n",
      "loss ->  2.7835748195648193\n",
      "loss ->  2.955122470855713\n",
      "loss ->  2.4574174880981445\n",
      "loss ->  2.9540743827819824\n",
      "loss ->  2.8061838150024414\n",
      "loss ->  2.625861406326294\n",
      "loss ->  2.8217949867248535\n",
      "loss ->  2.596680164337158\n",
      "loss ->  2.4515886306762695\n",
      "loss ->  2.4679384231567383\n",
      "loss ->  2.7337870597839355\n",
      "loss ->  2.8445210456848145\n",
      "loss ->  2.164389133453369\n",
      "loss ->  2.598829984664917\n",
      "loss ->  2.3399791717529297\n",
      "loss ->  2.9113543033599854\n",
      "loss ->  2.8822693824768066\n",
      "loss ->  2.7425360679626465\n",
      "loss ->  2.659438371658325\n",
      "loss ->  2.590393304824829\n",
      "loss ->  2.8209640979766846\n",
      "loss ->  2.50329327583313\n",
      "loss ->  2.9457476139068604\n",
      "loss ->  2.669066905975342\n",
      "loss ->  2.594385862350464\n",
      "loss ->  2.5259311199188232\n",
      "loss ->  2.6002087593078613\n",
      "loss ->  2.893613815307617\n",
      "loss ->  3.109508991241455\n",
      "loss ->  2.808318853378296\n",
      "loss ->  2.672150135040283\n",
      "loss ->  2.681490421295166\n",
      "loss ->  2.573578119277954\n",
      "loss ->  2.3786654472351074\n",
      "loss ->  2.868325710296631\n",
      "loss ->  2.705981969833374\n",
      "loss ->  2.9297678470611572\n",
      "loss ->  2.5751700401306152\n",
      "loss ->  2.5677778720855713\n",
      "loss ->  2.489870548248291\n",
      "loss ->  2.5714292526245117\n",
      "loss ->  2.887390375137329\n",
      "loss ->  2.2509658336639404\n",
      "loss ->  2.7340309619903564\n",
      "loss ->  2.4751081466674805\n",
      "loss ->  2.564514398574829\n",
      "loss ->  2.7335596084594727\n",
      "loss ->  2.9238505363464355\n",
      "loss ->  2.644000291824341\n",
      "loss ->  2.696138381958008\n",
      "loss ->  2.6730198860168457\n",
      "loss ->  2.429248094558716\n",
      "loss ->  3.1139025688171387\n",
      "loss ->  2.958743095397949\n",
      "loss ->  2.608360767364502\n",
      "loss ->  2.6804769039154053\n",
      "loss ->  2.896367073059082\n",
      "loss ->  2.478264331817627\n",
      "loss ->  2.347978115081787\n",
      "loss ->  2.463318109512329\n",
      "loss ->  2.8503401279449463\n",
      "loss ->  2.489935874938965\n",
      "loss ->  2.776491165161133\n",
      "loss ->  2.4940242767333984\n",
      "loss ->  2.3238778114318848\n",
      "loss ->  2.569801092147827\n",
      "loss ->  2.3873953819274902\n",
      "loss ->  2.4537007808685303\n",
      "loss ->  2.3053038120269775\n",
      "loss ->  2.8566458225250244\n",
      "loss ->  2.706852436065674\n",
      "loss ->  2.3817708492279053\n",
      "loss ->  3.0271458625793457\n",
      "loss ->  2.377450704574585\n",
      "loss ->  2.8393006324768066\n",
      "loss ->  2.193849563598633\n",
      "loss ->  2.760218858718872\n",
      "loss ->  2.3579893112182617\n",
      "loss ->  2.5381898880004883\n",
      "loss ->  2.6509499549865723\n",
      "loss ->  2.773139476776123\n",
      "loss ->  2.7365996837615967\n",
      "loss ->  2.471134662628174\n",
      "loss ->  2.744326591491699\n",
      "loss ->  2.8423168659210205\n",
      "loss ->  2.6710782051086426\n",
      "loss ->  2.3190507888793945\n",
      "loss ->  2.3781466484069824\n",
      "loss ->  3.0822880268096924\n",
      "loss ->  2.8558740615844727\n",
      "loss ->  2.935796022415161\n",
      "loss ->  2.331033706665039\n",
      "loss ->  2.5448110103607178\n",
      "loss ->  2.854907274246216\n",
      "loss ->  2.311279296875\n",
      "loss ->  3.3661136627197266\n",
      "loss ->  2.8985719680786133\n",
      "loss ->  2.4704341888427734\n",
      "loss ->  2.609628915786743\n",
      "loss ->  2.655207395553589\n",
      "loss ->  2.579530954360962\n",
      "loss ->  3.2459781169891357\n",
      "loss ->  2.388521432876587\n",
      "loss ->  2.5294764041900635\n",
      "loss ->  2.4455788135528564\n",
      "loss ->  2.8128955364227295\n",
      "loss ->  2.5902621746063232\n",
      "loss ->  2.8412983417510986\n",
      "loss ->  2.458575963973999\n",
      "loss ->  2.7303104400634766\n",
      "loss ->  3.122710704803467\n",
      "loss ->  2.985610246658325\n",
      "loss ->  2.8185935020446777\n",
      "loss ->  2.4506289958953857\n",
      "loss ->  2.4677751064300537\n",
      "loss ->  2.3973169326782227\n",
      "loss ->  2.7975237369537354\n",
      "loss ->  2.6123251914978027\n",
      "loss ->  2.503007650375366\n",
      "loss ->  2.260985851287842\n",
      "loss ->  2.409555196762085\n",
      "loss ->  2.8251590728759766\n",
      "loss ->  2.7575278282165527\n",
      "loss ->  2.901752471923828\n",
      "loss ->  2.5363669395446777\n",
      "loss ->  2.4284021854400635\n",
      "loss ->  2.593125581741333\n",
      "loss ->  2.3308093547821045\n",
      "loss ->  2.7065062522888184\n",
      "loss ->  2.466569662094116\n",
      "loss ->  2.669464349746704\n",
      "loss ->  2.4426538944244385\n",
      "loss ->  3.15006160736084\n",
      "loss ->  2.5541882514953613\n",
      "loss ->  2.71539044380188\n",
      "loss ->  2.9269227981567383\n",
      "loss ->  2.5052330493927\n",
      "loss ->  2.5124504566192627\n",
      "loss ->  2.5690107345581055\n",
      "loss ->  2.64510178565979\n",
      "loss ->  2.949057102203369\n",
      "loss ->  2.646655797958374\n",
      "loss ->  2.871307134628296\n",
      "loss ->  3.0376996994018555\n",
      "loss ->  3.5047104358673096\n",
      "loss ->  2.4440882205963135\n",
      "loss ->  2.4989383220672607\n",
      "loss ->  2.6064045429229736\n",
      "loss ->  2.476080894470215\n",
      "loss ->  2.4505481719970703\n",
      "loss ->  2.3159873485565186\n",
      "loss ->  2.4106125831604004\n",
      "loss ->  2.5106377601623535\n",
      "loss ->  2.6226658821105957\n",
      "loss ->  2.559861898422241\n",
      "loss ->  2.933990240097046\n",
      "loss ->  2.5180063247680664\n",
      "loss ->  2.7505831718444824\n",
      "loss ->  2.6864850521087646\n",
      "loss ->  3.1429085731506348\n",
      "loss ->  2.9258928298950195\n",
      "loss ->  2.554304838180542\n",
      "loss ->  2.982781171798706\n",
      "loss ->  2.90114164352417\n",
      "loss ->  2.6587491035461426\n",
      "loss ->  2.6821179389953613\n",
      "loss ->  2.8957784175872803\n",
      "loss ->  2.4874889850616455\n",
      "loss ->  2.7250421047210693\n",
      "loss ->  2.710906744003296\n",
      "loss ->  3.5613245964050293\n",
      "loss ->  2.4515128135681152\n",
      "loss ->  2.44830322265625\n",
      "loss ->  3.072711229324341\n",
      "loss ->  2.6056013107299805\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you can see that thee training is much faster and therefore, you can afford to increase the iterations for further minimizing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "\n",
    "How do you determine the learning rate? How do you know if it's too small (moving too slowly towards an optimized loss), or it's too big (over-stepping and missing the optimized loss)?\n",
    "\n",
    "One way is to find the min and max range, first. You can provide -0.0001 or lower and find the value that demonstrates a reasonable decrease in the loss. Then find a big number with the same analogy. Based in this, we can see that, the optimized learning rate should be between -0.001 and -1.\n",
    "\n",
    "We can use pytorch's library to create a linear array of learning rates between these two numbers for, say 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # linear rate exponential\n",
    "lrs = 10**lre # learning rates: 10^-3 = 0.001 and 10^0 = 1\n",
    "lrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's reset everything and iterate through possible learning rates to find the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  20.64219856262207\n",
      "loss ->  21.367820739746094\n",
      "loss ->  21.69013786315918\n",
      "loss ->  21.809776306152344\n",
      "loss ->  22.773935317993164\n",
      "loss ->  19.684816360473633\n",
      "loss ->  19.62133026123047\n",
      "loss ->  17.392841339111328\n",
      "loss ->  18.789398193359375\n",
      "loss ->  17.845462799072266\n",
      "loss ->  18.572010040283203\n",
      "loss ->  19.658260345458984\n",
      "loss ->  15.919187545776367\n",
      "loss ->  20.390913009643555\n",
      "loss ->  20.20417594909668\n",
      "loss ->  17.795604705810547\n",
      "loss ->  16.08721160888672\n",
      "loss ->  18.807891845703125\n",
      "loss ->  16.974454879760742\n",
      "loss ->  20.22616958618164\n",
      "loss ->  16.124752044677734\n",
      "loss ->  19.723262786865234\n",
      "loss ->  18.266300201416016\n",
      "loss ->  20.413724899291992\n",
      "loss ->  18.127092361450195\n",
      "loss ->  18.871150970458984\n",
      "loss ->  18.110036849975586\n",
      "loss ->  18.99818229675293\n",
      "loss ->  20.89285659790039\n",
      "loss ->  19.351041793823242\n",
      "loss ->  17.51865577697754\n",
      "loss ->  20.744159698486328\n",
      "loss ->  18.014678955078125\n",
      "loss ->  18.715267181396484\n",
      "loss ->  19.912965774536133\n",
      "loss ->  15.886554718017578\n",
      "loss ->  16.724040985107422\n",
      "loss ->  17.0728702545166\n",
      "loss ->  18.422861099243164\n",
      "loss ->  13.988327980041504\n",
      "loss ->  18.456947326660156\n",
      "loss ->  18.43444061279297\n",
      "loss ->  22.08467674255371\n",
      "loss ->  18.54610252380371\n",
      "loss ->  14.980876922607422\n",
      "loss ->  19.99053192138672\n",
      "loss ->  14.96312427520752\n",
      "loss ->  16.598840713500977\n",
      "loss ->  15.54959774017334\n",
      "loss ->  17.725343704223633\n",
      "loss ->  17.851238250732422\n",
      "loss ->  18.23824119567871\n",
      "loss ->  17.976966857910156\n",
      "loss ->  20.14744758605957\n",
      "loss ->  21.56678009033203\n",
      "loss ->  17.293535232543945\n",
      "loss ->  19.312503814697266\n",
      "loss ->  15.956489562988281\n",
      "loss ->  17.136518478393555\n",
      "loss ->  17.12830924987793\n",
      "loss ->  19.193222045898438\n",
      "loss ->  18.698204040527344\n",
      "loss ->  16.47928237915039\n",
      "loss ->  15.56422233581543\n",
      "loss ->  15.66418170928955\n",
      "loss ->  15.795034408569336\n",
      "loss ->  18.243436813354492\n",
      "loss ->  17.19196319580078\n",
      "loss ->  17.996417999267578\n",
      "loss ->  18.7741641998291\n",
      "loss ->  14.452434539794922\n",
      "loss ->  17.832738876342773\n",
      "loss ->  17.90734100341797\n",
      "loss ->  16.18941879272461\n",
      "loss ->  16.411609649658203\n",
      "loss ->  15.218268394470215\n",
      "loss ->  20.29476547241211\n",
      "loss ->  18.15041160583496\n",
      "loss ->  14.782443046569824\n",
      "loss ->  19.462158203125\n",
      "loss ->  18.164548873901367\n",
      "loss ->  17.945220947265625\n",
      "loss ->  17.133066177368164\n",
      "loss ->  17.979442596435547\n",
      "loss ->  16.281064987182617\n",
      "loss ->  15.21303653717041\n",
      "loss ->  18.881372451782227\n",
      "loss ->  16.436664581298828\n",
      "loss ->  19.126564025878906\n",
      "loss ->  17.234590530395508\n",
      "loss ->  18.637813568115234\n",
      "loss ->  17.464752197265625\n",
      "loss ->  17.602357864379883\n",
      "loss ->  19.341793060302734\n",
      "loss ->  15.92621898651123\n",
      "loss ->  17.951858520507812\n",
      "loss ->  15.06845474243164\n",
      "loss ->  13.354055404663086\n",
      "loss ->  15.183371543884277\n",
      "loss ->  16.395830154418945\n",
      "loss ->  18.04173469543457\n",
      "loss ->  15.173781394958496\n",
      "loss ->  18.19039535522461\n",
      "loss ->  13.616387367248535\n",
      "loss ->  16.573028564453125\n",
      "loss ->  21.311626434326172\n",
      "loss ->  17.63925552368164\n",
      "loss ->  19.186981201171875\n",
      "loss ->  15.831336975097656\n",
      "loss ->  15.56326675415039\n",
      "loss ->  14.4721040725708\n",
      "loss ->  17.778844833374023\n",
      "loss ->  13.95635986328125\n",
      "loss ->  14.829010963439941\n",
      "loss ->  15.89083480834961\n",
      "loss ->  18.17572021484375\n",
      "loss ->  14.507328033447266\n",
      "loss ->  20.340213775634766\n",
      "loss ->  19.237266540527344\n",
      "loss ->  17.017213821411133\n",
      "loss ->  14.6685152053833\n",
      "loss ->  16.246356964111328\n",
      "loss ->  15.34994125366211\n",
      "loss ->  11.924871444702148\n",
      "loss ->  12.01109504699707\n",
      "loss ->  15.68979549407959\n",
      "loss ->  15.629042625427246\n",
      "loss ->  15.213485717773438\n",
      "loss ->  15.526396751403809\n",
      "loss ->  16.60453987121582\n",
      "loss ->  17.366914749145508\n",
      "loss ->  15.33148193359375\n",
      "loss ->  13.889983177185059\n",
      "loss ->  14.400618553161621\n",
      "loss ->  16.289138793945312\n",
      "loss ->  16.829126358032227\n",
      "loss ->  11.164514541625977\n",
      "loss ->  17.910629272460938\n",
      "loss ->  13.968955993652344\n",
      "loss ->  16.277267456054688\n",
      "loss ->  13.976516723632812\n",
      "loss ->  15.717191696166992\n",
      "loss ->  15.206303596496582\n",
      "loss ->  11.896167755126953\n",
      "loss ->  17.818239212036133\n",
      "loss ->  17.917646408081055\n",
      "loss ->  15.71944522857666\n",
      "loss ->  12.97655200958252\n",
      "loss ->  14.240948677062988\n",
      "loss ->  15.135666847229004\n",
      "loss ->  16.06789779663086\n",
      "loss ->  14.00808048248291\n",
      "loss ->  14.472311973571777\n",
      "loss ->  14.473685264587402\n",
      "loss ->  14.414670944213867\n",
      "loss ->  14.851393699645996\n",
      "loss ->  18.014902114868164\n",
      "loss ->  13.251121520996094\n",
      "loss ->  13.517630577087402\n",
      "loss ->  16.307905197143555\n",
      "loss ->  14.664867401123047\n",
      "loss ->  14.779146194458008\n",
      "loss ->  15.943891525268555\n",
      "loss ->  13.922876358032227\n",
      "loss ->  15.094657897949219\n",
      "loss ->  14.160074234008789\n",
      "loss ->  13.468130111694336\n",
      "loss ->  15.316494941711426\n",
      "loss ->  12.71209716796875\n",
      "loss ->  15.579493522644043\n",
      "loss ->  13.080916404724121\n",
      "loss ->  15.89137077331543\n",
      "loss ->  17.694705963134766\n",
      "loss ->  13.122732162475586\n",
      "loss ->  14.237524032592773\n",
      "loss ->  14.145097732543945\n",
      "loss ->  15.388996124267578\n",
      "loss ->  14.607303619384766\n",
      "loss ->  13.833173751831055\n",
      "loss ->  12.26845932006836\n",
      "loss ->  13.12125015258789\n",
      "loss ->  14.95810317993164\n",
      "loss ->  13.513411521911621\n",
      "loss ->  14.815855026245117\n",
      "loss ->  14.467674255371094\n",
      "loss ->  16.21176528930664\n",
      "loss ->  16.04667091369629\n",
      "loss ->  12.934809684753418\n",
      "loss ->  12.70503044128418\n",
      "loss ->  14.439515113830566\n",
      "loss ->  12.313246726989746\n",
      "loss ->  13.103761672973633\n",
      "loss ->  13.044307708740234\n",
      "loss ->  14.093000411987305\n",
      "loss ->  13.185842514038086\n",
      "loss ->  11.988363265991211\n",
      "loss ->  15.644400596618652\n",
      "loss ->  13.549591064453125\n",
      "loss ->  16.0835018157959\n",
      "loss ->  14.342996597290039\n",
      "loss ->  14.884363174438477\n",
      "loss ->  14.218855857849121\n",
      "loss ->  14.15321159362793\n",
      "loss ->  13.911653518676758\n",
      "loss ->  12.194360733032227\n",
      "loss ->  13.29916000366211\n",
      "loss ->  13.73137378692627\n",
      "loss ->  13.245617866516113\n",
      "loss ->  13.305252075195312\n",
      "loss ->  12.629509925842285\n",
      "loss ->  12.18899917602539\n",
      "loss ->  14.086087226867676\n",
      "loss ->  14.352558135986328\n",
      "loss ->  13.305278778076172\n",
      "loss ->  11.696816444396973\n",
      "loss ->  13.995240211486816\n",
      "loss ->  13.220386505126953\n",
      "loss ->  13.71817398071289\n",
      "loss ->  13.291800498962402\n",
      "loss ->  13.88682746887207\n",
      "loss ->  10.934040069580078\n",
      "loss ->  12.191035270690918\n",
      "loss ->  12.367128372192383\n",
      "loss ->  11.15708065032959\n",
      "loss ->  13.316615104675293\n",
      "loss ->  10.940135955810547\n",
      "loss ->  14.61398983001709\n",
      "loss ->  14.941123008728027\n",
      "loss ->  12.682859420776367\n",
      "loss ->  13.693037986755371\n",
      "loss ->  12.617524147033691\n",
      "loss ->  14.931510925292969\n",
      "loss ->  15.554163932800293\n",
      "loss ->  12.890323638916016\n",
      "loss ->  13.999335289001465\n",
      "loss ->  11.638132095336914\n",
      "loss ->  12.42596435546875\n",
      "loss ->  13.50865364074707\n",
      "loss ->  12.920781135559082\n",
      "loss ->  12.41071891784668\n",
      "loss ->  13.926127433776855\n",
      "loss ->  14.01504898071289\n",
      "loss ->  12.896608352661133\n",
      "loss ->  9.695300102233887\n",
      "loss ->  12.480422973632812\n",
      "loss ->  10.92283821105957\n",
      "loss ->  14.95625114440918\n",
      "loss ->  10.910514831542969\n",
      "loss ->  13.160797119140625\n",
      "loss ->  11.639317512512207\n",
      "loss ->  13.617877960205078\n",
      "loss ->  15.449549674987793\n",
      "loss ->  13.367587089538574\n",
      "loss ->  12.389028549194336\n",
      "loss ->  12.868206977844238\n",
      "loss ->  10.844695091247559\n",
      "loss ->  12.437908172607422\n",
      "loss ->  11.103919982910156\n",
      "loss ->  11.976306915283203\n",
      "loss ->  12.479316711425781\n",
      "loss ->  11.397826194763184\n",
      "loss ->  10.50100326538086\n",
      "loss ->  11.166401863098145\n",
      "loss ->  10.60257339477539\n",
      "loss ->  13.374342918395996\n",
      "loss ->  10.375170707702637\n",
      "loss ->  10.7774076461792\n",
      "loss ->  10.283500671386719\n",
      "loss ->  11.919720649719238\n",
      "loss ->  15.411689758300781\n",
      "loss ->  12.055031776428223\n",
      "loss ->  9.605993270874023\n",
      "loss ->  11.373754501342773\n",
      "loss ->  11.245205879211426\n",
      "loss ->  11.327143669128418\n",
      "loss ->  11.51908016204834\n",
      "loss ->  11.091743469238281\n",
      "loss ->  12.33716106414795\n",
      "loss ->  11.888946533203125\n",
      "loss ->  9.73188304901123\n",
      "loss ->  9.71241569519043\n",
      "loss ->  10.131608963012695\n",
      "loss ->  11.341212272644043\n",
      "loss ->  10.90530014038086\n",
      "loss ->  11.649086952209473\n",
      "loss ->  11.142409324645996\n",
      "loss ->  8.802563667297363\n",
      "loss ->  11.13352108001709\n",
      "loss ->  9.054054260253906\n",
      "loss ->  14.124887466430664\n",
      "loss ->  10.217328071594238\n",
      "loss ->  11.741835594177246\n",
      "loss ->  10.187544822692871\n",
      "loss ->  9.961268424987793\n",
      "loss ->  9.388765335083008\n",
      "loss ->  11.494749069213867\n",
      "loss ->  12.524224281311035\n",
      "loss ->  14.377962112426758\n",
      "loss ->  11.626766204833984\n",
      "loss ->  12.614015579223633\n",
      "loss ->  9.08972454071045\n",
      "loss ->  11.712379455566406\n",
      "loss ->  12.109146118164062\n",
      "loss ->  9.457266807556152\n",
      "loss ->  11.26142406463623\n",
      "loss ->  13.915966987609863\n",
      "loss ->  9.174642562866211\n",
      "loss ->  12.037487983703613\n",
      "loss ->  12.903367042541504\n",
      "loss ->  9.904388427734375\n",
      "loss ->  7.721031665802002\n",
      "loss ->  9.74111270904541\n",
      "loss ->  9.447735786437988\n",
      "loss ->  12.283830642700195\n",
      "loss ->  9.526318550109863\n",
      "loss ->  10.659319877624512\n",
      "loss ->  11.326583862304688\n",
      "loss ->  10.312629699707031\n",
      "loss ->  8.938058853149414\n",
      "loss ->  8.835809707641602\n",
      "loss ->  9.86005687713623\n",
      "loss ->  9.90248966217041\n",
      "loss ->  11.304757118225098\n",
      "loss ->  9.568659782409668\n",
      "loss ->  11.568825721740723\n",
      "loss ->  9.80038070678711\n",
      "loss ->  9.691655158996582\n",
      "loss ->  11.05763053894043\n",
      "loss ->  12.948662757873535\n",
      "loss ->  9.589165687561035\n",
      "loss ->  8.883837699890137\n",
      "loss ->  11.372036933898926\n",
      "loss ->  11.028347969055176\n",
      "loss ->  7.970961570739746\n",
      "loss ->  13.16002368927002\n",
      "loss ->  10.911178588867188\n",
      "loss ->  10.295487403869629\n",
      "loss ->  12.470504760742188\n",
      "loss ->  7.094912528991699\n",
      "loss ->  8.482611656188965\n",
      "loss ->  9.028888702392578\n",
      "loss ->  8.092427253723145\n",
      "loss ->  7.990821361541748\n",
      "loss ->  7.078019142150879\n",
      "loss ->  9.399738311767578\n",
      "loss ->  8.029484748840332\n",
      "loss ->  8.999734878540039\n",
      "loss ->  9.766545295715332\n",
      "loss ->  9.98062801361084\n",
      "loss ->  8.572956085205078\n",
      "loss ->  9.858410835266113\n",
      "loss ->  8.264477729797363\n",
      "loss ->  12.278914451599121\n",
      "loss ->  9.787375450134277\n",
      "loss ->  8.571378707885742\n",
      "loss ->  11.08985710144043\n",
      "loss ->  7.846416473388672\n",
      "loss ->  10.549707412719727\n",
      "loss ->  11.498915672302246\n",
      "loss ->  11.852917671203613\n",
      "loss ->  9.428712844848633\n",
      "loss ->  9.468764305114746\n",
      "loss ->  9.11944580078125\n",
      "loss ->  8.850464820861816\n",
      "loss ->  9.571220397949219\n",
      "loss ->  8.220388412475586\n",
      "loss ->  7.296013832092285\n",
      "loss ->  6.494390964508057\n",
      "loss ->  8.748580932617188\n",
      "loss ->  7.499796390533447\n",
      "loss ->  7.098538875579834\n",
      "loss ->  7.320509910583496\n",
      "loss ->  7.181328296661377\n",
      "loss ->  8.331596374511719\n",
      "loss ->  10.790168762207031\n",
      "loss ->  9.178986549377441\n",
      "loss ->  9.89177131652832\n",
      "loss ->  9.32224178314209\n",
      "loss ->  7.739063739776611\n",
      "loss ->  9.786056518554688\n",
      "loss ->  6.695074558258057\n",
      "loss ->  8.520660400390625\n",
      "loss ->  7.005633354187012\n",
      "loss ->  8.45617961883545\n",
      "loss ->  11.343531608581543\n",
      "loss ->  8.923078536987305\n",
      "loss ->  9.93980884552002\n",
      "loss ->  8.151604652404785\n",
      "loss ->  6.560793399810791\n",
      "loss ->  11.328169822692871\n",
      "loss ->  7.513300895690918\n",
      "loss ->  9.982442855834961\n",
      "loss ->  8.291398048400879\n",
      "loss ->  11.476258277893066\n",
      "loss ->  9.853497505187988\n",
      "loss ->  8.472930908203125\n",
      "loss ->  7.246920585632324\n",
      "loss ->  10.263479232788086\n",
      "loss ->  8.135069847106934\n",
      "loss ->  9.117027282714844\n",
      "loss ->  6.180788040161133\n",
      "loss ->  8.31859302520752\n",
      "loss ->  7.232271671295166\n",
      "loss ->  8.591043472290039\n",
      "loss ->  6.282149791717529\n",
      "loss ->  6.56358528137207\n",
      "loss ->  6.2812323570251465\n",
      "loss ->  8.593642234802246\n",
      "loss ->  6.091433525085449\n",
      "loss ->  7.166039943695068\n",
      "loss ->  7.741825580596924\n",
      "loss ->  7.144514560699463\n",
      "loss ->  8.067452430725098\n",
      "loss ->  8.993461608886719\n",
      "loss ->  7.091543674468994\n",
      "loss ->  7.14292573928833\n",
      "loss ->  6.952244281768799\n",
      "loss ->  7.866815090179443\n",
      "loss ->  6.909946441650391\n",
      "loss ->  6.966777801513672\n",
      "loss ->  5.783840656280518\n",
      "loss ->  7.194055557250977\n",
      "loss ->  5.4701080322265625\n",
      "loss ->  7.765980243682861\n",
      "loss ->  7.314489364624023\n",
      "loss ->  10.31425952911377\n",
      "loss ->  9.112403869628906\n",
      "loss ->  4.741417407989502\n",
      "loss ->  5.671794891357422\n",
      "loss ->  8.251626014709473\n",
      "loss ->  7.240032196044922\n",
      "loss ->  6.23298454284668\n",
      "loss ->  6.462038993835449\n",
      "loss ->  7.392905235290527\n",
      "loss ->  5.554671764373779\n",
      "loss ->  6.526776313781738\n",
      "loss ->  7.050088405609131\n",
      "loss ->  7.030501842498779\n",
      "loss ->  6.6360015869140625\n",
      "loss ->  7.336916923522949\n",
      "loss ->  8.578924179077148\n",
      "loss ->  7.2933735847473145\n",
      "loss ->  6.380667686462402\n",
      "loss ->  4.638234615325928\n",
      "loss ->  6.662641525268555\n",
      "loss ->  6.350212097167969\n",
      "loss ->  6.679390907287598\n",
      "loss ->  5.594588756561279\n",
      "loss ->  6.3916335105896\n",
      "loss ->  8.803977012634277\n",
      "loss ->  5.945135593414307\n",
      "loss ->  7.735429763793945\n",
      "loss ->  7.908132076263428\n",
      "loss ->  6.194385528564453\n",
      "loss ->  6.1187520027160645\n",
      "loss ->  6.591976642608643\n",
      "loss ->  6.904428005218506\n",
      "loss ->  6.99177360534668\n",
      "loss ->  7.221380710601807\n",
      "loss ->  6.075488567352295\n",
      "loss ->  6.14734411239624\n",
      "loss ->  7.388932704925537\n",
      "loss ->  6.375082492828369\n",
      "loss ->  5.498671054840088\n",
      "loss ->  6.574939250946045\n",
      "loss ->  6.944493770599365\n",
      "loss ->  7.867734909057617\n",
      "loss ->  6.282325267791748\n",
      "loss ->  5.9222259521484375\n",
      "loss ->  7.0337300300598145\n",
      "loss ->  6.6570305824279785\n",
      "loss ->  6.748528480529785\n",
      "loss ->  7.0641560554504395\n",
      "loss ->  7.026579856872559\n",
      "loss ->  6.148660659790039\n",
      "loss ->  6.709966659545898\n",
      "loss ->  7.539551734924316\n",
      "loss ->  4.285502910614014\n",
      "loss ->  5.6276679039001465\n",
      "loss ->  4.97702169418335\n",
      "loss ->  4.303259372711182\n",
      "loss ->  6.571039199829102\n",
      "loss ->  6.042716026306152\n",
      "loss ->  5.982400417327881\n",
      "loss ->  7.176673889160156\n",
      "loss ->  5.632782936096191\n",
      "loss ->  5.103381156921387\n",
      "loss ->  6.849313735961914\n",
      "loss ->  4.941697120666504\n",
      "loss ->  4.891777038574219\n",
      "loss ->  5.888917446136475\n",
      "loss ->  4.539280414581299\n",
      "loss ->  6.6285552978515625\n",
      "loss ->  5.44219970703125\n",
      "loss ->  6.2139692306518555\n",
      "loss ->  4.5942254066467285\n",
      "loss ->  6.853444576263428\n",
      "loss ->  5.635793209075928\n",
      "loss ->  4.958737850189209\n",
      "loss ->  5.660822868347168\n",
      "loss ->  4.788396835327148\n",
      "loss ->  5.363152027130127\n",
      "loss ->  5.859928131103516\n",
      "loss ->  4.746262073516846\n",
      "loss ->  5.38639497756958\n",
      "loss ->  6.150785446166992\n",
      "loss ->  5.621089458465576\n",
      "loss ->  6.2742509841918945\n",
      "loss ->  5.749841690063477\n",
      "loss ->  6.13249397277832\n",
      "loss ->  4.908113956451416\n",
      "loss ->  5.548910617828369\n",
      "loss ->  3.71903395652771\n",
      "loss ->  8.277817726135254\n",
      "loss ->  5.2964959144592285\n",
      "loss ->  5.576500415802002\n",
      "loss ->  5.09607458114624\n",
      "loss ->  4.099802494049072\n",
      "loss ->  6.334749698638916\n",
      "loss ->  4.2983503341674805\n",
      "loss ->  5.358541965484619\n",
      "loss ->  4.081562042236328\n",
      "loss ->  4.140092372894287\n",
      "loss ->  6.644270896911621\n",
      "loss ->  5.473698139190674\n",
      "loss ->  3.935293436050415\n",
      "loss ->  5.626701354980469\n",
      "loss ->  4.9142937660217285\n",
      "loss ->  5.499729156494141\n",
      "loss ->  5.19127082824707\n",
      "loss ->  5.209653854370117\n",
      "loss ->  4.868149280548096\n",
      "loss ->  4.965651512145996\n",
      "loss ->  4.943933963775635\n",
      "loss ->  7.381475448608398\n",
      "loss ->  4.217343807220459\n",
      "loss ->  4.503692150115967\n",
      "loss ->  3.8325021266937256\n",
      "loss ->  4.70388650894165\n",
      "loss ->  3.434854507446289\n",
      "loss ->  5.245095729827881\n",
      "loss ->  4.959177494049072\n",
      "loss ->  4.7284722328186035\n",
      "loss ->  4.952047348022461\n",
      "loss ->  4.7923359870910645\n",
      "loss ->  4.194267749786377\n",
      "loss ->  3.8066391944885254\n",
      "loss ->  5.486667633056641\n",
      "loss ->  4.6829023361206055\n",
      "loss ->  5.0828857421875\n",
      "loss ->  4.779535293579102\n",
      "loss ->  3.4063000679016113\n",
      "loss ->  4.801991939544678\n",
      "loss ->  4.345427513122559\n",
      "loss ->  5.486747741699219\n",
      "loss ->  4.610213756561279\n",
      "loss ->  4.9498066902160645\n",
      "loss ->  4.475771427154541\n",
      "loss ->  3.781907081604004\n",
      "loss ->  3.927907943725586\n",
      "loss ->  4.819922924041748\n",
      "loss ->  5.723864555358887\n",
      "loss ->  4.269636154174805\n",
      "loss ->  4.528911590576172\n",
      "loss ->  3.580173969268799\n",
      "loss ->  4.246466636657715\n",
      "loss ->  5.212148189544678\n",
      "loss ->  4.113953590393066\n",
      "loss ->  4.60382604598999\n",
      "loss ->  4.049807071685791\n",
      "loss ->  4.039248943328857\n",
      "loss ->  3.4567372798919678\n",
      "loss ->  4.981328010559082\n",
      "loss ->  5.418502330780029\n",
      "loss ->  4.633685111999512\n",
      "loss ->  4.162659645080566\n",
      "loss ->  3.743671178817749\n",
      "loss ->  4.191739082336426\n",
      "loss ->  3.52824330329895\n",
      "loss ->  3.2926199436187744\n",
      "loss ->  4.583907127380371\n",
      "loss ->  3.836151361465454\n",
      "loss ->  3.744389057159424\n",
      "loss ->  3.6740612983703613\n",
      "loss ->  4.002502918243408\n",
      "loss ->  3.3604328632354736\n",
      "loss ->  4.31881046295166\n",
      "loss ->  4.378140926361084\n",
      "loss ->  4.012716770172119\n",
      "loss ->  4.659698486328125\n",
      "loss ->  4.070191860198975\n",
      "loss ->  3.8091912269592285\n",
      "loss ->  3.7848713397979736\n",
      "loss ->  4.405380725860596\n",
      "loss ->  4.531562328338623\n",
      "loss ->  3.392624855041504\n",
      "loss ->  3.4570634365081787\n",
      "loss ->  3.739344358444214\n",
      "loss ->  5.033804416656494\n",
      "loss ->  2.9473466873168945\n",
      "loss ->  4.773640155792236\n",
      "loss ->  4.188351631164551\n",
      "loss ->  3.6942989826202393\n",
      "loss ->  4.438365459442139\n",
      "loss ->  3.2475192546844482\n",
      "loss ->  3.811600923538208\n",
      "loss ->  3.4561984539031982\n",
      "loss ->  3.603471517562866\n",
      "loss ->  2.7142202854156494\n",
      "loss ->  2.9244275093078613\n",
      "loss ->  3.595895528793335\n",
      "loss ->  3.7299816608428955\n",
      "loss ->  3.707102060317993\n",
      "loss ->  3.9186344146728516\n",
      "loss ->  2.769498109817505\n",
      "loss ->  3.4097986221313477\n",
      "loss ->  3.580122947692871\n",
      "loss ->  4.299206256866455\n",
      "loss ->  3.6392507553100586\n",
      "loss ->  3.1959927082061768\n",
      "loss ->  3.433152914047241\n",
      "loss ->  3.6233999729156494\n",
      "loss ->  2.8755643367767334\n",
      "loss ->  3.374404191970825\n",
      "loss ->  2.9443769454956055\n",
      "loss ->  3.4929113388061523\n",
      "loss ->  3.4541635513305664\n",
      "loss ->  4.477351188659668\n",
      "loss ->  3.5936083793640137\n",
      "loss ->  3.2902162075042725\n",
      "loss ->  3.15596604347229\n",
      "loss ->  3.7574565410614014\n",
      "loss ->  3.1805219650268555\n",
      "loss ->  4.789834976196289\n",
      "loss ->  3.4407784938812256\n",
      "loss ->  3.3529560565948486\n",
      "loss ->  2.8676257133483887\n",
      "loss ->  3.995661973953247\n",
      "loss ->  3.374630928039551\n",
      "loss ->  2.8093647956848145\n",
      "loss ->  3.549271583557129\n",
      "loss ->  3.367893934249878\n",
      "loss ->  4.038920879364014\n",
      "loss ->  3.6423449516296387\n",
      "loss ->  3.585643768310547\n",
      "loss ->  3.3246750831604004\n",
      "loss ->  3.3939297199249268\n",
      "loss ->  2.8470299243927\n",
      "loss ->  2.343517541885376\n",
      "loss ->  3.6781558990478516\n",
      "loss ->  3.127425193786621\n",
      "loss ->  3.4235219955444336\n",
      "loss ->  3.3713347911834717\n",
      "loss ->  3.518523931503296\n",
      "loss ->  4.046712875366211\n",
      "loss ->  3.7712974548339844\n",
      "loss ->  3.304532766342163\n",
      "loss ->  3.9082727432250977\n",
      "loss ->  3.9579946994781494\n",
      "loss ->  2.9534003734588623\n",
      "loss ->  3.3262784481048584\n",
      "loss ->  3.1433181762695312\n",
      "loss ->  4.087808132171631\n",
      "loss ->  2.7311348915100098\n",
      "loss ->  3.0595920085906982\n",
      "loss ->  3.421731948852539\n",
      "loss ->  3.3057308197021484\n",
      "loss ->  2.8398330211639404\n",
      "loss ->  3.1124260425567627\n",
      "loss ->  3.112058162689209\n",
      "loss ->  2.9972450733184814\n",
      "loss ->  3.7323429584503174\n",
      "loss ->  3.801398754119873\n",
      "loss ->  2.780409097671509\n",
      "loss ->  3.5705907344818115\n",
      "loss ->  3.5389833450317383\n",
      "loss ->  3.8961849212646484\n",
      "loss ->  2.937091827392578\n",
      "loss ->  3.100560188293457\n",
      "loss ->  3.3984127044677734\n",
      "loss ->  2.5996038913726807\n",
      "loss ->  3.4667489528656006\n",
      "loss ->  3.4352662563323975\n",
      "loss ->  3.4607813358306885\n",
      "loss ->  3.0176432132720947\n",
      "loss ->  3.297926425933838\n",
      "loss ->  3.6929240226745605\n",
      "loss ->  3.7968733310699463\n",
      "loss ->  2.964613676071167\n",
      "loss ->  2.5865163803100586\n",
      "loss ->  3.08282470703125\n",
      "loss ->  3.670459508895874\n",
      "loss ->  3.724761962890625\n",
      "loss ->  4.452188014984131\n",
      "loss ->  2.9319357872009277\n",
      "loss ->  3.5787956714630127\n",
      "loss ->  3.0900180339813232\n",
      "loss ->  3.301471710205078\n",
      "loss ->  2.8371565341949463\n",
      "loss ->  2.456207752227783\n",
      "loss ->  2.8315608501434326\n",
      "loss ->  3.3575706481933594\n",
      "loss ->  4.511195659637451\n",
      "loss ->  2.7362380027770996\n",
      "loss ->  3.290205955505371\n",
      "loss ->  3.468585729598999\n",
      "loss ->  3.3872711658477783\n",
      "loss ->  3.2879765033721924\n",
      "loss ->  3.382173776626587\n",
      "loss ->  3.2379963397979736\n",
      "loss ->  2.9512932300567627\n",
      "loss ->  3.6681787967681885\n",
      "loss ->  3.2438459396362305\n",
      "loss ->  3.318603038787842\n",
      "loss ->  3.663975715637207\n",
      "loss ->  3.42223858833313\n",
      "loss ->  3.0266966819763184\n",
      "loss ->  2.7942283153533936\n",
      "loss ->  3.0066332817077637\n",
      "loss ->  2.6553006172180176\n",
      "loss ->  3.6487700939178467\n",
      "loss ->  3.592298984527588\n",
      "loss ->  3.676725149154663\n",
      "loss ->  3.338345766067505\n",
      "loss ->  3.1973347663879395\n",
      "loss ->  3.2375645637512207\n",
      "loss ->  3.1847383975982666\n",
      "loss ->  3.230595111846924\n",
      "loss ->  3.417184591293335\n",
      "loss ->  3.300915479660034\n",
      "loss ->  3.1843814849853516\n",
      "loss ->  3.543813943862915\n",
      "loss ->  3.5200624465942383\n",
      "loss ->  3.119281530380249\n",
      "loss ->  3.113934278488159\n",
      "loss ->  3.461308717727661\n",
      "loss ->  3.3194830417633057\n",
      "loss ->  3.365328311920166\n",
      "loss ->  3.1117048263549805\n",
      "loss ->  3.0565690994262695\n",
      "loss ->  2.559920310974121\n",
      "loss ->  3.0924723148345947\n",
      "loss ->  3.402179718017578\n",
      "loss ->  2.953761100769043\n",
      "loss ->  3.2633442878723145\n",
      "loss ->  3.207888126373291\n",
      "loss ->  3.016712188720703\n",
      "loss ->  3.375814437866211\n",
      "loss ->  3.202335834503174\n",
      "loss ->  2.633249521255493\n",
      "loss ->  3.149418592453003\n",
      "loss ->  3.1086277961730957\n",
      "loss ->  2.95998215675354\n",
      "loss ->  3.0271084308624268\n",
      "loss ->  3.3629589080810547\n",
      "loss ->  3.113630771636963\n",
      "loss ->  3.2281486988067627\n",
      "loss ->  3.0255064964294434\n",
      "loss ->  2.8894717693328857\n",
      "loss ->  2.9174675941467285\n",
      "loss ->  2.838369131088257\n",
      "loss ->  2.834378957748413\n",
      "loss ->  3.065382480621338\n",
      "loss ->  2.966217517852783\n",
      "loss ->  3.3027889728546143\n",
      "loss ->  2.5257983207702637\n",
      "loss ->  3.465848922729492\n",
      "loss ->  3.604133367538452\n",
      "loss ->  3.5348610877990723\n",
      "loss ->  3.3225889205932617\n",
      "loss ->  3.1538376808166504\n",
      "loss ->  4.646023273468018\n",
      "loss ->  3.5868237018585205\n",
      "loss ->  2.9186630249023438\n",
      "loss ->  3.491050958633423\n",
      "loss ->  3.113687038421631\n",
      "loss ->  2.9524943828582764\n",
      "loss ->  2.9193670749664307\n",
      "loss ->  3.647359609603882\n",
      "loss ->  2.901064872741699\n",
      "loss ->  3.2916765213012695\n",
      "loss ->  3.0550060272216797\n",
      "loss ->  2.9287211894989014\n",
      "loss ->  2.8677594661712646\n",
      "loss ->  3.41953444480896\n",
      "loss ->  3.1560819149017334\n",
      "loss ->  3.2455642223358154\n",
      "loss ->  2.864534854888916\n",
      "loss ->  2.790327787399292\n",
      "loss ->  2.8271067142486572\n",
      "loss ->  3.1722664833068848\n",
      "loss ->  3.252584218978882\n",
      "loss ->  3.428321361541748\n",
      "loss ->  3.3423547744750977\n",
      "loss ->  3.0147790908813477\n",
      "loss ->  2.9956300258636475\n",
      "loss ->  3.4497475624084473\n",
      "loss ->  2.942319869995117\n",
      "loss ->  3.3765387535095215\n",
      "loss ->  3.5131146907806396\n",
      "loss ->  2.841827869415283\n",
      "loss ->  3.335775136947632\n",
      "loss ->  3.622605800628662\n",
      "loss ->  3.3600881099700928\n",
      "loss ->  3.4475016593933105\n",
      "loss ->  3.4944794178009033\n",
      "loss ->  4.154754638671875\n",
      "loss ->  3.0610790252685547\n",
      "loss ->  2.7459394931793213\n",
      "loss ->  3.5532853603363037\n",
      "loss ->  3.1970388889312744\n",
      "loss ->  3.346609115600586\n",
      "loss ->  4.080436706542969\n",
      "loss ->  3.6505298614501953\n",
      "loss ->  3.971683979034424\n",
      "loss ->  4.224754810333252\n",
      "loss ->  3.651315689086914\n",
      "loss ->  3.2823333740234375\n",
      "loss ->  3.1092493534088135\n",
      "loss ->  3.6273818016052246\n",
      "loss ->  3.785688638687134\n",
      "loss ->  3.5678555965423584\n",
      "loss ->  3.106372594833374\n",
      "loss ->  3.1183106899261475\n",
      "loss ->  3.8163297176361084\n",
      "loss ->  3.7889561653137207\n",
      "loss ->  2.7436540126800537\n",
      "loss ->  3.12646222114563\n",
      "loss ->  3.0158162117004395\n",
      "loss ->  2.8318634033203125\n",
      "loss ->  2.9298174381256104\n",
      "loss ->  3.7142274379730225\n",
      "loss ->  3.601978063583374\n",
      "loss ->  3.342973232269287\n",
      "loss ->  3.5581109523773193\n",
      "loss ->  4.307916641235352\n",
      "loss ->  4.891367435455322\n",
      "loss ->  4.373033046722412\n",
      "loss ->  4.033885955810547\n",
      "loss ->  4.7650299072265625\n",
      "loss ->  3.661655902862549\n",
      "loss ->  3.5511057376861572\n",
      "loss ->  3.6075656414031982\n",
      "loss ->  3.3498148918151855\n",
      "loss ->  3.3661999702453613\n",
      "loss ->  2.996001720428467\n",
      "loss ->  3.891824960708618\n",
      "loss ->  4.385655879974365\n",
      "loss ->  3.7990458011627197\n",
      "loss ->  3.510359287261963\n",
      "loss ->  3.4352293014526367\n",
      "loss ->  3.6000638008117676\n",
      "loss ->  3.36836576461792\n",
      "loss ->  3.249929904937744\n",
      "loss ->  3.1763837337493896\n",
      "loss ->  3.747049570083618\n",
      "loss ->  3.920431613922119\n",
      "loss ->  4.341289043426514\n",
      "loss ->  5.035386562347412\n",
      "loss ->  4.357244491577148\n",
      "loss ->  3.4847536087036133\n",
      "loss ->  4.566347599029541\n",
      "loss ->  4.004057884216309\n",
      "loss ->  3.4076662063598633\n",
      "loss ->  6.271479606628418\n",
      "loss ->  6.360688209533691\n",
      "loss ->  5.99824857711792\n",
      "loss ->  6.135761737823486\n",
      "loss ->  4.690598487854004\n",
      "loss ->  4.53879451751709\n",
      "loss ->  4.8851237297058105\n",
      "loss ->  4.401656627655029\n",
      "loss ->  3.9768831729888916\n",
      "loss ->  4.983678817749023\n",
      "loss ->  4.848952293395996\n",
      "loss ->  4.098555564880371\n",
      "loss ->  4.377414226531982\n",
      "loss ->  4.865326881408691\n",
      "loss ->  4.696806907653809\n",
      "loss ->  4.223474502563477\n",
      "loss ->  5.188929080963135\n",
      "loss ->  5.554508209228516\n",
      "loss ->  5.515365123748779\n",
      "loss ->  4.792817115783691\n",
      "loss ->  4.843986988067627\n",
      "loss ->  4.708397388458252\n",
      "loss ->  4.883633136749268\n",
      "loss ->  3.4980525970458984\n",
      "loss ->  4.675349235534668\n",
      "loss ->  5.801815032958984\n",
      "loss ->  4.366067886352539\n",
      "loss ->  5.337247371673584\n",
      "loss ->  5.10312557220459\n",
      "loss ->  4.354382038116455\n",
      "loss ->  3.8347671031951904\n",
      "loss ->  5.025477409362793\n",
      "loss ->  4.484724521636963\n",
      "loss ->  5.791909217834473\n",
      "loss ->  4.466025352478027\n",
      "loss ->  4.821081638336182\n",
      "loss ->  4.8099260330200195\n",
      "loss ->  5.12992525100708\n",
      "loss ->  4.32957124710083\n",
      "loss ->  5.350406169891357\n",
      "loss ->  5.016955852508545\n",
      "loss ->  4.9823808670043945\n",
      "loss ->  4.361754417419434\n",
      "loss ->  3.620373010635376\n",
      "loss ->  4.269826412200928\n",
      "loss ->  4.563880920410156\n",
      "loss ->  4.132534027099609\n",
      "loss ->  4.732010841369629\n",
      "loss ->  5.468658447265625\n",
      "loss ->  4.586173057556152\n",
      "loss ->  4.363195419311523\n",
      "loss ->  4.019015789031982\n",
      "loss ->  5.168844699859619\n",
      "loss ->  5.905126094818115\n",
      "loss ->  6.3745436668396\n",
      "loss ->  6.298285961151123\n",
      "loss ->  5.154819488525391\n",
      "loss ->  5.682437419891357\n",
      "loss ->  5.510152339935303\n",
      "loss ->  5.108121395111084\n",
      "loss ->  4.890627861022949\n",
      "loss ->  4.324122905731201\n",
      "loss ->  4.651986122131348\n",
      "loss ->  3.6140224933624268\n",
      "loss ->  5.032905578613281\n",
      "loss ->  4.86259651184082\n",
      "loss ->  6.188440322875977\n",
      "loss ->  5.221271991729736\n",
      "loss ->  4.206188201904297\n",
      "loss ->  5.264747619628906\n",
      "loss ->  5.1441450119018555\n",
      "loss ->  4.280539512634277\n",
      "loss ->  6.507497310638428\n",
      "loss ->  4.830593109130859\n",
      "loss ->  5.12986421585083\n",
      "loss ->  7.135893821716309\n",
      "loss ->  5.827548027038574\n",
      "loss ->  5.352293491363525\n",
      "loss ->  6.240703582763672\n",
      "loss ->  4.423871994018555\n",
      "loss ->  7.32139778137207\n",
      "loss ->  5.324742317199707\n",
      "loss ->  4.537837982177734\n",
      "loss ->  5.1318793296813965\n",
      "loss ->  4.883347988128662\n",
      "loss ->  4.403406143188477\n",
      "loss ->  5.972259521484375\n",
      "loss ->  5.866495609283447\n",
      "loss ->  6.337454319000244\n",
      "loss ->  5.7528252601623535\n",
      "loss ->  4.840866565704346\n",
      "loss ->  5.302046298980713\n",
      "loss ->  5.124403476715088\n",
      "loss ->  8.793038368225098\n",
      "loss ->  10.155929565429688\n",
      "loss ->  6.977616786956787\n",
      "loss ->  5.102960109710693\n",
      "loss ->  5.303965091705322\n",
      "loss ->  4.590339660644531\n",
      "loss ->  6.36604642868042\n",
      "loss ->  6.103200435638428\n",
      "loss ->  5.000036716461182\n",
      "loss ->  6.6656646728515625\n",
      "loss ->  6.9042816162109375\n",
      "loss ->  7.926414489746094\n",
      "loss ->  5.597721099853516\n",
      "loss ->  8.224828720092773\n",
      "loss ->  7.029925346374512\n",
      "loss ->  7.072742938995361\n",
      "loss ->  5.808363914489746\n",
      "loss ->  6.740754127502441\n",
      "loss ->  5.388550281524658\n",
      "loss ->  5.062943935394287\n",
      "loss ->  6.965832710266113\n",
      "loss ->  5.560309886932373\n",
      "loss ->  7.695397853851318\n",
      "loss ->  5.790185928344727\n",
      "loss ->  7.200577735900879\n",
      "loss ->  7.7834649085998535\n",
      "loss ->  8.534221649169922\n",
      "loss ->  5.893982410430908\n",
      "loss ->  9.486821174621582\n",
      "loss ->  8.735203742980957\n",
      "loss ->  7.360048294067383\n",
      "loss ->  6.250300884246826\n",
      "loss ->  6.613854885101318\n",
      "loss ->  6.273859977722168\n",
      "loss ->  8.336811065673828\n",
      "loss ->  11.902159690856934\n",
      "loss ->  10.155515670776367\n",
      "loss ->  7.0756516456604\n",
      "loss ->  9.57588005065918\n",
      "loss ->  10.17352294921875\n",
      "loss ->  10.001822471618652\n",
      "loss ->  10.71374225616455\n",
      "loss ->  9.244976997375488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff14c59d9d0>]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# track the learning rates in respect to its loss\n",
    "lr_i = []\n",
    "loss_i = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data.add_(-lr * p.grad)\n",
    "\n",
    "    lr_i.append(lr)\n",
    "    loss_i.append(loss.item())\n",
    "\n",
    "plt.plot(lr_i, loss_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the learning rate was stable between 0.0 and 0.1, and after that it became unstable. So we have narrowed down the learning rate's range in respect to minimized loss function. So we can choose 0.1 safely now and also increase the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    # print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.491891860961914\n"
     ]
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print('loss -> ', loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Decay\n",
    "\n",
    "One more thing to remember is that, once we found our optimized learning rate, we can run the training with it a few times. But then you realize the loss is not moving lower by much. In that case, on a trained NN, you can further reduce the learning rate, by a factor of 10, and continue thee training with same amount of iterations. You can continue this until you reach a plateau. This approach is called learning decay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split - Train/Validate/Test\n",
    "\n",
    "If you increase your parameters to exceed your input size, you will end up over-fitting the data. Meaning that, your NN will memorize the data and outputs exactly what it saw, rather then being creative and output something new. \n",
    "\n",
    "To make sure we are not over-fitting or under-fitting we can split the data into training (80%), validation/dev (10%), and testing sets (10%). \n",
    "\n",
    "Validation is used to find the best hyper-params and settings of the NN. You can try multiple variations to evaluate which one is best for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of chars and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s2i = {s:i+1 for i,s in enumerate(chars)}\n",
    "s2i['.'] = 0\n",
    "i2s = {i:s for s,i in s2i.items()}\n",
    "vocab_size = len(i2s)\n",
    "\n",
    "# MPL model\n",
    "n_emb = 10 # the dimension of the character embedding vector\n",
    "n_hidden = 200 # The number of neurons in the hidden layer of MLP\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_emb),                generator=g, requires_grad=True)\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden),    generator=g, requires_grad=True)\n",
    "b1 = torch.randn((n_hidden),                        generator=g, requires_grad=True)\n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g, requires_grad=True)\n",
    "b2 = torch.randn((vocab_size),                      generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# split the dataset randomly for train, dev and test\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words)) # 80% of the words\n",
    "n2 = int(0.9 * len(words)) # 90% of the words\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train on 80% of the words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # validate on 10% of the words\n",
    "Xtst, Ytst = build_dataset(words[n2:]) # test on 10% of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "def train_NN(X,Y):\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # mini-batch\n",
    "        ix = torch.randint(0, X.shape[0], (batch_size,), generator=g) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "        h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "        # print('loss -> ', loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay after 100k steps\n",
    "        for p in parameters:\n",
    "            p.data.add_(-lr * p.grad)\n",
    "\n",
    "        # track stats\n",
    "        if i % 1000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def getLoss(X,Y):\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "   1000/ 200000: 4.2208\n",
      "   2000/ 200000: 2.9934\n",
      "   3000/ 200000: 2.9829\n",
      "   4000/ 200000: 2.2823\n",
      "   5000/ 200000: 2.4180\n",
      "   6000/ 200000: 2.4152\n",
      "   7000/ 200000: 2.8149\n",
      "   8000/ 200000: 2.5133\n",
      "   9000/ 200000: 2.5542\n",
      "  10000/ 200000: 2.8619\n",
      "  11000/ 200000: 2.0939\n",
      "  12000/ 200000: 2.4401\n",
      "  13000/ 200000: 2.4719\n",
      "  14000/ 200000: 2.9341\n",
      "  15000/ 200000: 2.3488\n",
      "  16000/ 200000: 2.6091\n",
      "  17000/ 200000: 2.7315\n",
      "  18000/ 200000: 2.4622\n",
      "  19000/ 200000: 2.4676\n",
      "  20000/ 200000: 2.5443\n",
      "  21000/ 200000: 2.6354\n",
      "  22000/ 200000: 2.1334\n",
      "  23000/ 200000: 2.2140\n",
      "  24000/ 200000: 3.0568\n",
      "  25000/ 200000: 2.3262\n",
      "  26000/ 200000: 2.5554\n",
      "  27000/ 200000: 2.6473\n",
      "  28000/ 200000: 2.3433\n",
      "  29000/ 200000: 3.2092\n",
      "  30000/ 200000: 2.8801\n",
      "  31000/ 200000: 2.6749\n",
      "  32000/ 200000: 2.5013\n",
      "  33000/ 200000: 2.5535\n",
      "  34000/ 200000: 2.1968\n",
      "  35000/ 200000: 2.5608\n",
      "  36000/ 200000: 2.1742\n",
      "  37000/ 200000: 2.2611\n",
      "  38000/ 200000: 2.9491\n",
      "  39000/ 200000: 2.2258\n",
      "  40000/ 200000: 2.0870\n",
      "  41000/ 200000: 2.6509\n",
      "  42000/ 200000: 2.7214\n",
      "  43000/ 200000: 2.5077\n",
      "  44000/ 200000: 1.9335\n",
      "  45000/ 200000: 1.9379\n",
      "  46000/ 200000: 2.3766\n",
      "  47000/ 200000: 2.0148\n",
      "  48000/ 200000: 2.5559\n",
      "  49000/ 200000: 2.1091\n",
      "  50000/ 200000: 2.4970\n",
      "  51000/ 200000: 2.8095\n",
      "  52000/ 200000: 2.3097\n",
      "  53000/ 200000: 2.5160\n",
      "  54000/ 200000: 2.0923\n",
      "  55000/ 200000: 2.0019\n",
      "  56000/ 200000: 2.0314\n",
      "  57000/ 200000: 1.6952\n",
      "  58000/ 200000: 1.7215\n",
      "  59000/ 200000: 2.2863\n",
      "  60000/ 200000: 2.3531\n",
      "  61000/ 200000: 2.2811\n",
      "  62000/ 200000: 2.5759\n",
      "  63000/ 200000: 2.5570\n",
      "  64000/ 200000: 2.3393\n",
      "  65000/ 200000: 2.1078\n",
      "  66000/ 200000: 2.7216\n",
      "  67000/ 200000: 2.6313\n",
      "  68000/ 200000: 1.8496\n",
      "  69000/ 200000: 2.2077\n",
      "  70000/ 200000: 2.0899\n",
      "  71000/ 200000: 2.4715\n",
      "  72000/ 200000: 2.4842\n",
      "  73000/ 200000: 2.4365\n",
      "  74000/ 200000: 1.9462\n",
      "  75000/ 200000: 2.6709\n",
      "  76000/ 200000: 1.9228\n",
      "  77000/ 200000: 2.4907\n",
      "  78000/ 200000: 2.4087\n",
      "  79000/ 200000: 2.1045\n",
      "  80000/ 200000: 2.2251\n",
      "  81000/ 200000: 2.2868\n",
      "  82000/ 200000: 2.2953\n",
      "  83000/ 200000: 1.8582\n",
      "  84000/ 200000: 2.6504\n",
      "  85000/ 200000: 2.6792\n",
      "  86000/ 200000: 2.4411\n",
      "  87000/ 200000: 2.2592\n",
      "  88000/ 200000: 2.3983\n",
      "  89000/ 200000: 2.3576\n",
      "  90000/ 200000: 2.2938\n",
      "  91000/ 200000: 2.1120\n",
      "  92000/ 200000: 2.6734\n",
      "  93000/ 200000: 1.9373\n",
      "  94000/ 200000: 1.9838\n",
      "  95000/ 200000: 2.2660\n",
      "  96000/ 200000: 2.1820\n",
      "  97000/ 200000: 2.3315\n",
      "  98000/ 200000: 1.9951\n",
      "  99000/ 200000: 2.2031\n",
      " 100000/ 200000: 2.0505\n",
      " 101000/ 200000: 1.8468\n",
      " 102000/ 200000: 2.2829\n",
      " 103000/ 200000: 2.1208\n",
      " 104000/ 200000: 2.2316\n",
      " 105000/ 200000: 2.1003\n",
      " 106000/ 200000: 2.4503\n",
      " 107000/ 200000: 2.1897\n",
      " 108000/ 200000: 2.1861\n",
      " 109000/ 200000: 2.2914\n",
      " 110000/ 200000: 2.3233\n",
      " 111000/ 200000: 2.2460\n",
      " 112000/ 200000: 1.9179\n",
      " 113000/ 200000: 2.4554\n",
      " 114000/ 200000: 2.1865\n",
      " 115000/ 200000: 2.2684\n",
      " 116000/ 200000: 2.1883\n",
      " 117000/ 200000: 2.3121\n",
      " 118000/ 200000: 2.2531\n",
      " 119000/ 200000: 2.3287\n",
      " 120000/ 200000: 1.9138\n",
      " 121000/ 200000: 2.3233\n",
      " 122000/ 200000: 2.2421\n",
      " 123000/ 200000: 2.5611\n",
      " 124000/ 200000: 2.8363\n",
      " 125000/ 200000: 1.9914\n",
      " 126000/ 200000: 2.3399\n",
      " 127000/ 200000: 2.2281\n",
      " 128000/ 200000: 2.3658\n",
      " 129000/ 200000: 2.1814\n",
      " 130000/ 200000: 2.4587\n",
      " 131000/ 200000: 1.8899\n",
      " 132000/ 200000: 1.8009\n",
      " 133000/ 200000: 2.2614\n",
      " 134000/ 200000: 2.1432\n",
      " 135000/ 200000: 2.0901\n",
      " 136000/ 200000: 1.9836\n",
      " 137000/ 200000: 2.0067\n",
      " 138000/ 200000: 2.1657\n",
      " 139000/ 200000: 1.9864\n",
      " 140000/ 200000: 2.1852\n",
      " 141000/ 200000: 1.6911\n",
      " 142000/ 200000: 2.0869\n",
      " 143000/ 200000: 2.3377\n",
      " 144000/ 200000: 2.1629\n",
      " 145000/ 200000: 2.0586\n",
      " 146000/ 200000: 2.0977\n",
      " 147000/ 200000: 2.2760\n",
      " 148000/ 200000: 2.4775\n",
      " 149000/ 200000: 2.4470\n",
      " 150000/ 200000: 2.1685\n",
      " 151000/ 200000: 2.4384\n",
      " 152000/ 200000: 1.9039\n",
      " 153000/ 200000: 2.2755\n",
      " 154000/ 200000: 2.2780\n",
      " 155000/ 200000: 2.0809\n",
      " 156000/ 200000: 1.9429\n",
      " 157000/ 200000: 2.1572\n",
      " 158000/ 200000: 2.0704\n",
      " 159000/ 200000: 2.2378\n",
      " 160000/ 200000: 2.1231\n",
      " 161000/ 200000: 2.0540\n",
      " 162000/ 200000: 1.9258\n",
      " 163000/ 200000: 2.0323\n",
      " 164000/ 200000: 2.1506\n",
      " 165000/ 200000: 2.3690\n",
      " 166000/ 200000: 2.2092\n",
      " 167000/ 200000: 2.3640\n",
      " 168000/ 200000: 1.8544\n",
      " 169000/ 200000: 2.3253\n",
      " 170000/ 200000: 1.8288\n",
      " 171000/ 200000: 2.3161\n",
      " 172000/ 200000: 2.1267\n",
      " 173000/ 200000: 2.2176\n",
      " 174000/ 200000: 2.3718\n",
      " 175000/ 200000: 1.9058\n",
      " 176000/ 200000: 1.8792\n",
      " 177000/ 200000: 2.0864\n",
      " 178000/ 200000: 1.9006\n",
      " 179000/ 200000: 2.1148\n",
      " 180000/ 200000: 2.0834\n",
      " 181000/ 200000: 2.1133\n",
      " 182000/ 200000: 2.2690\n",
      " 183000/ 200000: 2.2654\n",
      " 184000/ 200000: 2.5008\n",
      " 185000/ 200000: 2.2572\n",
      " 186000/ 200000: 1.8644\n",
      " 187000/ 200000: 1.7568\n",
      " 188000/ 200000: 2.1368\n",
      " 189000/ 200000: 2.3998\n",
      " 190000/ 200000: 1.8222\n",
      " 191000/ 200000: 2.2477\n",
      " 192000/ 200000: 2.4226\n",
      " 193000/ 200000: 2.1188\n",
      " 194000/ 200000: 2.0034\n",
      " 195000/ 200000: 2.2530\n",
      " 196000/ 200000: 2.2179\n",
      " 197000/ 200000: 2.1540\n",
      " 198000/ 200000: 1.9826\n",
      " 199000/ 200000: 1.8020\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_NN(Xtr, Ytr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the loss of the training dataset and validation set are too different, that means that our NN hasn't learned much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how the embeddings change after randomly setting them up and training the NN, run below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.1294455528259277\n",
      "loss ->  2.173779249191284\n"
     ]
    }
   ],
   "source": [
    "# training data loss\n",
    "getLoss(Xtr, Ytr)\n",
    "# validation loss\n",
    "getLoss(Xdev, Ydev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the embeddings\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), i2s[i], ha='center', va='center', color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naitabree.\n",
      "jaezi.\n",
      "aje.\n",
      "maxanley.\n",
      "lei.\n",
      "jacaraanly.\n",
      "erilameenah.\n",
      "ell.\n",
      "adalisa.\n",
      "ere.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 1000)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] \n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1) \n",
    "        logits = h @ W2 + b2 \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0: # stop at the end of the word\n",
    "            break\n",
    "    print(''.join(i2s[i] for i in out))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at this point you can start working on the possible configuration tunings and improve your loos and validations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
