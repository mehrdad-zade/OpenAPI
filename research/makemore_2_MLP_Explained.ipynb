{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrej Karpathy's /makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://www.youtube.com/watch?v=TCH_1BHY58I\n",
    "\n",
    "https://github.com/karpathy/makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bigram char level model, we only used two chars which created a 27x27 data space. if we move deeper in this approach to enhance the loss function and the model itself, the only avenue was to explor adding more dimensions, i.e. 27x27x27. however this path suddenly explodes in terms of data and parameters that we want to use for this model.\n",
    "\n",
    "Therefore, we need to explore a better model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)\n",
    "#### Bengio et al. 2003\n",
    "\n",
    "This is another char level model to predict the next char, however the paper is based on word predictions. \n",
    "\n",
    "The proposed approach is to take 'w' number of words, and associate to each word, 'm' number of feature vectors. Meaning that, each word is embedded in a 'm' dimensional feature space. Initially these words are initialized randomly but later we'll tune them using backpropagation. \n",
    "\n",
    "To imagine this approach, think about words that are similar or synonyms. They will end up in the same part of the space. And those that are different will be separated. \n",
    "\n",
    "The modeling approach is similar to the NN approach for Bigram. They use multi-layer NN to predict the next words, given the previous words. To train the NN, they ```maximize the log-likelihood of the training data```.\n",
    "\n",
    "Let's look at an ```example``` for this approach. Assume, we are not given the sentence \"A dog was running in a room\". But now for testing the model we are providing it with \"A dog was running in a ...\" and expecting the model to fill in the blank. Since it hasn't seen this exact sentence, we call it, ```out of distribution```. However, MLP doesn't need to have seen the exact words to predict 'room' for the blank. Because it might have seen \"The dog was running in a room\" and based on the learnings, it has put the embeddings of 'The' and 'A' near by each other in the space. So now that we are asking it to fill the blank based on \"A dog was running in a ...\", it will match it up with \"The dog was running in a room\". This is called ```knowledge transfer```.\n",
    "\n",
    "Let's look at the ```architecture``` of this approach. \n",
    "\n",
    "Assume the NN's input, takes 3 previous-words. And the output is the fourth word. Each of the incoming words, will go through a look-up table, to match up the corresponding embedding ('m' feature vector) for that word. So there will be $3 \\times m$ neurons holding the 3 words. \n",
    "\n",
    "Then we need to build a hidden layer. The size is a ```hyper-parameter```. Meaning that, we need to come up with the right size based on try-error. So all the input neurons goes into the hidden layer. And there will be a ```tanh``` function applied for non-linearity. \n",
    "\n",
    "The output layer is a huge one, because the number of neurons is equivalent to $w$, the number of words in our data set. All the neurons in the hidden layer are connected to the output neurons. That's why there will be lots of params in between these two layers, and therefore, it's going to be computationally expensive. On top of the output layer we have ```softmax``` (exponentiate the logits and normalize, so that it will sum up to 1). This way, we'll get a nice probability distribution for the next word in the sequence. \n",
    "\n",
    "During training, because we have xs and ys, we will get the probability for each x and minimize the NN's loss by improving the parameters. The optimization used here is also ```backpropagation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# read from another package while we are in a separate package\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "file_path = os.path.join(parent_directory, 'opensource/makemore', 'names.txt')\n",
    "\n",
    "words = open(file_path, 'r').read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of chars and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s2i = {s:i+1 for i,s in enumerate(chars)}\n",
    "s2i['.'] = 0\n",
    "i2s = {i:s for s,i in s2i.items()}\n",
    "print(i2s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words[:5]: # the examples we can generate from the first 5 words\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(i2s[i] for i in context), '--->', i2s[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Embeddings\n",
    "\n",
    "the paper used 70000 words with 30 embeddings. we have 27 chars, so we'll go with 2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3023, 0.4651])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3023,  0.4651],\n",
       "        [ 0.5387, -0.0413],\n",
       "        [-1.8265, -0.3889]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve embeddings with a list of lookups\n",
    "C[[5,6,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 1.3023,  0.4651]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 1.3023,  0.4651],\n",
       "         [-0.1166,  0.3032]],\n",
       "\n",
       "        [[ 1.3023,  0.4651],\n",
       "         [-0.1166,  0.3032],\n",
       "         [-0.1166,  0.3032]],\n",
       "\n",
       "        [[-0.1166,  0.3032],\n",
       "         [-0.1166,  0.3032],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [-0.6195,  0.3101]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [-0.6195,  0.3101],\n",
       "         [-0.5541, -1.7226]],\n",
       "\n",
       "        [[-0.6195,  0.3101],\n",
       "         [-0.5541, -1.7226],\n",
       "         [ 0.6410,  1.1742]],\n",
       "\n",
       "        [[-0.5541, -1.7226],\n",
       "         [ 0.6410,  1.1742],\n",
       "         [ 0.3023, -0.8974]],\n",
       "\n",
       "        [[ 0.6410,  1.1742],\n",
       "         [ 0.3023, -0.8974],\n",
       "         [ 0.6410,  1.1742]],\n",
       "\n",
       "        [[ 0.3023, -0.8974],\n",
       "         [ 0.6410,  1.1742],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.6645,  0.7288],\n",
       "         [ 0.3023, -0.8974]],\n",
       "\n",
       "        [[ 0.6645,  0.7288],\n",
       "         [ 0.3023, -0.8974],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.6410,  1.1742]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.6410,  1.1742],\n",
       "         [-1.6627,  0.9340]],\n",
       "\n",
       "        [[ 0.6410,  1.1742],\n",
       "         [-1.6627,  0.9340],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[-1.6627,  0.9340],\n",
       "         [ 0.6645,  0.7288],\n",
       "         [-0.9615, -0.0783]],\n",
       "\n",
       "        [[ 0.6645,  0.7288],\n",
       "         [-0.9615, -0.0783],\n",
       "         [ 1.3023,  0.4651]],\n",
       "\n",
       "        [[-0.9615, -0.0783],\n",
       "         [ 1.3023,  0.4651],\n",
       "         [-0.5541, -1.7226]],\n",
       "\n",
       "        [[ 1.3023,  0.4651],\n",
       "         [-0.5541, -1.7226],\n",
       "         [-0.5541, -1.7226]],\n",
       "\n",
       "        [[-0.5541, -1.7226],\n",
       "         [-0.5541, -1.7226],\n",
       "         [ 0.6645,  0.7288]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [ 0.4650, -0.2010],\n",
       "         [-1.6627,  0.9340]],\n",
       "\n",
       "        [[ 0.4650, -0.2010],\n",
       "         [-1.6627,  0.9340],\n",
       "         [-0.6195,  0.3101]],\n",
       "\n",
       "        [[-1.6627,  0.9340],\n",
       "         [-0.6195,  0.3101],\n",
       "         [-0.8039,  1.2122]],\n",
       "\n",
       "        [[-0.6195,  0.3101],\n",
       "         [-0.8039,  1.2122],\n",
       "         [-1.8265, -0.3889]],\n",
       "\n",
       "        [[-0.8039,  1.2122],\n",
       "         [-1.8265, -0.3889],\n",
       "         [ 0.6410,  1.1742]],\n",
       "\n",
       "        [[-1.8265, -0.3889],\n",
       "         [ 0.6410,  1.1742],\n",
       "         [ 0.6645,  0.7288]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# therefore this works\n",
    "emb = C[X]\n",
    "emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_hyperparameter_size = 100\n",
    "num_of_words = 3\n",
    "num_of_embeddings = 2\n",
    "num_of_inputs = num_of_words * num_of_embeddings\n",
    "\n",
    "w1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size))\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wee want to setup the tensor's shapes in such a way that ```emb @ w1 + b1``` would work.\n",
    "\n",
    "http://blog.ezyang.com/2019/05/pytorch-internals/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8953, -1.6409, -0.8250,  ...,  2.2597,  1.3783,  0.8145],\n",
       "        [-0.9660, -2.4800,  1.8895,  ...,  3.9144,  0.9641,  1.3991],\n",
       "        [-3.2081, -3.9777, -2.2143,  ...,  1.2977,  2.3624,  2.4231],\n",
       "        ...,\n",
       "        [-3.9781,  2.3380, -6.9441,  ..., -2.1299,  0.5283, -0.0794],\n",
       "        [ 3.7887,  5.4175,  3.0423,  ..., -0.4846,  0.2327,  0.2111],\n",
       "        [-3.1855, -3.7977,  1.1798,  ...,  4.1962,  1.2162,  2.1287]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_size = emb.shape[0] # or use -1 for pytorch to figure it out\n",
    "emb.view(x_size, num_of_inputs) @ w1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9558, -0.9276, -0.6778,  ...,  0.9784,  0.8806,  0.6721],\n",
       "        [-0.7469, -0.9861,  0.9553,  ...,  0.9992,  0.7461,  0.8852],\n",
       "        [-0.9967, -0.9993, -0.9764,  ...,  0.8611,  0.9824,  0.9844],\n",
       "        ...,\n",
       "        [-0.9993,  0.9815, -1.0000,  ..., -0.9721,  0.4841, -0.0793],\n",
       "        [ 0.9990,  1.0000,  0.9955,  ..., -0.4499,  0.2286,  0.2080],\n",
       "        [-0.9966, -0.9990,  0.8274,  ...,  0.9995,  0.8385,  0.9721]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ w1 + b1) # added tanh to bring all the values between -1 and 1 for non-linearity\n",
    "h "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output layer\n",
    "w2 = torch.randn((hidden_layer_hyperparameter_size, 27))\n",
    "b2 = torch.randn((27))\n",
    "logits = h @ w2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # get fake counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize to get the probabilities\n",
    "probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proof of normalized probs is to check if every row sums up to =1\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4305e-04, 6.4292e-01, 2.0060e-06, 1.7346e-01, 1.8115e-05, 6.0897e-03,\n",
       "        1.8932e-06, 5.3520e-06, 6.0917e-03, 2.7039e-05, 4.8216e-09, 3.3954e-08,\n",
       "        1.7566e-02, 7.4114e-03, 6.1974e-01, 5.5244e-13, 7.1050e-04, 2.7553e-04,\n",
       "        1.0000e+00, 4.2845e-07, 3.3437e-13, 6.6014e-11, 4.8313e-13, 2.2374e-08,\n",
       "        1.3035e-17, 1.7353e-04, 1.1354e-06, 2.6870e-06, 1.8639e-12, 2.5290e-04,\n",
       "        4.5072e-08, 2.4664e-08])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Small Dataset (over-fitting)\n",
    "\n",
    "we haven't trained the NN yet so the probabilities are far from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.3317)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to minimize this loss\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp() # (32, 27)\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of using the above code we can also use pytorch library to arrive at the same loss, which is more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's setup the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  17.769712448120117\n",
      "loss ->  13.656402587890625\n",
      "loss ->  11.298768997192383\n",
      "loss ->  9.4524564743042\n",
      "loss ->  7.984262943267822\n",
      "loss ->  6.891320705413818\n",
      "loss ->  6.100014686584473\n",
      "loss ->  5.452036380767822\n",
      "loss ->  4.898151874542236\n",
      "loss ->  4.414663791656494\n",
      "loss ->  3.985849380493164\n",
      "loss ->  3.6028308868408203\n",
      "loss ->  3.2621421813964844\n",
      "loss ->  2.961381435394287\n",
      "loss ->  2.6982975006103516\n",
      "loss ->  2.469712734222412\n",
      "loss ->  2.271660566329956\n",
      "loss ->  2.101283550262451\n",
      "loss ->  1.9571772813796997\n",
      "loss ->  1.8374857902526855\n",
      "loss ->  1.7380967140197754\n",
      "loss ->  1.653511643409729\n",
      "loss ->  1.5790899991989136\n",
      "loss ->  1.5117664337158203\n",
      "loss ->  1.449604868888855\n",
      "loss ->  1.3913118839263916\n",
      "loss ->  1.3359922170639038\n",
      "loss ->  1.2830528020858765\n",
      "loss ->  1.2321910858154297\n",
      "loss ->  1.18338143825531\n",
      "loss ->  1.136798620223999\n",
      "loss ->  1.092664122581482\n",
      "loss ->  1.051092267036438\n",
      "loss ->  1.0120269060134888\n",
      "loss ->  0.9752703309059143\n",
      "loss ->  0.940556526184082\n",
      "loss ->  0.9076125621795654\n",
      "loss ->  0.8761920928955078\n",
      "loss ->  0.8460890650749207\n",
      "loss ->  0.8171356916427612\n",
      "loss ->  0.7891989946365356\n",
      "loss ->  0.7621745467185974\n",
      "loss ->  0.7359813451766968\n",
      "loss ->  0.7105579972267151\n",
      "loss ->  0.6858609914779663\n",
      "loss ->  0.6618651747703552\n",
      "loss ->  0.6385655403137207\n",
      "loss ->  0.6159816980361938\n",
      "loss ->  0.5941658616065979\n",
      "loss ->  0.5732104182243347\n",
      "loss ->  0.5532563924789429\n",
      "loss ->  0.5344882011413574\n",
      "loss ->  0.5171165466308594\n",
      "loss ->  0.5013313293457031\n",
      "loss ->  0.48724260926246643\n",
      "loss ->  0.4748404622077942\n",
      "loss ->  0.4639977812767029\n",
      "loss ->  0.45451444387435913\n",
      "loss ->  0.44617098569869995\n",
      "loss ->  0.4387663006782532\n",
      "loss ->  0.4321332573890686\n",
      "loss ->  0.42613884806632996\n",
      "loss ->  0.42067989706993103\n",
      "loss ->  0.4156752824783325\n",
      "loss ->  0.4110615849494934\n",
      "loss ->  0.4067871868610382\n",
      "loss ->  0.402810662984848\n",
      "loss ->  0.3990972936153412\n",
      "loss ->  0.3956180214881897\n",
      "loss ->  0.39234787225723267\n",
      "loss ->  0.3892652988433838\n",
      "loss ->  0.38635194301605225\n",
      "loss ->  0.38359174132347107\n",
      "loss ->  0.38096994161605835\n",
      "loss ->  0.37847423553466797\n",
      "loss ->  0.37609291076660156\n",
      "loss ->  0.3738164007663727\n",
      "loss ->  0.3716350197792053\n",
      "loss ->  0.3695409893989563\n",
      "loss ->  0.3675268888473511\n",
      "loss ->  0.3655855357646942\n",
      "loss ->  0.3637113869190216\n",
      "loss ->  0.3618983030319214\n",
      "loss ->  0.36014166474342346\n",
      "loss ->  0.3584362864494324\n",
      "loss ->  0.3567781150341034\n",
      "loss ->  0.3551627993583679\n",
      "loss ->  0.35358694195747375\n",
      "loss ->  0.35204702615737915\n",
      "loss ->  0.3505397439002991\n",
      "loss ->  0.3490622937679291\n",
      "loss ->  0.3476122319698334\n",
      "loss ->  0.34618666768074036\n",
      "loss ->  0.34478360414505005\n",
      "loss ->  0.3434009850025177\n",
      "loss ->  0.3420368432998657\n",
      "loss ->  0.34068992733955383\n",
      "loss ->  0.33935868740081787\n",
      "loss ->  0.33804193139076233\n",
      "loss ->  0.3367388844490051\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we reached above optimized loss with 5 words and 32 examples. so let's pull in all the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training On Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words: # the examples we can generate from the first 5 words\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  19.505229949951172\n",
      "loss ->  17.084482192993164\n",
      "loss ->  15.776532173156738\n",
      "loss ->  14.833340644836426\n",
      "loss ->  14.002605438232422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  13.253263473510742\n",
      "loss ->  12.57991886138916\n",
      "loss ->  11.983102798461914\n",
      "loss ->  11.47049331665039\n",
      "loss ->  11.05185604095459\n",
      "loss ->  10.709586143493652\n",
      "loss ->  10.407631874084473\n",
      "loss ->  10.127808570861816\n",
      "loss ->  9.864364624023438\n",
      "loss ->  9.614501953125\n",
      "loss ->  9.376439094543457\n",
      "loss ->  9.148944854736328\n",
      "loss ->  8.931110382080078\n",
      "loss ->  8.722230911254883\n",
      "loss ->  8.521749496459961\n",
      "loss ->  8.329227447509766\n",
      "loss ->  8.144325256347656\n",
      "loss ->  7.966791152954102\n",
      "loss ->  7.796450614929199\n",
      "loss ->  7.633185386657715\n",
      "loss ->  7.476908206939697\n",
      "loss ->  7.327521800994873\n",
      "loss ->  7.184885501861572\n",
      "loss ->  7.048792362213135\n",
      "loss ->  6.9189534187316895\n",
      "loss ->  6.795018196105957\n",
      "loss ->  6.6766037940979\n",
      "loss ->  6.563319206237793\n",
      "loss ->  6.4547905921936035\n",
      "loss ->  6.350669860839844\n",
      "loss ->  6.250644207000732\n",
      "loss ->  6.154432773590088\n",
      "loss ->  6.06178617477417\n",
      "loss ->  5.972483158111572\n",
      "loss ->  5.88632869720459\n",
      "loss ->  5.8031487464904785\n",
      "loss ->  5.722784519195557\n",
      "loss ->  5.6450958251953125\n",
      "loss ->  5.569947719573975\n",
      "loss ->  5.497213840484619\n",
      "loss ->  5.42678165435791\n",
      "loss ->  5.358536720275879\n",
      "loss ->  5.29237699508667\n",
      "loss ->  5.228204727172852\n",
      "loss ->  5.165929317474365\n",
      "loss ->  5.105469226837158\n",
      "loss ->  5.046748638153076\n",
      "loss ->  4.989700794219971\n",
      "loss ->  4.934263229370117\n",
      "loss ->  4.880381107330322\n",
      "loss ->  4.828007221221924\n",
      "loss ->  4.777096748352051\n",
      "loss ->  4.727611064910889\n",
      "loss ->  4.6795148849487305\n",
      "loss ->  4.6327805519104\n",
      "loss ->  4.587380409240723\n",
      "loss ->  4.543290138244629\n",
      "loss ->  4.500492095947266\n",
      "loss ->  4.458968639373779\n",
      "loss ->  4.418702125549316\n",
      "loss ->  4.3796772956848145\n",
      "loss ->  4.341879844665527\n",
      "loss ->  4.305293560028076\n",
      "loss ->  4.269900798797607\n",
      "loss ->  4.235682010650635\n",
      "loss ->  4.202613830566406\n",
      "loss ->  4.170671463012695\n",
      "loss ->  4.139826774597168\n",
      "loss ->  4.110045909881592\n",
      "loss ->  4.081294059753418\n",
      "loss ->  4.05353307723999\n",
      "loss ->  4.026722431182861\n",
      "loss ->  4.000819206237793\n",
      "loss ->  3.9757814407348633\n",
      "loss ->  3.9515655040740967\n",
      "loss ->  3.9281296730041504\n",
      "loss ->  3.9054324626922607\n",
      "loss ->  3.883435010910034\n",
      "loss ->  3.862100124359131\n",
      "loss ->  3.841392993927002\n",
      "loss ->  3.821281671524048\n",
      "loss ->  3.8017358779907227\n",
      "loss ->  3.782728672027588\n",
      "loss ->  3.7642343044281006\n",
      "loss ->  3.7462313175201416\n",
      "loss ->  3.728696823120117\n",
      "loss ->  3.7116124629974365\n",
      "loss ->  3.694960594177246\n",
      "loss ->  3.6787242889404297\n",
      "loss ->  3.662888288497925\n",
      "loss ->  3.6474387645721436\n",
      "loss ->  3.6323626041412354\n",
      "loss ->  3.6176466941833496\n",
      "loss ->  3.6032800674438477\n",
      "loss ->  3.5892508029937744\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] \n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch\n",
    "\n",
    "The reason why above training takes a lot of time is that, it's doing a forward and backward pass on a large dataset. To optimize the training process we introduce mini-batches. \n",
    "\n",
    "With mini-batch, we do the forward and backward passes on a smaller dataset. Once optimized we move to another batch for training and optimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  3.1884219646453857\n",
      "loss ->  2.6396396160125732\n",
      "loss ->  3.610919237136841\n",
      "loss ->  3.5851998329162598\n",
      "loss ->  3.5940868854522705\n",
      "loss ->  3.5498619079589844\n",
      "loss ->  3.665818929672241\n",
      "loss ->  3.285625696182251\n",
      "loss ->  2.651365280151367\n",
      "loss ->  3.231239080429077\n",
      "loss ->  3.1525485515594482\n",
      "loss ->  4.06334114074707\n",
      "loss ->  3.3632075786590576\n",
      "loss ->  3.2622134685516357\n",
      "loss ->  3.379981756210327\n",
      "loss ->  2.9979629516601562\n",
      "loss ->  3.654086112976074\n",
      "loss ->  3.3383402824401855\n",
      "loss ->  4.203357696533203\n",
      "loss ->  3.4861645698547363\n",
      "loss ->  3.7279107570648193\n",
      "loss ->  4.203322410583496\n",
      "loss ->  3.948085308074951\n",
      "loss ->  3.5631096363067627\n",
      "loss ->  3.4001104831695557\n",
      "loss ->  3.1193482875823975\n",
      "loss ->  3.2921817302703857\n",
      "loss ->  3.7679190635681152\n",
      "loss ->  2.769350290298462\n",
      "loss ->  3.3689568042755127\n",
      "loss ->  3.662027359008789\n",
      "loss ->  3.27282452583313\n",
      "loss ->  3.299266815185547\n",
      "loss ->  3.092632532119751\n",
      "loss ->  3.3296799659729004\n",
      "loss ->  3.416055202484131\n",
      "loss ->  3.5944199562072754\n",
      "loss ->  3.970398187637329\n",
      "loss ->  3.849419116973877\n",
      "loss ->  3.0856738090515137\n",
      "loss ->  2.746652126312256\n",
      "loss ->  3.629901170730591\n",
      "loss ->  3.9466781616210938\n",
      "loss ->  3.4020891189575195\n",
      "loss ->  3.460392475128174\n",
      "loss ->  3.4067747592926025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  3.2446959018707275\n",
      "loss ->  3.3627681732177734\n",
      "loss ->  3.1986143589019775\n",
      "loss ->  3.773404598236084\n",
      "loss ->  3.751447916030884\n",
      "loss ->  3.195234537124634\n",
      "loss ->  3.9484617710113525\n",
      "loss ->  3.4398956298828125\n",
      "loss ->  4.073247909545898\n",
      "loss ->  2.7067341804504395\n",
      "loss ->  3.398693084716797\n",
      "loss ->  3.573239803314209\n",
      "loss ->  3.8909106254577637\n",
      "loss ->  3.0182156562805176\n",
      "loss ->  3.5211079120635986\n",
      "loss ->  3.1365466117858887\n",
      "loss ->  3.2156403064727783\n",
      "loss ->  3.786299467086792\n",
      "loss ->  3.3963048458099365\n",
      "loss ->  3.1231532096862793\n",
      "loss ->  3.041530132293701\n",
      "loss ->  3.753558874130249\n",
      "loss ->  3.0077710151672363\n",
      "loss ->  3.538760185241699\n",
      "loss ->  2.8724145889282227\n",
      "loss ->  2.533097982406616\n",
      "loss ->  3.139467716217041\n",
      "loss ->  2.7051522731781006\n",
      "loss ->  3.2663731575012207\n",
      "loss ->  3.3483805656433105\n",
      "loss ->  3.2062134742736816\n",
      "loss ->  2.580139636993408\n",
      "loss ->  3.5793402194976807\n",
      "loss ->  3.56064510345459\n",
      "loss ->  3.064148426055908\n",
      "loss ->  3.37174129486084\n",
      "loss ->  2.757254123687744\n",
      "loss ->  3.1136226654052734\n",
      "loss ->  2.769501209259033\n",
      "loss ->  3.1848230361938477\n",
      "loss ->  2.860508680343628\n",
      "loss ->  3.313001871109009\n",
      "loss ->  2.8971197605133057\n",
      "loss ->  3.0383048057556152\n",
      "loss ->  3.1936771869659424\n",
      "loss ->  2.9140679836273193\n",
      "loss ->  3.1891515254974365\n",
      "loss ->  2.809349775314331\n",
      "loss ->  2.6550791263580322\n",
      "loss ->  2.7697396278381348\n",
      "loss ->  4.174935817718506\n",
      "loss ->  2.8219165802001953\n",
      "loss ->  3.316042184829712\n",
      "loss ->  2.894226551055908\n",
      "loss ->  2.9475059509277344\n",
      "loss ->  3.111938714981079\n",
      "loss ->  3.182497978210449\n",
      "loss ->  3.1943283081054688\n",
      "loss ->  2.66216778755188\n",
      "loss ->  2.2956600189208984\n",
      "loss ->  2.8461575508117676\n",
      "loss ->  2.9574522972106934\n",
      "loss ->  3.3885762691497803\n",
      "loss ->  3.4875853061676025\n",
      "loss ->  4.030627727508545\n",
      "loss ->  3.1062958240509033\n",
      "loss ->  3.2410733699798584\n",
      "loss ->  4.023406505584717\n",
      "loss ->  3.846304416656494\n",
      "loss ->  2.436171054840088\n",
      "loss ->  2.2930173873901367\n",
      "loss ->  2.4585840702056885\n",
      "loss ->  2.8561348915100098\n",
      "loss ->  2.879413604736328\n",
      "loss ->  2.5367624759674072\n",
      "loss ->  3.423118829727173\n",
      "loss ->  2.643129825592041\n",
      "loss ->  2.6800341606140137\n",
      "loss ->  2.7455849647521973\n",
      "loss ->  3.174163579940796\n",
      "loss ->  2.6006948947906494\n",
      "loss ->  3.1981399059295654\n",
      "loss ->  3.070537805557251\n",
      "loss ->  3.1996920108795166\n",
      "loss ->  3.15675950050354\n",
      "loss ->  3.390354871749878\n",
      "loss ->  2.8403079509735107\n",
      "loss ->  3.1100542545318604\n",
      "loss ->  2.771015167236328\n",
      "loss ->  2.9293763637542725\n",
      "loss ->  2.911219596862793\n",
      "loss ->  3.074050188064575\n",
      "loss ->  2.720897912979126\n",
      "loss ->  3.4712343215942383\n",
      "loss ->  2.6515800952911377\n",
      "loss ->  2.887242317199707\n",
      "loss ->  3.1490163803100586\n",
      "loss ->  2.6007657051086426\n",
      "loss ->  2.694641351699829\n",
      "loss ->  2.7010905742645264\n",
      "loss ->  2.93534517288208\n",
      "loss ->  3.058645486831665\n",
      "loss ->  2.506892442703247\n",
      "loss ->  2.9854133129119873\n",
      "loss ->  3.2644195556640625\n",
      "loss ->  2.8095688819885254\n",
      "loss ->  3.0585672855377197\n",
      "loss ->  3.525722026824951\n",
      "loss ->  3.3861405849456787\n",
      "loss ->  2.689371347427368\n",
      "loss ->  2.467109203338623\n",
      "loss ->  3.167334794998169\n",
      "loss ->  2.579547643661499\n",
      "loss ->  3.2401437759399414\n",
      "loss ->  3.3486292362213135\n",
      "loss ->  2.548163414001465\n",
      "loss ->  2.8495676517486572\n",
      "loss ->  2.894005060195923\n",
      "loss ->  3.087437152862549\n",
      "loss ->  3.5540363788604736\n",
      "loss ->  2.633986473083496\n",
      "loss ->  3.0208609104156494\n",
      "loss ->  4.600262641906738\n",
      "loss ->  2.9480478763580322\n",
      "loss ->  2.835024356842041\n",
      "loss ->  2.7495243549346924\n",
      "loss ->  3.0130722522735596\n",
      "loss ->  3.3216586112976074\n",
      "loss ->  2.6460046768188477\n",
      "loss ->  3.3777267932891846\n",
      "loss ->  2.9705710411071777\n",
      "loss ->  3.182180404663086\n",
      "loss ->  2.814758062362671\n",
      "loss ->  3.321040630340576\n",
      "loss ->  3.0412473678588867\n",
      "loss ->  3.5735926628112793\n",
      "loss ->  2.7726128101348877\n",
      "loss ->  2.8431551456451416\n",
      "loss ->  3.2059249877929688\n",
      "loss ->  3.5240988731384277\n",
      "loss ->  2.913649320602417\n",
      "loss ->  3.3019495010375977\n",
      "loss ->  2.709979772567749\n",
      "loss ->  2.6838536262512207\n",
      "loss ->  2.762758493423462\n",
      "loss ->  2.875218629837036\n",
      "loss ->  2.7491748332977295\n",
      "loss ->  3.048511266708374\n",
      "loss ->  3.2028143405914307\n",
      "loss ->  2.839552402496338\n",
      "loss ->  2.4386842250823975\n",
      "loss ->  2.795872688293457\n",
      "loss ->  2.8471641540527344\n",
      "loss ->  3.0111019611358643\n",
      "loss ->  2.91619873046875\n",
      "loss ->  3.04640793800354\n",
      "loss ->  2.706669330596924\n",
      "loss ->  3.0186033248901367\n",
      "loss ->  2.8007192611694336\n",
      "loss ->  2.908348560333252\n",
      "loss ->  2.486886501312256\n",
      "loss ->  3.4172661304473877\n",
      "loss ->  3.0179014205932617\n",
      "loss ->  3.2362892627716064\n",
      "loss ->  3.059767484664917\n",
      "loss ->  2.649406909942627\n",
      "loss ->  2.8669371604919434\n",
      "loss ->  2.866820812225342\n",
      "loss ->  3.151890754699707\n",
      "loss ->  3.264021873474121\n",
      "loss ->  2.7625088691711426\n",
      "loss ->  2.5864200592041016\n",
      "loss ->  3.3491227626800537\n",
      "loss ->  3.174091339111328\n",
      "loss ->  2.9277780055999756\n",
      "loss ->  3.1868505477905273\n",
      "loss ->  2.9684360027313232\n",
      "loss ->  3.3601834774017334\n",
      "loss ->  3.4077839851379395\n",
      "loss ->  3.160550117492676\n",
      "loss ->  2.9752306938171387\n",
      "loss ->  2.84891414642334\n",
      "loss ->  2.911073684692383\n",
      "loss ->  2.5520341396331787\n",
      "loss ->  2.7334535121917725\n",
      "loss ->  2.624293327331543\n",
      "loss ->  3.524585008621216\n",
      "loss ->  2.981114625930786\n",
      "loss ->  2.5114986896514893\n",
      "loss ->  2.717721462249756\n",
      "loss ->  2.9334752559661865\n",
      "loss ->  3.1665239334106445\n",
      "loss ->  2.783759593963623\n",
      "loss ->  2.744297504425049\n",
      "loss ->  3.067986249923706\n",
      "loss ->  2.7266485691070557\n",
      "loss ->  2.5474853515625\n",
      "loss ->  2.6614978313446045\n",
      "loss ->  3.510115623474121\n",
      "loss ->  3.208083152770996\n",
      "loss ->  2.8392090797424316\n",
      "loss ->  2.506988763809204\n",
      "loss ->  2.8540682792663574\n",
      "loss ->  2.8829360008239746\n",
      "loss ->  2.6973023414611816\n",
      "loss ->  2.807042121887207\n",
      "loss ->  2.957953691482544\n",
      "loss ->  2.935986280441284\n",
      "loss ->  2.995654344558716\n",
      "loss ->  3.117145538330078\n",
      "loss ->  3.439652919769287\n",
      "loss ->  2.985139846801758\n",
      "loss ->  3.109104871749878\n",
      "loss ->  3.0076496601104736\n",
      "loss ->  2.759143352508545\n",
      "loss ->  2.755824565887451\n",
      "loss ->  2.6764392852783203\n",
      "loss ->  3.1242947578430176\n",
      "loss ->  2.694349527359009\n",
      "loss ->  2.870908498764038\n",
      "loss ->  2.598440408706665\n",
      "loss ->  2.6474735736846924\n",
      "loss ->  2.751464366912842\n",
      "loss ->  2.9252583980560303\n",
      "loss ->  2.587750196456909\n",
      "loss ->  2.763106346130371\n",
      "loss ->  2.4265177249908447\n",
      "loss ->  2.6279847621917725\n",
      "loss ->  2.835153818130493\n",
      "loss ->  2.7810745239257812\n",
      "loss ->  3.1314899921417236\n",
      "loss ->  3.0223336219787598\n",
      "loss ->  3.042437791824341\n",
      "loss ->  2.9709956645965576\n",
      "loss ->  2.466143846511841\n",
      "loss ->  3.004960060119629\n",
      "loss ->  2.967970371246338\n",
      "loss ->  2.231545925140381\n",
      "loss ->  3.308682918548584\n",
      "loss ->  3.30680775642395\n",
      "loss ->  3.1413772106170654\n",
      "loss ->  2.688620090484619\n",
      "loss ->  2.570207357406616\n",
      "loss ->  3.3021607398986816\n",
      "loss ->  2.719008445739746\n",
      "loss ->  2.4837605953216553\n",
      "loss ->  3.3854503631591797\n",
      "loss ->  2.757038116455078\n",
      "loss ->  2.873934030532837\n",
      "loss ->  2.6123130321502686\n",
      "loss ->  3.3225035667419434\n",
      "loss ->  2.7389087677001953\n",
      "loss ->  2.2110729217529297\n",
      "loss ->  3.439420700073242\n",
      "loss ->  2.833747625350952\n",
      "loss ->  2.9459290504455566\n",
      "loss ->  3.037646770477295\n",
      "loss ->  2.7485737800598145\n",
      "loss ->  2.262665033340454\n",
      "loss ->  2.8865604400634766\n",
      "loss ->  2.8706398010253906\n",
      "loss ->  2.6663365364074707\n",
      "loss ->  3.294652223587036\n",
      "loss ->  2.9499189853668213\n",
      "loss ->  2.943963050842285\n",
      "loss ->  2.3994863033294678\n",
      "loss ->  3.021272659301758\n",
      "loss ->  2.8069968223571777\n",
      "loss ->  3.1451690196990967\n",
      "loss ->  2.9296300411224365\n",
      "loss ->  3.1750059127807617\n",
      "loss ->  2.699854612350464\n",
      "loss ->  3.310145378112793\n",
      "loss ->  2.8739609718322754\n",
      "loss ->  2.922794818878174\n",
      "loss ->  3.3123419284820557\n",
      "loss ->  2.3905162811279297\n",
      "loss ->  2.813884735107422\n",
      "loss ->  2.581071615219116\n",
      "loss ->  2.983443260192871\n",
      "loss ->  2.813788414001465\n",
      "loss ->  2.8174479007720947\n",
      "loss ->  2.747661590576172\n",
      "loss ->  2.8241829872131348\n",
      "loss ->  2.4708821773529053\n",
      "loss ->  3.0636184215545654\n",
      "loss ->  2.864394187927246\n",
      "loss ->  2.7606945037841797\n",
      "loss ->  2.390706777572632\n",
      "loss ->  2.4920148849487305\n",
      "loss ->  3.0242786407470703\n",
      "loss ->  2.6585843563079834\n",
      "loss ->  2.7938666343688965\n",
      "loss ->  2.758096694946289\n",
      "loss ->  2.570333957672119\n",
      "loss ->  3.33575439453125\n",
      "loss ->  2.8737432956695557\n",
      "loss ->  2.7912275791168213\n",
      "loss ->  3.1335229873657227\n",
      "loss ->  2.619844913482666\n",
      "loss ->  2.8410675525665283\n",
      "loss ->  2.547842264175415\n",
      "loss ->  3.086568593978882\n",
      "loss ->  3.105776309967041\n",
      "loss ->  3.1451234817504883\n",
      "loss ->  2.962522506713867\n",
      "loss ->  2.543562650680542\n",
      "loss ->  2.7343926429748535\n",
      "loss ->  2.990302085876465\n",
      "loss ->  3.192092180252075\n",
      "loss ->  2.3938233852386475\n",
      "loss ->  2.87776255607605\n",
      "loss ->  2.892129421234131\n",
      "loss ->  2.9070067405700684\n",
      "loss ->  3.3534088134765625\n",
      "loss ->  2.9578585624694824\n",
      "loss ->  2.545431137084961\n",
      "loss ->  2.3981029987335205\n",
      "loss ->  2.900217294692993\n",
      "loss ->  2.7031872272491455\n",
      "loss ->  2.806724786758423\n",
      "loss ->  3.0689074993133545\n",
      "loss ->  2.3823118209838867\n",
      "loss ->  2.8203630447387695\n",
      "loss ->  2.7952263355255127\n",
      "loss ->  3.12774395942688\n",
      "loss ->  2.727724313735962\n",
      "loss ->  3.126870632171631\n",
      "loss ->  2.9493002891540527\n",
      "loss ->  2.7302470207214355\n",
      "loss ->  3.141964912414551\n",
      "loss ->  3.032378673553467\n",
      "loss ->  2.6952037811279297\n",
      "loss ->  3.073802947998047\n",
      "loss ->  2.444045066833496\n",
      "loss ->  2.799237012863159\n",
      "loss ->  2.6638741493225098\n",
      "loss ->  2.474013566970825\n",
      "loss ->  3.4494693279266357\n",
      "loss ->  2.886357545852661\n",
      "loss ->  2.7972967624664307\n",
      "loss ->  2.838256597518921\n",
      "loss ->  3.0713822841644287\n",
      "loss ->  2.7021591663360596\n",
      "loss ->  2.827599048614502\n",
      "loss ->  2.6072192192077637\n",
      "loss ->  3.2369508743286133\n",
      "loss ->  3.0494515895843506\n",
      "loss ->  2.7430875301361084\n",
      "loss ->  2.6953887939453125\n",
      "loss ->  2.67543888092041\n",
      "loss ->  3.2012364864349365\n",
      "loss ->  3.045430898666382\n",
      "loss ->  3.1673877239227295\n",
      "loss ->  2.820694923400879\n",
      "loss ->  3.0906450748443604\n",
      "loss ->  2.9421603679656982\n",
      "loss ->  2.956158399581909\n",
      "loss ->  2.3044886589050293\n",
      "loss ->  2.993248224258423\n",
      "loss ->  3.207298755645752\n",
      "loss ->  2.6543707847595215\n",
      "loss ->  2.4307119846343994\n",
      "loss ->  2.6309821605682373\n",
      "loss ->  3.066953182220459\n",
      "loss ->  2.4549949169158936\n",
      "loss ->  2.746617078781128\n",
      "loss ->  2.709660768508911\n",
      "loss ->  2.93902850151062\n",
      "loss ->  2.808530569076538\n",
      "loss ->  2.8995864391326904\n",
      "loss ->  2.791492462158203\n",
      "loss ->  3.090665340423584\n",
      "loss ->  3.069533348083496\n",
      "loss ->  2.533720016479492\n",
      "loss ->  2.6323904991149902\n",
      "loss ->  2.998382329940796\n",
      "loss ->  2.513400077819824\n",
      "loss ->  2.29120135307312\n",
      "loss ->  2.8339121341705322\n",
      "loss ->  2.8612618446350098\n",
      "loss ->  2.725635290145874\n",
      "loss ->  2.836652994155884\n",
      "loss ->  2.719871997833252\n",
      "loss ->  2.786851644515991\n",
      "loss ->  2.7653729915618896\n",
      "loss ->  2.598324775695801\n",
      "loss ->  2.536781072616577\n",
      "loss ->  3.1687002182006836\n",
      "loss ->  2.9034264087677\n",
      "loss ->  3.063234806060791\n",
      "loss ->  2.9209933280944824\n",
      "loss ->  2.8121113777160645\n",
      "loss ->  2.6517882347106934\n",
      "loss ->  2.706965446472168\n",
      "loss ->  3.7098548412323\n",
      "loss ->  2.604189157485962\n",
      "loss ->  3.0711429119110107\n",
      "loss ->  2.7505288124084473\n",
      "loss ->  2.6356260776519775\n",
      "loss ->  2.851017475128174\n",
      "loss ->  2.538093328475952\n",
      "loss ->  2.491624355316162\n",
      "loss ->  2.417595863342285\n",
      "loss ->  2.5239455699920654\n",
      "loss ->  2.981260299682617\n",
      "loss ->  2.9299614429473877\n",
      "loss ->  2.9085328578948975\n",
      "loss ->  2.588301658630371\n",
      "loss ->  2.944133996963501\n",
      "loss ->  2.4516050815582275\n",
      "loss ->  2.7960312366485596\n",
      "loss ->  2.818268060684204\n",
      "loss ->  3.02653169631958\n",
      "loss ->  2.684951066970825\n",
      "loss ->  2.582186222076416\n",
      "loss ->  3.257948875427246\n",
      "loss ->  2.6419477462768555\n",
      "loss ->  2.9093854427337646\n",
      "loss ->  3.0468106269836426\n",
      "loss ->  2.943554639816284\n",
      "loss ->  2.339224100112915\n",
      "loss ->  2.8180902004241943\n",
      "loss ->  2.6132349967956543\n",
      "loss ->  2.9175257682800293\n",
      "loss ->  2.7327535152435303\n",
      "loss ->  2.651109218597412\n",
      "loss ->  2.8272664546966553\n",
      "loss ->  2.84883451461792\n",
      "loss ->  2.731633186340332\n",
      "loss ->  2.5509860515594482\n",
      "loss ->  2.4513070583343506\n",
      "loss ->  2.240018844604492\n",
      "loss ->  2.7456624507904053\n",
      "loss ->  2.5565133094787598\n",
      "loss ->  2.9091603755950928\n",
      "loss ->  2.9294722080230713\n",
      "loss ->  3.1065096855163574\n",
      "loss ->  2.9173431396484375\n",
      "loss ->  2.5449132919311523\n",
      "loss ->  2.601661443710327\n",
      "loss ->  2.715331554412842\n",
      "loss ->  3.33695650100708\n",
      "loss ->  2.773017644882202\n",
      "loss ->  2.6657111644744873\n",
      "loss ->  3.345390558242798\n",
      "loss ->  2.943159341812134\n",
      "loss ->  3.1009886264801025\n",
      "loss ->  2.677417278289795\n",
      "loss ->  2.653191328048706\n",
      "loss ->  2.7862279415130615\n",
      "loss ->  2.7878217697143555\n",
      "loss ->  2.9574995040893555\n",
      "loss ->  3.0812129974365234\n",
      "loss ->  2.819361686706543\n",
      "loss ->  2.7076590061187744\n",
      "loss ->  2.5886621475219727\n",
      "loss ->  2.514873743057251\n",
      "loss ->  2.54131817817688\n",
      "loss ->  2.5109360218048096\n",
      "loss ->  2.5676069259643555\n",
      "loss ->  2.451394557952881\n",
      "loss ->  2.5561113357543945\n",
      "loss ->  2.880606174468994\n",
      "loss ->  2.4015371799468994\n",
      "loss ->  2.6372604370117188\n",
      "loss ->  2.4889254570007324\n",
      "loss ->  2.9204726219177246\n",
      "loss ->  2.7159266471862793\n",
      "loss ->  2.5463740825653076\n",
      "loss ->  2.652092456817627\n",
      "loss ->  2.9447989463806152\n",
      "loss ->  3.143557071685791\n",
      "loss ->  3.2863264083862305\n",
      "loss ->  2.464869976043701\n",
      "loss ->  3.023001194000244\n",
      "loss ->  2.6726057529449463\n",
      "loss ->  3.1124374866485596\n",
      "loss ->  2.7872366905212402\n",
      "loss ->  2.5448293685913086\n",
      "loss ->  2.718170166015625\n",
      "loss ->  2.6638567447662354\n",
      "loss ->  2.6437618732452393\n",
      "loss ->  3.4128451347351074\n",
      "loss ->  2.3571670055389404\n",
      "loss ->  2.7455949783325195\n",
      "loss ->  2.656357765197754\n",
      "loss ->  3.0948970317840576\n",
      "loss ->  2.3738605976104736\n",
      "loss ->  2.958374500274658\n",
      "loss ->  3.175964593887329\n",
      "loss ->  2.826542615890503\n",
      "loss ->  2.839740753173828\n",
      "loss ->  2.584307909011841\n",
      "loss ->  2.6920032501220703\n",
      "loss ->  2.966317653656006\n",
      "loss ->  2.6915283203125\n",
      "loss ->  2.390150308609009\n",
      "loss ->  2.792863607406616\n",
      "loss ->  3.1541919708251953\n",
      "loss ->  2.744124412536621\n",
      "loss ->  2.6992287635803223\n",
      "loss ->  2.437603235244751\n",
      "loss ->  2.6510703563690186\n",
      "loss ->  3.3013505935668945\n",
      "loss ->  2.530867099761963\n",
      "loss ->  3.2165043354034424\n",
      "loss ->  2.604975700378418\n",
      "loss ->  2.743950128555298\n",
      "loss ->  2.6421453952789307\n",
      "loss ->  2.9899702072143555\n",
      "loss ->  2.646469831466675\n",
      "loss ->  2.7629685401916504\n",
      "loss ->  2.5247604846954346\n",
      "loss ->  2.9837937355041504\n",
      "loss ->  2.461005926132202\n",
      "loss ->  2.5804073810577393\n",
      "loss ->  3.016413927078247\n",
      "loss ->  2.74556565284729\n",
      "loss ->  3.051523208618164\n",
      "loss ->  2.710728406906128\n",
      "loss ->  2.6481196880340576\n",
      "loss ->  3.001209020614624\n",
      "loss ->  2.9269137382507324\n",
      "loss ->  2.336883068084717\n",
      "loss ->  2.50703763961792\n",
      "loss ->  2.890800952911377\n",
      "loss ->  3.0480358600616455\n",
      "loss ->  2.697786808013916\n",
      "loss ->  2.6725080013275146\n",
      "loss ->  2.4284908771514893\n",
      "loss ->  2.4953560829162598\n",
      "loss ->  2.344055652618408\n",
      "loss ->  2.9810848236083984\n",
      "loss ->  2.674145460128784\n",
      "loss ->  2.818305730819702\n",
      "loss ->  2.4805970191955566\n",
      "loss ->  2.6716465950012207\n",
      "loss ->  3.022136688232422\n",
      "loss ->  2.674276113510132\n",
      "loss ->  2.6543633937835693\n",
      "loss ->  2.7459230422973633\n",
      "loss ->  2.8788962364196777\n",
      "loss ->  2.362982749938965\n",
      "loss ->  2.8443093299865723\n",
      "loss ->  2.8121557235717773\n",
      "loss ->  2.921281337738037\n",
      "loss ->  3.264230728149414\n",
      "loss ->  2.8143255710601807\n",
      "loss ->  2.898414373397827\n",
      "loss ->  2.597445011138916\n",
      "loss ->  2.349269390106201\n",
      "loss ->  2.761152982711792\n",
      "loss ->  2.792985439300537\n",
      "loss ->  2.7875077724456787\n",
      "loss ->  2.2652862071990967\n",
      "loss ->  2.9057552814483643\n",
      "loss ->  2.3625237941741943\n",
      "loss ->  2.6473114490509033\n",
      "loss ->  2.5450329780578613\n",
      "loss ->  2.7718567848205566\n",
      "loss ->  2.9566054344177246\n",
      "loss ->  2.3643229007720947\n",
      "loss ->  2.7080729007720947\n",
      "loss ->  2.638080358505249\n",
      "loss ->  3.0636494159698486\n",
      "loss ->  2.6282896995544434\n",
      "loss ->  2.6329236030578613\n",
      "loss ->  2.1892287731170654\n",
      "loss ->  2.7032768726348877\n",
      "loss ->  3.106210708618164\n",
      "loss ->  3.158891439437866\n",
      "loss ->  3.186779022216797\n",
      "loss ->  2.4948885440826416\n",
      "loss ->  3.143916130065918\n",
      "loss ->  2.7087838649749756\n",
      "loss ->  3.375795841217041\n",
      "loss ->  2.5440280437469482\n",
      "loss ->  2.6279492378234863\n",
      "loss ->  2.5435378551483154\n",
      "loss ->  2.5125732421875\n",
      "loss ->  2.4528841972351074\n",
      "loss ->  3.0098540782928467\n",
      "loss ->  3.059372901916504\n",
      "loss ->  2.583808422088623\n",
      "loss ->  2.9363112449645996\n",
      "loss ->  2.780679225921631\n",
      "loss ->  2.656036138534546\n",
      "loss ->  2.452394485473633\n",
      "loss ->  2.898221015930176\n",
      "loss ->  3.201681137084961\n",
      "loss ->  2.8386454582214355\n",
      "loss ->  2.9450690746307373\n",
      "loss ->  2.662290573120117\n",
      "loss ->  3.2062621116638184\n",
      "loss ->  2.7415730953216553\n",
      "loss ->  2.885918617248535\n",
      "loss ->  2.4478890895843506\n",
      "loss ->  2.4755494594573975\n",
      "loss ->  2.232701539993286\n",
      "loss ->  2.6612327098846436\n",
      "loss ->  2.8048946857452393\n",
      "loss ->  2.7107455730438232\n",
      "loss ->  3.0043883323669434\n",
      "loss ->  2.698054313659668\n",
      "loss ->  2.5775880813598633\n",
      "loss ->  3.2374048233032227\n",
      "loss ->  2.4884774684906006\n",
      "loss ->  2.7078256607055664\n",
      "loss ->  2.8141870498657227\n",
      "loss ->  2.678184986114502\n",
      "loss ->  2.9961960315704346\n",
      "loss ->  3.1114418506622314\n",
      "loss ->  3.429326057434082\n",
      "loss ->  2.6109986305236816\n",
      "loss ->  2.846890687942505\n",
      "loss ->  2.7537193298339844\n",
      "loss ->  3.3863978385925293\n",
      "loss ->  2.4953582286834717\n",
      "loss ->  2.7142374515533447\n",
      "loss ->  2.388918161392212\n",
      "loss ->  2.815664052963257\n",
      "loss ->  2.99448823928833\n",
      "loss ->  2.748114585876465\n",
      "loss ->  2.505449056625366\n",
      "loss ->  2.5751843452453613\n",
      "loss ->  3.26063871383667\n",
      "loss ->  2.6779141426086426\n",
      "loss ->  2.857475519180298\n",
      "loss ->  2.8118410110473633\n",
      "loss ->  2.815965414047241\n",
      "loss ->  2.835719585418701\n",
      "loss ->  2.944875955581665\n",
      "loss ->  2.6168243885040283\n",
      "loss ->  2.561786413192749\n",
      "loss ->  2.3578619956970215\n",
      "loss ->  2.9383151531219482\n",
      "loss ->  2.824810028076172\n",
      "loss ->  2.8265864849090576\n",
      "loss ->  2.481637716293335\n",
      "loss ->  3.275639533996582\n",
      "loss ->  3.3717753887176514\n",
      "loss ->  2.9146833419799805\n",
      "loss ->  3.2500510215759277\n",
      "loss ->  2.830353260040283\n",
      "loss ->  2.721794605255127\n",
      "loss ->  2.4819984436035156\n",
      "loss ->  3.164548635482788\n",
      "loss ->  2.6567940711975098\n",
      "loss ->  2.794222831726074\n",
      "loss ->  2.501377582550049\n",
      "loss ->  2.9767699241638184\n",
      "loss ->  2.660595417022705\n",
      "loss ->  3.4701058864593506\n",
      "loss ->  2.318469524383545\n",
      "loss ->  2.8897616863250732\n",
      "loss ->  2.7240545749664307\n",
      "loss ->  2.5765273571014404\n",
      "loss ->  2.635219097137451\n",
      "loss ->  2.1995105743408203\n",
      "loss ->  2.7441141605377197\n",
      "loss ->  3.1246016025543213\n",
      "loss ->  2.577531576156616\n",
      "loss ->  2.695565938949585\n",
      "loss ->  2.8454651832580566\n",
      "loss ->  2.689648389816284\n",
      "loss ->  2.675264835357666\n",
      "loss ->  3.1156423091888428\n",
      "loss ->  2.550607919692993\n",
      "loss ->  2.11081600189209\n",
      "loss ->  2.8682897090911865\n",
      "loss ->  2.4396607875823975\n",
      "loss ->  2.953969955444336\n",
      "loss ->  2.532994508743286\n",
      "loss ->  2.4787471294403076\n",
      "loss ->  2.7374472618103027\n",
      "loss ->  2.7436540126800537\n",
      "loss ->  3.1743557453155518\n",
      "loss ->  2.459517478942871\n",
      "loss ->  2.6192474365234375\n",
      "loss ->  2.65460467338562\n",
      "loss ->  2.242757797241211\n",
      "loss ->  2.6917195320129395\n",
      "loss ->  3.381399631500244\n",
      "loss ->  2.5877268314361572\n",
      "loss ->  2.8455138206481934\n",
      "loss ->  2.8939995765686035\n",
      "loss ->  2.3828980922698975\n",
      "loss ->  2.6469335556030273\n",
      "loss ->  2.420792579650879\n",
      "loss ->  2.9736227989196777\n",
      "loss ->  2.5637905597686768\n",
      "loss ->  2.6658666133880615\n",
      "loss ->  2.711078405380249\n",
      "loss ->  2.4208199977874756\n",
      "loss ->  2.8266537189483643\n",
      "loss ->  2.389711618423462\n",
      "loss ->  2.529320240020752\n",
      "loss ->  2.7148704528808594\n",
      "loss ->  2.9976959228515625\n",
      "loss ->  3.2703285217285156\n",
      "loss ->  2.9781346321105957\n",
      "loss ->  2.324610948562622\n",
      "loss ->  2.122807741165161\n",
      "loss ->  2.5379738807678223\n",
      "loss ->  2.634352684020996\n",
      "loss ->  2.141195058822632\n",
      "loss ->  3.0548179149627686\n",
      "loss ->  3.0634469985961914\n",
      "loss ->  2.8032469749450684\n",
      "loss ->  3.010072708129883\n",
      "loss ->  2.8365657329559326\n",
      "loss ->  3.123398542404175\n",
      "loss ->  2.7362618446350098\n",
      "loss ->  2.606476068496704\n",
      "loss ->  2.68377947807312\n",
      "loss ->  2.698896646499634\n",
      "loss ->  2.9523911476135254\n",
      "loss ->  2.5252697467803955\n",
      "loss ->  3.0370922088623047\n",
      "loss ->  2.874634027481079\n",
      "loss ->  2.5360028743743896\n",
      "loss ->  3.0001282691955566\n",
      "loss ->  3.005605459213257\n",
      "loss ->  2.928236722946167\n",
      "loss ->  2.9786972999572754\n",
      "loss ->  3.388920307159424\n",
      "loss ->  2.6829047203063965\n",
      "loss ->  2.868492603302002\n",
      "loss ->  2.72477126121521\n",
      "loss ->  3.0714974403381348\n",
      "loss ->  2.8728013038635254\n",
      "loss ->  2.7839467525482178\n",
      "loss ->  2.9212701320648193\n",
      "loss ->  2.7118728160858154\n",
      "loss ->  2.488879442214966\n",
      "loss ->  2.5042433738708496\n",
      "loss ->  2.7387750148773193\n",
      "loss ->  2.919218063354492\n",
      "loss ->  2.5943307876586914\n",
      "loss ->  2.465847969055176\n",
      "loss ->  2.622372627258301\n",
      "loss ->  2.8128137588500977\n",
      "loss ->  2.484361410140991\n",
      "loss ->  3.0937845706939697\n",
      "loss ->  2.8841395378112793\n",
      "loss ->  2.80838942527771\n",
      "loss ->  2.8946187496185303\n",
      "loss ->  2.4944396018981934\n",
      "loss ->  2.7795848846435547\n",
      "loss ->  2.563638925552368\n",
      "loss ->  2.5876071453094482\n",
      "loss ->  2.3235955238342285\n",
      "loss ->  2.495755910873413\n",
      "loss ->  2.661691427230835\n",
      "loss ->  2.7785465717315674\n",
      "loss ->  2.880065441131592\n",
      "loss ->  2.2038991451263428\n",
      "loss ->  2.824411392211914\n",
      "loss ->  3.2347419261932373\n",
      "loss ->  2.649977684020996\n",
      "loss ->  2.630689859390259\n",
      "loss ->  2.7103588581085205\n",
      "loss ->  2.695884943008423\n",
      "loss ->  2.876662015914917\n",
      "loss ->  2.5835354328155518\n",
      "loss ->  2.548013925552368\n",
      "loss ->  2.643765449523926\n",
      "loss ->  2.7163705825805664\n",
      "loss ->  2.690845251083374\n",
      "loss ->  2.560199022293091\n",
      "loss ->  3.0242702960968018\n",
      "loss ->  2.1969380378723145\n",
      "loss ->  2.771040916442871\n",
      "loss ->  2.7936553955078125\n",
      "loss ->  3.019257068634033\n",
      "loss ->  2.5799384117126465\n",
      "loss ->  2.791066884994507\n",
      "loss ->  2.7231485843658447\n",
      "loss ->  2.609172821044922\n",
      "loss ->  3.0792510509490967\n",
      "loss ->  2.185253858566284\n",
      "loss ->  3.029653310775757\n",
      "loss ->  2.493281602859497\n",
      "loss ->  3.1352779865264893\n",
      "loss ->  3.000308036804199\n",
      "loss ->  2.9475810527801514\n",
      "loss ->  2.6762771606445312\n",
      "loss ->  2.8525867462158203\n",
      "loss ->  2.786271572113037\n",
      "loss ->  2.834444761276245\n",
      "loss ->  2.6295692920684814\n",
      "loss ->  2.5808935165405273\n",
      "loss ->  2.890098810195923\n",
      "loss ->  2.33138108253479\n",
      "loss ->  2.7913200855255127\n",
      "loss ->  2.6599090099334717\n",
      "loss ->  2.674729585647583\n",
      "loss ->  2.3616905212402344\n",
      "loss ->  2.2942638397216797\n",
      "loss ->  2.690281391143799\n",
      "loss ->  2.590075731277466\n",
      "loss ->  2.9905996322631836\n",
      "loss ->  2.8284945487976074\n",
      "loss ->  2.252100944519043\n",
      "loss ->  2.721329927444458\n",
      "loss ->  2.8991267681121826\n",
      "loss ->  2.8698220252990723\n",
      "loss ->  2.9031174182891846\n",
      "loss ->  2.9813461303710938\n",
      "loss ->  2.7533388137817383\n",
      "loss ->  2.7060599327087402\n",
      "loss ->  2.4331283569335938\n",
      "loss ->  3.0764896869659424\n",
      "loss ->  2.4481911659240723\n",
      "loss ->  2.9190170764923096\n",
      "loss ->  3.1935207843780518\n",
      "loss ->  2.5374248027801514\n",
      "loss ->  2.5600733757019043\n",
      "loss ->  2.774637222290039\n",
      "loss ->  2.645082712173462\n",
      "loss ->  2.90255069732666\n",
      "loss ->  2.8803980350494385\n",
      "loss ->  2.6535120010375977\n",
      "loss ->  2.5756423473358154\n",
      "loss ->  2.944951295852661\n",
      "loss ->  2.590808153152466\n",
      "loss ->  2.576799154281616\n",
      "loss ->  2.9343347549438477\n",
      "loss ->  2.5296897888183594\n",
      "loss ->  2.6700596809387207\n",
      "loss ->  2.478177309036255\n",
      "loss ->  3.2052414417266846\n",
      "loss ->  2.670593023300171\n",
      "loss ->  2.4855306148529053\n",
      "loss ->  2.746338129043579\n",
      "loss ->  2.6785221099853516\n",
      "loss ->  2.8634657859802246\n",
      "loss ->  2.198899507522583\n",
      "loss ->  2.433915615081787\n",
      "loss ->  2.6788113117218018\n",
      "loss ->  2.2396726608276367\n",
      "loss ->  2.5548062324523926\n",
      "loss ->  2.8071746826171875\n",
      "loss ->  3.2404351234436035\n",
      "loss ->  2.8185558319091797\n",
      "loss ->  2.879945993423462\n",
      "loss ->  3.154858112335205\n",
      "loss ->  2.6237542629241943\n",
      "loss ->  2.7958359718322754\n",
      "loss ->  3.0247981548309326\n",
      "loss ->  2.3127994537353516\n",
      "loss ->  2.50836181640625\n",
      "loss ->  2.733297109603882\n",
      "loss ->  2.718122959136963\n",
      "loss ->  2.723220109939575\n",
      "loss ->  2.8644092082977295\n",
      "loss ->  2.4874377250671387\n",
      "loss ->  2.579270124435425\n",
      "loss ->  2.709385633468628\n",
      "loss ->  2.7864267826080322\n",
      "loss ->  3.017359495162964\n",
      "loss ->  2.8833062648773193\n",
      "loss ->  2.442533493041992\n",
      "loss ->  2.951908588409424\n",
      "loss ->  2.6018548011779785\n",
      "loss ->  2.925034284591675\n",
      "loss ->  2.824531316757202\n",
      "loss ->  2.4511210918426514\n",
      "loss ->  2.9381890296936035\n",
      "loss ->  2.6027092933654785\n",
      "loss ->  2.5845940113067627\n",
      "loss ->  2.791686534881592\n",
      "loss ->  2.3650479316711426\n",
      "loss ->  3.287402391433716\n",
      "loss ->  2.6042580604553223\n",
      "loss ->  2.626404047012329\n",
      "loss ->  2.5615198612213135\n",
      "loss ->  2.9100754261016846\n",
      "loss ->  2.9246292114257812\n",
      "loss ->  2.5594563484191895\n",
      "loss ->  2.5542678833007812\n",
      "loss ->  2.352168560028076\n",
      "loss ->  2.4930989742279053\n",
      "loss ->  2.82963490486145\n",
      "loss ->  3.0610430240631104\n",
      "loss ->  2.889172315597534\n",
      "loss ->  2.729233503341675\n",
      "loss ->  2.8417458534240723\n",
      "loss ->  2.8607256412506104\n",
      "loss ->  2.463200092315674\n",
      "loss ->  2.9906649589538574\n",
      "loss ->  3.123368978500366\n",
      "loss ->  2.898906946182251\n",
      "loss ->  2.7116665840148926\n",
      "loss ->  2.6914262771606445\n",
      "loss ->  2.532886505126953\n",
      "loss ->  2.67716646194458\n",
      "loss ->  2.423867702484131\n",
      "loss ->  2.5130796432495117\n",
      "loss ->  2.7559027671813965\n",
      "loss ->  2.8017520904541016\n",
      "loss ->  2.494297742843628\n",
      "loss ->  2.8026297092437744\n",
      "loss ->  3.06205153465271\n",
      "loss ->  3.3788352012634277\n",
      "loss ->  2.582740306854248\n",
      "loss ->  2.5357248783111572\n",
      "loss ->  2.2504329681396484\n",
      "loss ->  2.4354050159454346\n",
      "loss ->  3.1755805015563965\n",
      "loss ->  2.5591025352478027\n",
      "loss ->  3.0132129192352295\n",
      "loss ->  3.006779670715332\n",
      "loss ->  2.819797992706299\n",
      "loss ->  2.578742027282715\n",
      "loss ->  2.3836722373962402\n",
      "loss ->  2.9597201347351074\n",
      "loss ->  2.8641650676727295\n",
      "loss ->  2.6671416759490967\n",
      "loss ->  2.8535537719726562\n",
      "loss ->  3.0710866451263428\n",
      "loss ->  2.675784111022949\n",
      "loss ->  2.5363848209381104\n",
      "loss ->  2.734666109085083\n",
      "loss ->  2.583901882171631\n",
      "loss ->  2.670766592025757\n",
      "loss ->  2.676490306854248\n",
      "loss ->  2.896289825439453\n",
      "loss ->  2.4276320934295654\n",
      "loss ->  2.6933133602142334\n",
      "loss ->  2.682894229888916\n",
      "loss ->  2.558063507080078\n",
      "loss ->  2.3869385719299316\n",
      "loss ->  2.8226585388183594\n",
      "loss ->  2.8950328826904297\n",
      "loss ->  2.8724498748779297\n",
      "loss ->  3.0536444187164307\n",
      "loss ->  2.5837602615356445\n",
      "loss ->  2.891724109649658\n",
      "loss ->  2.345453977584839\n",
      "loss ->  2.3008265495300293\n",
      "loss ->  2.2908833026885986\n",
      "loss ->  2.8553519248962402\n",
      "loss ->  2.487241268157959\n",
      "loss ->  2.5905585289001465\n",
      "loss ->  3.0595955848693848\n",
      "loss ->  2.7169578075408936\n",
      "loss ->  2.782656669616699\n",
      "loss ->  2.4227306842803955\n",
      "loss ->  2.8166708946228027\n",
      "loss ->  2.4924545288085938\n",
      "loss ->  3.316129684448242\n",
      "loss ->  2.5543086528778076\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you can see that thee training is much faster and therefore, you can afford to increase the iterations for further minimizing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "\n",
    "How do you determine the learning rate? How do you know if it's too small (moving too slowly towards an optimized loss), or it's too big (over-stepping and missing the optimized loss)?\n",
    "\n",
    "One way is to find the min and max range, first. You can provide -0.0001 or lower and find the value that demonstrates a reasonable decrease in the loss. Then find a big number with the same analogy. Based in this, we can see that, the optimized learning rate should be between -0.001 and -1.\n",
    "\n",
    "We can use pytorch's library to create a linear array of learning rates between these two numbers for, say 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # linear rate exponential\n",
    "lrs = 10**lre # learning rates: 10^-3 = 0.001 and 10^0 = 1\n",
    "lrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's reset everything and iterate through possible learning rates to find the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  17.818588256835938\n",
      "loss ->  17.207744598388672\n",
      "loss ->  17.988798141479492\n",
      "loss ->  20.785329818725586\n",
      "loss ->  20.873462677001953\n",
      "loss ->  18.135498046875\n",
      "loss ->  18.609798431396484\n",
      "loss ->  20.673307418823242\n",
      "loss ->  16.66250228881836\n",
      "loss ->  21.12810516357422\n",
      "loss ->  17.779855728149414\n",
      "loss ->  17.02388572692871\n",
      "loss ->  15.62646770477295\n",
      "loss ->  18.628421783447266\n",
      "loss ->  17.707677841186523\n",
      "loss ->  18.769920349121094\n",
      "loss ->  19.127243041992188\n",
      "loss ->  17.14310646057129\n",
      "loss ->  20.252927780151367\n",
      "loss ->  22.728914260864258\n",
      "loss ->  19.412267684936523\n",
      "loss ->  22.063735961914062\n",
      "loss ->  18.95575714111328\n",
      "loss ->  18.117475509643555\n",
      "loss ->  19.736312866210938\n",
      "loss ->  17.780424118041992\n",
      "loss ->  18.236976623535156\n",
      "loss ->  22.279130935668945\n",
      "loss ->  17.801116943359375\n",
      "loss ->  17.21622657775879\n",
      "loss ->  16.967979431152344\n",
      "loss ->  17.564491271972656\n",
      "loss ->  17.102432250976562\n",
      "loss ->  21.47294044494629\n",
      "loss ->  18.45301055908203\n",
      "loss ->  19.186065673828125\n",
      "loss ->  21.716041564941406\n",
      "loss ->  17.54133415222168\n",
      "loss ->  22.55615234375\n",
      "loss ->  17.825498580932617\n",
      "loss ->  16.71475601196289\n",
      "loss ->  15.73196029663086\n",
      "loss ->  15.287490844726562\n",
      "loss ->  22.465137481689453\n",
      "loss ->  18.270748138427734\n",
      "loss ->  19.169349670410156\n",
      "loss ->  15.620478630065918\n",
      "loss ->  19.203508377075195\n",
      "loss ->  19.177940368652344\n",
      "loss ->  16.625749588012695\n",
      "loss ->  18.858932495117188\n",
      "loss ->  17.627445220947266\n",
      "loss ->  17.65172576904297\n",
      "loss ->  18.837249755859375\n",
      "loss ->  17.665159225463867\n",
      "loss ->  18.794567108154297\n",
      "loss ->  15.870450019836426\n",
      "loss ->  16.26371192932129\n",
      "loss ->  19.62697982788086\n",
      "loss ->  19.43778419494629\n",
      "loss ->  18.9267578125\n",
      "loss ->  18.004066467285156\n",
      "loss ->  20.147558212280273\n",
      "loss ->  15.894144058227539\n",
      "loss ->  17.26469612121582\n",
      "loss ->  16.15572166442871\n",
      "loss ->  20.64175796508789\n",
      "loss ->  15.529400825500488\n",
      "loss ->  20.512435913085938\n",
      "loss ->  16.217206954956055\n",
      "loss ->  16.013233184814453\n",
      "loss ->  15.332643508911133\n",
      "loss ->  19.639434814453125\n",
      "loss ->  15.748458862304688\n",
      "loss ->  19.089052200317383\n",
      "loss ->  17.217283248901367\n",
      "loss ->  19.23548126220703\n",
      "loss ->  17.203937530517578\n",
      "loss ->  18.286624908447266\n",
      "loss ->  17.60593032836914\n",
      "loss ->  18.48563003540039\n",
      "loss ->  16.37277603149414\n",
      "loss ->  15.443891525268555\n",
      "loss ->  19.74250602722168\n",
      "loss ->  16.853721618652344\n",
      "loss ->  17.42133140563965\n",
      "loss ->  16.50066566467285\n",
      "loss ->  15.979920387268066\n",
      "loss ->  15.31175422668457\n",
      "loss ->  17.312297821044922\n",
      "loss ->  16.670108795166016\n",
      "loss ->  15.199868202209473\n",
      "loss ->  17.83730125427246\n",
      "loss ->  16.9716796875\n",
      "loss ->  17.6366024017334\n",
      "loss ->  15.672210693359375\n",
      "loss ->  17.433090209960938\n",
      "loss ->  16.779264450073242\n",
      "loss ->  14.504195213317871\n",
      "loss ->  15.92406940460205\n",
      "loss ->  16.793296813964844\n",
      "loss ->  15.906143188476562\n",
      "loss ->  16.044729232788086\n",
      "loss ->  13.5645751953125\n",
      "loss ->  18.381778717041016\n",
      "loss ->  15.575592994689941\n",
      "loss ->  17.62857437133789\n",
      "loss ->  16.312496185302734\n",
      "loss ->  14.299957275390625\n",
      "loss ->  16.739707946777344\n",
      "loss ->  17.661624908447266\n",
      "loss ->  16.480941772460938\n",
      "loss ->  15.166065216064453\n",
      "loss ->  15.125469207763672\n",
      "loss ->  17.104759216308594\n",
      "loss ->  16.59653663635254\n",
      "loss ->  16.672460556030273\n",
      "loss ->  17.804851531982422\n",
      "loss ->  20.356903076171875\n",
      "loss ->  16.584999084472656\n",
      "loss ->  15.214910507202148\n",
      "loss ->  14.09407901763916\n",
      "loss ->  15.845037460327148\n",
      "loss ->  16.111984252929688\n",
      "loss ->  16.31787109375\n",
      "loss ->  15.110956192016602\n",
      "loss ->  17.962751388549805\n",
      "loss ->  15.559111595153809\n",
      "loss ->  13.112187385559082\n",
      "loss ->  16.916793823242188\n",
      "loss ->  15.910941123962402\n",
      "loss ->  13.568110466003418\n",
      "loss ->  16.056015014648438\n",
      "loss ->  16.474952697753906\n",
      "loss ->  18.32109832763672\n",
      "loss ->  17.166458129882812\n",
      "loss ->  15.533581733703613\n",
      "loss ->  13.603761672973633\n",
      "loss ->  14.032119750976562\n",
      "loss ->  17.96251678466797\n",
      "loss ->  14.2632474899292\n",
      "loss ->  19.334880828857422\n",
      "loss ->  14.161398887634277\n",
      "loss ->  16.10371208190918\n",
      "loss ->  15.592325210571289\n",
      "loss ->  16.293272018432617\n",
      "loss ->  13.956291198730469\n",
      "loss ->  15.987275123596191\n",
      "loss ->  15.782715797424316\n",
      "loss ->  14.0541353225708\n",
      "loss ->  17.61638641357422\n",
      "loss ->  14.704229354858398\n",
      "loss ->  15.231337547302246\n",
      "loss ->  13.409820556640625\n",
      "loss ->  15.819284439086914\n",
      "loss ->  18.296274185180664\n",
      "loss ->  14.12851333618164\n",
      "loss ->  18.198448181152344\n",
      "loss ->  13.970293998718262\n",
      "loss ->  14.927781105041504\n",
      "loss ->  17.06662368774414\n",
      "loss ->  15.44095230102539\n",
      "loss ->  15.21241569519043\n",
      "loss ->  15.607431411743164\n",
      "loss ->  13.266845703125\n",
      "loss ->  14.992154121398926\n",
      "loss ->  16.926103591918945\n",
      "loss ->  12.592386245727539\n",
      "loss ->  12.04273509979248\n",
      "loss ->  12.633797645568848\n",
      "loss ->  16.110139846801758\n",
      "loss ->  17.507539749145508\n",
      "loss ->  17.15896224975586\n",
      "loss ->  11.108833312988281\n",
      "loss ->  15.705878257751465\n",
      "loss ->  12.636873245239258\n",
      "loss ->  15.149870872497559\n",
      "loss ->  12.40650463104248\n",
      "loss ->  11.141661643981934\n",
      "loss ->  14.53771686553955\n",
      "loss ->  16.819612503051758\n",
      "loss ->  11.711137771606445\n",
      "loss ->  15.001623153686523\n",
      "loss ->  11.309407234191895\n",
      "loss ->  13.378911972045898\n",
      "loss ->  15.441974639892578\n",
      "loss ->  14.403761863708496\n",
      "loss ->  14.483667373657227\n",
      "loss ->  17.790977478027344\n",
      "loss ->  16.412206649780273\n",
      "loss ->  12.263585090637207\n",
      "loss ->  11.469863891601562\n",
      "loss ->  14.352192878723145\n",
      "loss ->  13.645618438720703\n",
      "loss ->  14.480493545532227\n",
      "loss ->  11.683615684509277\n",
      "loss ->  14.358451843261719\n",
      "loss ->  13.970601081848145\n",
      "loss ->  15.00464153289795\n",
      "loss ->  15.96337604522705\n",
      "loss ->  15.728059768676758\n",
      "loss ->  11.336629867553711\n",
      "loss ->  13.1746187210083\n",
      "loss ->  13.689642906188965\n",
      "loss ->  15.214460372924805\n",
      "loss ->  15.98100757598877\n",
      "loss ->  12.943106651306152\n",
      "loss ->  13.47856330871582\n",
      "loss ->  13.524924278259277\n",
      "loss ->  15.776738166809082\n",
      "loss ->  12.157858848571777\n",
      "loss ->  12.806915283203125\n",
      "loss ->  14.367472648620605\n",
      "loss ->  13.144009590148926\n",
      "loss ->  13.135956764221191\n",
      "loss ->  14.207083702087402\n",
      "loss ->  13.11515998840332\n",
      "loss ->  12.914426803588867\n",
      "loss ->  14.912988662719727\n",
      "loss ->  12.75916862487793\n",
      "loss ->  10.804880142211914\n",
      "loss ->  13.097886085510254\n",
      "loss ->  13.224571228027344\n",
      "loss ->  14.023456573486328\n",
      "loss ->  13.343667984008789\n",
      "loss ->  13.028108596801758\n",
      "loss ->  15.787437438964844\n",
      "loss ->  12.360869407653809\n",
      "loss ->  9.091840744018555\n",
      "loss ->  11.718550682067871\n",
      "loss ->  15.353879928588867\n",
      "loss ->  14.019659996032715\n",
      "loss ->  11.674993515014648\n",
      "loss ->  13.015288352966309\n",
      "loss ->  11.177377700805664\n",
      "loss ->  10.55807113647461\n",
      "loss ->  10.766685485839844\n",
      "loss ->  13.628876686096191\n",
      "loss ->  9.682295799255371\n",
      "loss ->  12.144325256347656\n",
      "loss ->  10.453149795532227\n",
      "loss ->  13.808960914611816\n",
      "loss ->  12.07746410369873\n",
      "loss ->  11.664168357849121\n",
      "loss ->  14.14193344116211\n",
      "loss ->  10.619626998901367\n",
      "loss ->  15.58375072479248\n",
      "loss ->  15.018195152282715\n",
      "loss ->  9.34996509552002\n",
      "loss ->  12.1754732131958\n",
      "loss ->  12.765385627746582\n",
      "loss ->  12.233344078063965\n",
      "loss ->  12.52571964263916\n",
      "loss ->  13.647894859313965\n",
      "loss ->  14.791521072387695\n",
      "loss ->  10.954558372497559\n",
      "loss ->  12.288110733032227\n",
      "loss ->  11.038283348083496\n",
      "loss ->  13.991031646728516\n",
      "loss ->  9.52863597869873\n",
      "loss ->  12.139893531799316\n",
      "loss ->  11.566926002502441\n",
      "loss ->  10.129068374633789\n",
      "loss ->  11.14036750793457\n",
      "loss ->  11.854352951049805\n",
      "loss ->  13.864048957824707\n",
      "loss ->  12.38448429107666\n",
      "loss ->  12.324552536010742\n",
      "loss ->  12.586535453796387\n",
      "loss ->  10.11099624633789\n",
      "loss ->  11.196382522583008\n",
      "loss ->  10.282811164855957\n",
      "loss ->  12.677310943603516\n",
      "loss ->  10.915802001953125\n",
      "loss ->  12.46096134185791\n",
      "loss ->  12.40473747253418\n",
      "loss ->  11.11902141571045\n",
      "loss ->  11.837747573852539\n",
      "loss ->  11.19214153289795\n",
      "loss ->  12.830118179321289\n",
      "loss ->  9.283364295959473\n",
      "loss ->  9.115910530090332\n",
      "loss ->  12.790547370910645\n",
      "loss ->  12.496990203857422\n",
      "loss ->  12.30195426940918\n",
      "loss ->  12.8580322265625\n",
      "loss ->  9.7821044921875\n",
      "loss ->  9.958890914916992\n",
      "loss ->  11.693202018737793\n",
      "loss ->  11.05566692352295\n",
      "loss ->  10.730361938476562\n",
      "loss ->  11.918022155761719\n",
      "loss ->  9.050751686096191\n",
      "loss ->  10.692131042480469\n",
      "loss ->  10.154603004455566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  8.6114501953125\n",
      "loss ->  9.79350757598877\n",
      "loss ->  10.733607292175293\n",
      "loss ->  10.819635391235352\n",
      "loss ->  11.903679847717285\n",
      "loss ->  13.183263778686523\n",
      "loss ->  10.20171070098877\n",
      "loss ->  12.106488227844238\n",
      "loss ->  13.204911231994629\n",
      "loss ->  10.672565460205078\n",
      "loss ->  10.946775436401367\n",
      "loss ->  10.51285171508789\n",
      "loss ->  11.21959114074707\n",
      "loss ->  12.171158790588379\n",
      "loss ->  10.082282066345215\n",
      "loss ->  9.49066162109375\n",
      "loss ->  11.373885154724121\n",
      "loss ->  13.22693920135498\n",
      "loss ->  10.17165470123291\n",
      "loss ->  8.857242584228516\n",
      "loss ->  10.976099967956543\n",
      "loss ->  10.234500885009766\n",
      "loss ->  11.430596351623535\n",
      "loss ->  9.86681079864502\n",
      "loss ->  9.83964729309082\n",
      "loss ->  12.072699546813965\n",
      "loss ->  7.562723159790039\n",
      "loss ->  9.785326957702637\n",
      "loss ->  10.59708309173584\n",
      "loss ->  8.06691837310791\n",
      "loss ->  9.147026062011719\n",
      "loss ->  10.198055267333984\n",
      "loss ->  9.455120086669922\n",
      "loss ->  9.41597843170166\n",
      "loss ->  9.17795181274414\n",
      "loss ->  7.488502502441406\n",
      "loss ->  9.414750099182129\n",
      "loss ->  11.278045654296875\n",
      "loss ->  9.095611572265625\n",
      "loss ->  10.266167640686035\n",
      "loss ->  11.938130378723145\n",
      "loss ->  12.283631324768066\n",
      "loss ->  11.140149116516113\n",
      "loss ->  13.078448295593262\n",
      "loss ->  7.58393669128418\n",
      "loss ->  9.20666217803955\n",
      "loss ->  10.060050010681152\n",
      "loss ->  9.231904983520508\n",
      "loss ->  11.47284984588623\n",
      "loss ->  9.010599136352539\n",
      "loss ->  9.448182106018066\n",
      "loss ->  9.672778129577637\n",
      "loss ->  10.966697692871094\n",
      "loss ->  12.720680236816406\n",
      "loss ->  10.01246452331543\n",
      "loss ->  6.81584358215332\n",
      "loss ->  8.389861106872559\n",
      "loss ->  9.93088150024414\n",
      "loss ->  13.223371505737305\n",
      "loss ->  10.067266464233398\n",
      "loss ->  11.181857109069824\n",
      "loss ->  10.183001518249512\n",
      "loss ->  10.391510009765625\n",
      "loss ->  11.601974487304688\n",
      "loss ->  6.956448554992676\n",
      "loss ->  10.197511672973633\n",
      "loss ->  7.546834945678711\n",
      "loss ->  7.853392124176025\n",
      "loss ->  13.091245651245117\n",
      "loss ->  9.422907829284668\n",
      "loss ->  10.851740837097168\n",
      "loss ->  8.054390907287598\n",
      "loss ->  8.569215774536133\n",
      "loss ->  10.927687644958496\n",
      "loss ->  8.442849159240723\n",
      "loss ->  9.807571411132812\n",
      "loss ->  7.732173919677734\n",
      "loss ->  9.700057983398438\n",
      "loss ->  9.072763442993164\n",
      "loss ->  6.885879993438721\n",
      "loss ->  9.070761680603027\n",
      "loss ->  10.03807258605957\n",
      "loss ->  7.334052562713623\n",
      "loss ->  9.481036186218262\n",
      "loss ->  8.473834037780762\n",
      "loss ->  6.969018936157227\n",
      "loss ->  6.824958324432373\n",
      "loss ->  7.920530319213867\n",
      "loss ->  8.40494155883789\n",
      "loss ->  9.57587718963623\n",
      "loss ->  7.087244033813477\n",
      "loss ->  9.96667766571045\n",
      "loss ->  7.351144790649414\n",
      "loss ->  8.993118286132812\n",
      "loss ->  7.727452278137207\n",
      "loss ->  8.42380428314209\n",
      "loss ->  6.551092624664307\n",
      "loss ->  7.925812244415283\n",
      "loss ->  6.670701026916504\n",
      "loss ->  9.364113807678223\n",
      "loss ->  7.531502723693848\n",
      "loss ->  7.587027549743652\n",
      "loss ->  9.208853721618652\n",
      "loss ->  6.784102439880371\n",
      "loss ->  9.222212791442871\n",
      "loss ->  9.858262062072754\n",
      "loss ->  7.533483028411865\n",
      "loss ->  6.543849468231201\n",
      "loss ->  7.071488857269287\n",
      "loss ->  8.69660472869873\n",
      "loss ->  11.31380844116211\n",
      "loss ->  8.19653606414795\n",
      "loss ->  7.964107990264893\n",
      "loss ->  8.755186080932617\n",
      "loss ->  7.910871505737305\n",
      "loss ->  8.190611839294434\n",
      "loss ->  8.513723373413086\n",
      "loss ->  8.318516731262207\n",
      "loss ->  7.771124362945557\n",
      "loss ->  7.1490960121154785\n",
      "loss ->  8.945337295532227\n",
      "loss ->  8.90559196472168\n",
      "loss ->  9.8555908203125\n",
      "loss ->  8.663949966430664\n",
      "loss ->  6.855587959289551\n",
      "loss ->  5.676336765289307\n",
      "loss ->  6.634425640106201\n",
      "loss ->  6.85403299331665\n",
      "loss ->  6.998166561126709\n",
      "loss ->  7.755427360534668\n",
      "loss ->  5.840245723724365\n",
      "loss ->  6.64901065826416\n",
      "loss ->  9.046795845031738\n",
      "loss ->  6.965780735015869\n",
      "loss ->  7.260414123535156\n",
      "loss ->  7.0175886154174805\n",
      "loss ->  5.735681056976318\n",
      "loss ->  9.576578140258789\n",
      "loss ->  6.513352394104004\n",
      "loss ->  7.048755168914795\n",
      "loss ->  6.674585342407227\n",
      "loss ->  5.036264419555664\n",
      "loss ->  6.7520270347595215\n",
      "loss ->  9.135726928710938\n",
      "loss ->  7.556339740753174\n",
      "loss ->  8.149615287780762\n",
      "loss ->  6.406125545501709\n",
      "loss ->  5.571310997009277\n",
      "loss ->  6.458847522735596\n",
      "loss ->  6.2343549728393555\n",
      "loss ->  7.224706649780273\n",
      "loss ->  5.978302478790283\n",
      "loss ->  7.443661689758301\n",
      "loss ->  7.451767921447754\n",
      "loss ->  6.75609016418457\n",
      "loss ->  5.943716049194336\n",
      "loss ->  6.953161239624023\n",
      "loss ->  3.9638679027557373\n",
      "loss ->  5.959860801696777\n",
      "loss ->  8.27652359008789\n",
      "loss ->  4.215775012969971\n",
      "loss ->  5.928572654724121\n",
      "loss ->  4.1776814460754395\n",
      "loss ->  6.1796770095825195\n",
      "loss ->  8.243045806884766\n",
      "loss ->  8.044707298278809\n",
      "loss ->  7.097711086273193\n",
      "loss ->  5.73460578918457\n",
      "loss ->  6.864355087280273\n",
      "loss ->  6.769809246063232\n",
      "loss ->  6.636353015899658\n",
      "loss ->  7.532527923583984\n",
      "loss ->  5.359614849090576\n",
      "loss ->  7.448232173919678\n",
      "loss ->  5.966253757476807\n",
      "loss ->  6.061166286468506\n",
      "loss ->  8.120166778564453\n",
      "loss ->  6.519518852233887\n",
      "loss ->  7.553409576416016\n",
      "loss ->  3.997314929962158\n",
      "loss ->  7.006365776062012\n",
      "loss ->  4.81742525100708\n",
      "loss ->  5.410883903503418\n",
      "loss ->  5.782837867736816\n",
      "loss ->  6.988909721374512\n",
      "loss ->  4.935892105102539\n",
      "loss ->  5.023526668548584\n",
      "loss ->  6.360191345214844\n",
      "loss ->  7.509313106536865\n",
      "loss ->  5.808602333068848\n",
      "loss ->  5.108997344970703\n",
      "loss ->  5.3467254638671875\n",
      "loss ->  6.350180149078369\n",
      "loss ->  5.047910213470459\n",
      "loss ->  4.607457160949707\n",
      "loss ->  4.824520587921143\n",
      "loss ->  4.829946517944336\n",
      "loss ->  7.470127105712891\n",
      "loss ->  5.554059028625488\n",
      "loss ->  5.141803741455078\n",
      "loss ->  5.8704142570495605\n",
      "loss ->  4.195637226104736\n",
      "loss ->  5.266543388366699\n",
      "loss ->  4.683743000030518\n",
      "loss ->  4.823688983917236\n",
      "loss ->  6.20469856262207\n",
      "loss ->  6.546086311340332\n",
      "loss ->  5.697802543640137\n",
      "loss ->  4.977287292480469\n",
      "loss ->  5.89919900894165\n",
      "loss ->  4.7022504806518555\n",
      "loss ->  5.7173309326171875\n",
      "loss ->  5.626089572906494\n",
      "loss ->  4.859432220458984\n",
      "loss ->  5.433114051818848\n",
      "loss ->  5.534030437469482\n",
      "loss ->  4.770918369293213\n",
      "loss ->  5.775104522705078\n",
      "loss ->  5.697479248046875\n",
      "loss ->  5.767390251159668\n",
      "loss ->  4.794119834899902\n",
      "loss ->  5.372209548950195\n",
      "loss ->  5.428668022155762\n",
      "loss ->  4.557544231414795\n",
      "loss ->  6.063882827758789\n",
      "loss ->  4.87504768371582\n",
      "loss ->  5.282818794250488\n",
      "loss ->  5.370133399963379\n",
      "loss ->  5.657231330871582\n",
      "loss ->  4.9649763107299805\n",
      "loss ->  4.343540191650391\n",
      "loss ->  5.036244869232178\n",
      "loss ->  5.567706108093262\n",
      "loss ->  3.855720043182373\n",
      "loss ->  4.794682502746582\n",
      "loss ->  5.061222553253174\n",
      "loss ->  4.633205413818359\n",
      "loss ->  6.211921691894531\n",
      "loss ->  5.957350730895996\n",
      "loss ->  4.63578462600708\n",
      "loss ->  4.0462188720703125\n",
      "loss ->  4.553534984588623\n",
      "loss ->  5.745405673980713\n",
      "loss ->  5.0218610763549805\n",
      "loss ->  5.369269371032715\n",
      "loss ->  4.109112739562988\n",
      "loss ->  4.351373195648193\n",
      "loss ->  4.842265605926514\n",
      "loss ->  4.471305847167969\n",
      "loss ->  4.342769622802734\n",
      "loss ->  4.059107780456543\n",
      "loss ->  5.69456148147583\n",
      "loss ->  4.2300004959106445\n",
      "loss ->  3.4831392765045166\n",
      "loss ->  5.055178642272949\n",
      "loss ->  4.321035861968994\n",
      "loss ->  4.299635887145996\n",
      "loss ->  3.7981362342834473\n",
      "loss ->  4.644561767578125\n",
      "loss ->  5.270917892456055\n",
      "loss ->  3.8795838356018066\n",
      "loss ->  4.5657501220703125\n",
      "loss ->  5.263833045959473\n",
      "loss ->  4.465679168701172\n",
      "loss ->  4.338068008422852\n",
      "loss ->  3.8510191440582275\n",
      "loss ->  5.43610143661499\n",
      "loss ->  3.8126087188720703\n",
      "loss ->  5.088261604309082\n",
      "loss ->  4.495244979858398\n",
      "loss ->  3.2837586402893066\n",
      "loss ->  5.004532337188721\n",
      "loss ->  3.6700141429901123\n",
      "loss ->  3.975928783416748\n",
      "loss ->  4.187282085418701\n",
      "loss ->  3.942897319793701\n",
      "loss ->  4.402803421020508\n",
      "loss ->  4.1892266273498535\n",
      "loss ->  4.484455585479736\n",
      "loss ->  3.5974602699279785\n",
      "loss ->  5.054281234741211\n",
      "loss ->  3.457779884338379\n",
      "loss ->  3.692352056503296\n",
      "loss ->  3.8163933753967285\n",
      "loss ->  3.962425708770752\n",
      "loss ->  4.708582878112793\n",
      "loss ->  3.681640625\n",
      "loss ->  2.8138465881347656\n",
      "loss ->  4.469870090484619\n",
      "loss ->  3.804471969604492\n",
      "loss ->  4.177872180938721\n",
      "loss ->  2.7918546199798584\n",
      "loss ->  4.90116548538208\n",
      "loss ->  3.494856357574463\n",
      "loss ->  4.356545448303223\n",
      "loss ->  3.9646670818328857\n",
      "loss ->  4.723150730133057\n",
      "loss ->  3.8013081550598145\n",
      "loss ->  3.9293744564056396\n",
      "loss ->  3.460045576095581\n",
      "loss ->  4.032174110412598\n",
      "loss ->  4.512244701385498\n",
      "loss ->  3.466155529022217\n",
      "loss ->  4.114262580871582\n",
      "loss ->  5.117386341094971\n",
      "loss ->  4.426608562469482\n",
      "loss ->  4.011208534240723\n",
      "loss ->  3.780466079711914\n",
      "loss ->  3.3338136672973633\n",
      "loss ->  3.042330741882324\n",
      "loss ->  2.986008405685425\n",
      "loss ->  3.2690820693969727\n",
      "loss ->  3.6852540969848633\n",
      "loss ->  4.365446090698242\n",
      "loss ->  3.2529752254486084\n",
      "loss ->  4.119260787963867\n",
      "loss ->  2.9605934619903564\n",
      "loss ->  3.8797104358673096\n",
      "loss ->  4.369398593902588\n",
      "loss ->  2.822662830352783\n",
      "loss ->  3.437737464904785\n",
      "loss ->  2.715862512588501\n",
      "loss ->  4.411957263946533\n",
      "loss ->  4.671970844268799\n",
      "loss ->  4.049750804901123\n",
      "loss ->  3.7381153106689453\n",
      "loss ->  4.48844051361084\n",
      "loss ->  4.559271335601807\n",
      "loss ->  3.349641799926758\n",
      "loss ->  3.6621477603912354\n",
      "loss ->  3.3099660873413086\n",
      "loss ->  3.6825575828552246\n",
      "loss ->  3.7733333110809326\n",
      "loss ->  3.3599016666412354\n",
      "loss ->  3.2717950344085693\n",
      "loss ->  2.7998557090759277\n",
      "loss ->  3.374729871749878\n",
      "loss ->  2.809324026107788\n",
      "loss ->  3.5392370223999023\n",
      "loss ->  2.580991744995117\n",
      "loss ->  3.1607706546783447\n",
      "loss ->  3.0372302532196045\n",
      "loss ->  2.76653790473938\n",
      "loss ->  3.89206862449646\n",
      "loss ->  3.11669659614563\n",
      "loss ->  2.5390701293945312\n",
      "loss ->  3.18613338470459\n",
      "loss ->  3.2828238010406494\n",
      "loss ->  3.9045891761779785\n",
      "loss ->  3.8895890712738037\n",
      "loss ->  3.2940919399261475\n",
      "loss ->  2.903165340423584\n",
      "loss ->  3.5948307514190674\n",
      "loss ->  3.2691869735717773\n",
      "loss ->  3.8782858848571777\n",
      "loss ->  2.4507384300231934\n",
      "loss ->  2.899064540863037\n",
      "loss ->  3.2104170322418213\n",
      "loss ->  3.875767469406128\n",
      "loss ->  2.953890562057495\n",
      "loss ->  3.318324327468872\n",
      "loss ->  3.5031399726867676\n",
      "loss ->  3.918184518814087\n",
      "loss ->  2.6587975025177\n",
      "loss ->  3.493816614151001\n",
      "loss ->  3.1445326805114746\n",
      "loss ->  2.6670405864715576\n",
      "loss ->  3.5018699169158936\n",
      "loss ->  2.7744805812835693\n",
      "loss ->  3.4398813247680664\n",
      "loss ->  4.15700626373291\n",
      "loss ->  4.149272441864014\n",
      "loss ->  3.4980130195617676\n",
      "loss ->  3.475013494491577\n",
      "loss ->  3.302609443664551\n",
      "loss ->  3.469820499420166\n",
      "loss ->  3.1171555519104004\n",
      "loss ->  3.6067841053009033\n",
      "loss ->  3.37312912940979\n",
      "loss ->  3.5952377319335938\n",
      "loss ->  3.1755306720733643\n",
      "loss ->  3.0795395374298096\n",
      "loss ->  4.160619735717773\n",
      "loss ->  3.53298282623291\n",
      "loss ->  2.9294705390930176\n",
      "loss ->  3.0547683238983154\n",
      "loss ->  3.4228270053863525\n",
      "loss ->  3.0054006576538086\n",
      "loss ->  2.8918819427490234\n",
      "loss ->  3.2462806701660156\n",
      "loss ->  3.4429800510406494\n",
      "loss ->  3.21826434135437\n",
      "loss ->  3.726318120956421\n",
      "loss ->  2.945180892944336\n",
      "loss ->  2.862442970275879\n",
      "loss ->  3.3344526290893555\n",
      "loss ->  2.639620780944824\n",
      "loss ->  2.7419192790985107\n",
      "loss ->  3.3645682334899902\n",
      "loss ->  3.454911708831787\n",
      "loss ->  3.484628677368164\n",
      "loss ->  3.4095263481140137\n",
      "loss ->  2.692943811416626\n",
      "loss ->  2.835583209991455\n",
      "loss ->  3.420522689819336\n",
      "loss ->  3.330355405807495\n",
      "loss ->  3.230664014816284\n",
      "loss ->  3.0387911796569824\n",
      "loss ->  3.8460147380828857\n",
      "loss ->  2.744225263595581\n",
      "loss ->  2.966892719268799\n",
      "loss ->  2.8835644721984863\n",
      "loss ->  3.4335474967956543\n",
      "loss ->  3.545564651489258\n",
      "loss ->  3.193394184112549\n",
      "loss ->  3.2768657207489014\n",
      "loss ->  3.448354959487915\n",
      "loss ->  3.6255784034729004\n",
      "loss ->  3.3810760974884033\n",
      "loss ->  2.5516257286071777\n",
      "loss ->  3.042942523956299\n",
      "loss ->  3.5612382888793945\n",
      "loss ->  3.162236213684082\n",
      "loss ->  2.2643837928771973\n",
      "loss ->  3.47698712348938\n",
      "loss ->  2.7673888206481934\n",
      "loss ->  2.8911428451538086\n",
      "loss ->  3.1802992820739746\n",
      "loss ->  3.4046716690063477\n",
      "loss ->  3.714977979660034\n",
      "loss ->  3.1531057357788086\n",
      "loss ->  2.7186851501464844\n",
      "loss ->  3.0809733867645264\n",
      "loss ->  2.608599901199341\n",
      "loss ->  3.5245776176452637\n",
      "loss ->  3.1048929691314697\n",
      "loss ->  3.2457993030548096\n",
      "loss ->  2.8607876300811768\n",
      "loss ->  3.766359329223633\n",
      "loss ->  2.593019723892212\n",
      "loss ->  2.964787244796753\n",
      "loss ->  2.9367945194244385\n",
      "loss ->  2.8251821994781494\n",
      "loss ->  3.965470314025879\n",
      "loss ->  2.7770209312438965\n",
      "loss ->  2.7155261039733887\n",
      "loss ->  2.8119187355041504\n",
      "loss ->  4.07239294052124\n",
      "loss ->  2.6716575622558594\n",
      "loss ->  2.8615334033966064\n",
      "loss ->  2.3785853385925293\n",
      "loss ->  3.1340465545654297\n",
      "loss ->  3.2015042304992676\n",
      "loss ->  2.9613327980041504\n",
      "loss ->  2.8438849449157715\n",
      "loss ->  3.4401779174804688\n",
      "loss ->  2.6163172721862793\n",
      "loss ->  3.0279035568237305\n",
      "loss ->  3.5774741172790527\n",
      "loss ->  3.1923465728759766\n",
      "loss ->  2.9437599182128906\n",
      "loss ->  2.5912017822265625\n",
      "loss ->  2.63431453704834\n",
      "loss ->  2.9202075004577637\n",
      "loss ->  3.6314358711242676\n",
      "loss ->  3.125645399093628\n",
      "loss ->  3.0352964401245117\n",
      "loss ->  3.3581361770629883\n",
      "loss ->  3.5440590381622314\n",
      "loss ->  3.214960813522339\n",
      "loss ->  2.967139959335327\n",
      "loss ->  2.9178991317749023\n",
      "loss ->  3.730821132659912\n",
      "loss ->  4.304125785827637\n",
      "loss ->  3.449841022491455\n",
      "loss ->  2.790851593017578\n",
      "loss ->  3.3345911502838135\n",
      "loss ->  3.5831680297851562\n",
      "loss ->  3.0527286529541016\n",
      "loss ->  3.8551909923553467\n",
      "loss ->  3.098205804824829\n",
      "loss ->  3.1330714225769043\n",
      "loss ->  2.9267561435699463\n",
      "loss ->  2.7501022815704346\n",
      "loss ->  2.9686880111694336\n",
      "loss ->  3.7682666778564453\n",
      "loss ->  3.380845069885254\n",
      "loss ->  3.0787711143493652\n",
      "loss ->  3.3083508014678955\n",
      "loss ->  3.359957456588745\n",
      "loss ->  2.775068998336792\n",
      "loss ->  3.817819118499756\n",
      "loss ->  2.9127230644226074\n",
      "loss ->  2.8100974559783936\n",
      "loss ->  3.759274482727051\n",
      "loss ->  2.9079368114471436\n",
      "loss ->  3.6571779251098633\n",
      "loss ->  3.088179111480713\n",
      "loss ->  2.7800421714782715\n",
      "loss ->  3.3300602436065674\n",
      "loss ->  2.8166303634643555\n",
      "loss ->  3.2637038230895996\n",
      "loss ->  2.9513518810272217\n",
      "loss ->  3.146315336227417\n",
      "loss ->  2.9659812450408936\n",
      "loss ->  3.76674222946167\n",
      "loss ->  3.274801015853882\n",
      "loss ->  2.994007110595703\n",
      "loss ->  2.8452961444854736\n",
      "loss ->  3.1700592041015625\n",
      "loss ->  2.5277836322784424\n",
      "loss ->  3.5384292602539062\n",
      "loss ->  3.2813851833343506\n",
      "loss ->  3.7070746421813965\n",
      "loss ->  3.2810707092285156\n",
      "loss ->  3.2672007083892822\n",
      "loss ->  3.8170318603515625\n",
      "loss ->  3.3085169792175293\n",
      "loss ->  3.508849859237671\n",
      "loss ->  3.6739144325256348\n",
      "loss ->  3.466618537902832\n",
      "loss ->  4.152522087097168\n",
      "loss ->  3.8739495277404785\n",
      "loss ->  4.1892876625061035\n",
      "loss ->  4.470331192016602\n",
      "loss ->  3.0459189414978027\n",
      "loss ->  3.260197162628174\n",
      "loss ->  3.1367969512939453\n",
      "loss ->  4.675813674926758\n",
      "loss ->  3.9593868255615234\n",
      "loss ->  4.882221698760986\n",
      "loss ->  3.7124969959259033\n",
      "loss ->  4.858310699462891\n",
      "loss ->  3.6271157264709473\n",
      "loss ->  5.091170310974121\n",
      "loss ->  3.7925586700439453\n",
      "loss ->  4.299531936645508\n",
      "loss ->  4.043663501739502\n",
      "loss ->  3.1904590129852295\n",
      "loss ->  2.9904682636260986\n",
      "loss ->  3.649848222732544\n",
      "loss ->  3.76965594291687\n",
      "loss ->  2.9202487468719482\n",
      "loss ->  3.592177152633667\n",
      "loss ->  3.552839756011963\n",
      "loss ->  3.5609054565429688\n",
      "loss ->  3.550128698348999\n",
      "loss ->  3.7930448055267334\n",
      "loss ->  4.409440994262695\n",
      "loss ->  3.413287401199341\n",
      "loss ->  3.514216899871826\n",
      "loss ->  3.409388303756714\n",
      "loss ->  3.204629421234131\n",
      "loss ->  3.2305498123168945\n",
      "loss ->  3.59885835647583\n",
      "loss ->  3.4956109523773193\n",
      "loss ->  3.035449266433716\n",
      "loss ->  3.332904100418091\n",
      "loss ->  3.9878814220428467\n",
      "loss ->  4.4899163246154785\n",
      "loss ->  4.292756080627441\n",
      "loss ->  3.397209882736206\n",
      "loss ->  3.756153106689453\n",
      "loss ->  3.770293951034546\n",
      "loss ->  4.743653774261475\n",
      "loss ->  5.4470696449279785\n",
      "loss ->  3.5859713554382324\n",
      "loss ->  3.972508430480957\n",
      "loss ->  3.2150394916534424\n",
      "loss ->  3.4022979736328125\n",
      "loss ->  4.057125091552734\n",
      "loss ->  3.711275339126587\n",
      "loss ->  3.191998243331909\n",
      "loss ->  3.0543110370635986\n",
      "loss ->  3.8198838233947754\n",
      "loss ->  3.846705675125122\n",
      "loss ->  3.595106363296509\n",
      "loss ->  3.140377998352051\n",
      "loss ->  5.508676528930664\n",
      "loss ->  5.223338603973389\n",
      "loss ->  4.362502098083496\n",
      "loss ->  3.716207265853882\n",
      "loss ->  3.5422005653381348\n",
      "loss ->  4.363816261291504\n",
      "loss ->  3.5181548595428467\n",
      "loss ->  3.6341848373413086\n",
      "loss ->  5.891912937164307\n",
      "loss ->  4.551084518432617\n",
      "loss ->  5.47654390335083\n",
      "loss ->  4.6754961013793945\n",
      "loss ->  4.987699031829834\n",
      "loss ->  6.901597499847412\n",
      "loss ->  4.645278453826904\n",
      "loss ->  4.59131383895874\n",
      "loss ->  4.946453094482422\n",
      "loss ->  5.036199569702148\n",
      "loss ->  3.9163286685943604\n",
      "loss ->  4.979222297668457\n",
      "loss ->  4.089341640472412\n",
      "loss ->  3.4802207946777344\n",
      "loss ->  4.599471569061279\n",
      "loss ->  3.8047075271606445\n",
      "loss ->  4.378511428833008\n",
      "loss ->  4.472630500793457\n",
      "loss ->  4.891645431518555\n",
      "loss ->  4.5729851722717285\n",
      "loss ->  3.873236656188965\n",
      "loss ->  4.397573947906494\n",
      "loss ->  5.228333950042725\n",
      "loss ->  5.76754093170166\n",
      "loss ->  4.477144718170166\n",
      "loss ->  3.300691843032837\n",
      "loss ->  4.687340259552002\n",
      "loss ->  3.8033459186553955\n",
      "loss ->  4.339417457580566\n",
      "loss ->  5.380173683166504\n",
      "loss ->  6.244886875152588\n",
      "loss ->  4.214104652404785\n",
      "loss ->  3.5523393154144287\n",
      "loss ->  5.8475141525268555\n",
      "loss ->  4.714795112609863\n",
      "loss ->  5.294944763183594\n",
      "loss ->  4.81102180480957\n",
      "loss ->  4.756349563598633\n",
      "loss ->  4.378352642059326\n",
      "loss ->  4.232538223266602\n",
      "loss ->  5.001258850097656\n",
      "loss ->  6.236527919769287\n",
      "loss ->  5.119393348693848\n",
      "loss ->  8.068127632141113\n",
      "loss ->  6.704117298126221\n",
      "loss ->  6.780434608459473\n",
      "loss ->  4.952159404754639\n",
      "loss ->  4.663125038146973\n",
      "loss ->  4.452433109283447\n",
      "loss ->  4.514344692230225\n",
      "loss ->  2.7692041397094727\n",
      "loss ->  3.822573184967041\n",
      "loss ->  4.7091193199157715\n",
      "loss ->  4.703799247741699\n",
      "loss ->  4.7177510261535645\n",
      "loss ->  5.12073278427124\n",
      "loss ->  5.071234226226807\n",
      "loss ->  5.204378604888916\n",
      "loss ->  9.783337593078613\n",
      "loss ->  5.525564193725586\n",
      "loss ->  9.206680297851562\n",
      "loss ->  4.6715850830078125\n",
      "loss ->  4.783869743347168\n",
      "loss ->  5.978507041931152\n",
      "loss ->  5.387856960296631\n",
      "loss ->  6.59554386138916\n",
      "loss ->  5.009989261627197\n",
      "loss ->  5.910703182220459\n",
      "loss ->  7.126165390014648\n",
      "loss ->  6.06220817565918\n",
      "loss ->  5.985751152038574\n",
      "loss ->  5.242925643920898\n",
      "loss ->  6.947595119476318\n",
      "loss ->  5.022735595703125\n",
      "loss ->  6.982889175415039\n",
      "loss ->  4.867603778839111\n",
      "loss ->  6.246865272521973\n",
      "loss ->  6.658622741699219\n",
      "loss ->  8.758871078491211\n",
      "loss ->  6.546969890594482\n",
      "loss ->  7.10567045211792\n",
      "loss ->  7.366309642791748\n",
      "loss ->  6.996142387390137\n",
      "loss ->  4.790984153747559\n",
      "loss ->  5.119333744049072\n",
      "loss ->  5.474555492401123\n",
      "loss ->  6.9528422355651855\n",
      "loss ->  7.895773887634277\n",
      "loss ->  7.046011924743652\n",
      "loss ->  5.105966567993164\n",
      "loss ->  5.182511806488037\n",
      "loss ->  8.114768981933594\n",
      "loss ->  9.045519828796387\n",
      "loss ->  10.998802185058594\n",
      "loss ->  7.15981912612915\n",
      "loss ->  7.604252338409424\n",
      "loss ->  8.8951997756958\n",
      "loss ->  5.781667232513428\n",
      "loss ->  5.456396579742432\n",
      "loss ->  8.0549955368042\n",
      "loss ->  6.587028503417969\n",
      "loss ->  8.247594833374023\n",
      "loss ->  5.333600044250488\n",
      "loss ->  6.921684265136719\n",
      "loss ->  5.651225566864014\n",
      "loss ->  6.472898483276367\n",
      "loss ->  7.367962837219238\n",
      "loss ->  15.139542579650879\n",
      "loss ->  10.32310676574707\n",
      "loss ->  8.065311431884766\n",
      "loss ->  13.752946853637695\n",
      "loss ->  9.142930030822754\n",
      "loss ->  6.2457275390625\n",
      "loss ->  9.70619010925293\n",
      "loss ->  8.683158874511719\n",
      "loss ->  10.840786933898926\n",
      "loss ->  8.941400527954102\n",
      "loss ->  9.203361511230469\n",
      "loss ->  6.529238700866699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8150c84070>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# track the learning rates in respect to its loss\n",
    "lr_i = []\n",
    "loss_i = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data.add_(-lr * p.grad)\n",
    "\n",
    "    lr_i.append(lr)\n",
    "    loss_i.append(loss.item())\n",
    "\n",
    "plt.plot(lr_i, loss_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the learning rate was stable between 0.0 and 0.1, and after that it became unstable. So we have narrowed down the learning rate's range in respect to minimized loss function. So we can choose 0.1 safely now and also increase the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    # print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.4760119915008545\n"
     ]
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print('loss -> ', loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Decay\n",
    "\n",
    "One more thing to remember is that, once we found our optimized learning rate, we can run the training with it a few times. But then you realize the loss is not moving lower by much. In that case, on a trained NN, you can further reduce the learning rate, by a factor of 10, and continue thee training with same amount of iterations. You can continue this until you reach a plateau. This approach is called learning decay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split - Train/Validate/Test\n",
    "\n",
    "If you increase your parameters to exceed your input size, you will end up over-fitting the data. Meaning that, your NN will memorize the data and outputs exactly what it saw, rather then being creative and output something new. \n",
    "\n",
    "To make sure we are not over-fitting or under-fitting we can split the data into training (80%), validation/dev (10%), and testing sets (10%). \n",
    "\n",
    "Validation is used to find the best hyper-params and settings of the NN. You can try multiple variations to evaluate which one is best for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of chars and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s2i = {s:i+1 for i,s in enumerate(chars)}\n",
    "s2i['.'] = 0\n",
    "i2s = {i:s for s,i in s2i.items()}\n",
    "vocab_size = len(i2s)\n",
    "\n",
    "# MPL model\n",
    "n_emb = 10 # the dimension of the character embedding vector\n",
    "n_hidden = 200 # The number of neurons in the hidden layer of MLP\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_emb),                generator=g, requires_grad=True)\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden),    generator=g, requires_grad=True)\n",
    "b1 = torch.randn((n_hidden),                        generator=g, requires_grad=True)\n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g, requires_grad=True)\n",
    "b2 = torch.randn((vocab_size),                      generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# split the dataset randomly for train, dev and test\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words)) # 80% of the words\n",
    "n2 = int(0.9 * len(words)) # 90% of the words\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train on 80% of the words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # validate on 10% of the words\n",
    "Xtst, Ytst = build_dataset(words[n2:]) # test on 10% of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "def train_NN(X,Y):\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # mini-batch\n",
    "        ix = torch.randint(0, X.shape[0], (batch_size,), generator=g) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "        h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "        # print('loss -> ', loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay after 100k steps\n",
    "        for p in parameters:\n",
    "            p.data.add_(-lr * p.grad)\n",
    "\n",
    "        # track stats\n",
    "        if i % 1000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def getLoss(X,Y):\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(emb.shape[0], -1) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "   1000/ 200000: 4.2208\n",
      "   2000/ 200000: 2.9934\n",
      "   3000/ 200000: 2.9829\n",
      "   4000/ 200000: 2.2823\n",
      "   5000/ 200000: 2.4180\n",
      "   6000/ 200000: 2.4152\n",
      "   7000/ 200000: 2.8149\n",
      "   8000/ 200000: 2.5133\n",
      "   9000/ 200000: 2.5542\n",
      "  10000/ 200000: 2.8619\n",
      "  11000/ 200000: 2.0939\n",
      "  12000/ 200000: 2.4401\n",
      "  13000/ 200000: 2.4719\n",
      "  14000/ 200000: 2.9341\n",
      "  15000/ 200000: 2.3488\n",
      "  16000/ 200000: 2.6091\n",
      "  17000/ 200000: 2.7315\n",
      "  18000/ 200000: 2.4622\n",
      "  19000/ 200000: 2.4676\n",
      "  20000/ 200000: 2.5443\n",
      "  21000/ 200000: 2.6354\n",
      "  22000/ 200000: 2.1334\n",
      "  23000/ 200000: 2.2140\n",
      "  24000/ 200000: 3.0568\n",
      "  25000/ 200000: 2.3262\n",
      "  26000/ 200000: 2.5554\n",
      "  27000/ 200000: 2.6473\n",
      "  28000/ 200000: 2.3433\n",
      "  29000/ 200000: 3.2092\n",
      "  30000/ 200000: 2.8801\n",
      "  31000/ 200000: 2.6749\n",
      "  32000/ 200000: 2.5013\n",
      "  33000/ 200000: 2.5535\n",
      "  34000/ 200000: 2.1968\n",
      "  35000/ 200000: 2.5608\n",
      "  36000/ 200000: 2.1742\n",
      "  37000/ 200000: 2.2611\n",
      "  38000/ 200000: 2.9491\n",
      "  39000/ 200000: 2.2258\n",
      "  40000/ 200000: 2.0870\n",
      "  41000/ 200000: 2.6509\n",
      "  42000/ 200000: 2.7214\n",
      "  43000/ 200000: 2.5077\n",
      "  44000/ 200000: 1.9335\n",
      "  45000/ 200000: 1.9379\n",
      "  46000/ 200000: 2.3766\n",
      "  47000/ 200000: 2.0148\n",
      "  48000/ 200000: 2.5559\n",
      "  49000/ 200000: 2.1091\n",
      "  50000/ 200000: 2.4970\n",
      "  51000/ 200000: 2.8095\n",
      "  52000/ 200000: 2.3097\n",
      "  53000/ 200000: 2.5160\n",
      "  54000/ 200000: 2.0923\n",
      "  55000/ 200000: 2.0019\n",
      "  56000/ 200000: 2.0314\n",
      "  57000/ 200000: 1.6952\n",
      "  58000/ 200000: 1.7215\n",
      "  59000/ 200000: 2.2863\n",
      "  60000/ 200000: 2.3531\n",
      "  61000/ 200000: 2.2811\n",
      "  62000/ 200000: 2.5759\n",
      "  63000/ 200000: 2.5570\n",
      "  64000/ 200000: 2.3393\n",
      "  65000/ 200000: 2.1078\n",
      "  66000/ 200000: 2.7216\n",
      "  67000/ 200000: 2.6313\n",
      "  68000/ 200000: 1.8496\n",
      "  69000/ 200000: 2.2077\n",
      "  70000/ 200000: 2.0899\n",
      "  71000/ 200000: 2.4715\n",
      "  72000/ 200000: 2.4842\n",
      "  73000/ 200000: 2.4365\n",
      "  74000/ 200000: 1.9462\n",
      "  75000/ 200000: 2.6709\n",
      "  76000/ 200000: 1.9228\n",
      "  77000/ 200000: 2.4907\n",
      "  78000/ 200000: 2.4087\n",
      "  79000/ 200000: 2.1045\n",
      "  80000/ 200000: 2.2251\n",
      "  81000/ 200000: 2.2868\n",
      "  82000/ 200000: 2.2953\n",
      "  83000/ 200000: 1.8582\n",
      "  84000/ 200000: 2.6504\n",
      "  85000/ 200000: 2.6792\n",
      "  86000/ 200000: 2.4411\n",
      "  87000/ 200000: 2.2592\n",
      "  88000/ 200000: 2.3983\n",
      "  89000/ 200000: 2.3576\n",
      "  90000/ 200000: 2.2938\n",
      "  91000/ 200000: 2.1120\n",
      "  92000/ 200000: 2.6734\n",
      "  93000/ 200000: 1.9373\n",
      "  94000/ 200000: 1.9838\n",
      "  95000/ 200000: 2.2660\n",
      "  96000/ 200000: 2.1820\n",
      "  97000/ 200000: 2.3315\n",
      "  98000/ 200000: 1.9951\n",
      "  99000/ 200000: 2.2031\n",
      " 100000/ 200000: 2.0505\n",
      " 101000/ 200000: 1.8468\n",
      " 102000/ 200000: 2.2829\n",
      " 103000/ 200000: 2.1208\n",
      " 104000/ 200000: 2.2316\n",
      " 105000/ 200000: 2.1003\n",
      " 106000/ 200000: 2.4503\n",
      " 107000/ 200000: 2.1897\n",
      " 108000/ 200000: 2.1861\n",
      " 109000/ 200000: 2.2914\n",
      " 110000/ 200000: 2.3233\n",
      " 111000/ 200000: 2.2460\n",
      " 112000/ 200000: 1.9179\n",
      " 113000/ 200000: 2.4554\n",
      " 114000/ 200000: 2.1865\n",
      " 115000/ 200000: 2.2684\n",
      " 116000/ 200000: 2.1883\n",
      " 117000/ 200000: 2.3121\n",
      " 118000/ 200000: 2.2531\n",
      " 119000/ 200000: 2.3287\n",
      " 120000/ 200000: 1.9138\n",
      " 121000/ 200000: 2.3233\n",
      " 122000/ 200000: 2.2421\n",
      " 123000/ 200000: 2.5611\n",
      " 124000/ 200000: 2.8363\n",
      " 125000/ 200000: 1.9914\n",
      " 126000/ 200000: 2.3399\n",
      " 127000/ 200000: 2.2281\n",
      " 128000/ 200000: 2.3658\n",
      " 129000/ 200000: 2.1814\n",
      " 130000/ 200000: 2.4587\n",
      " 131000/ 200000: 1.8899\n",
      " 132000/ 200000: 1.8009\n",
      " 133000/ 200000: 2.2614\n",
      " 134000/ 200000: 2.1432\n",
      " 135000/ 200000: 2.0901\n",
      " 136000/ 200000: 1.9836\n",
      " 137000/ 200000: 2.0067\n",
      " 138000/ 200000: 2.1657\n",
      " 139000/ 200000: 1.9864\n",
      " 140000/ 200000: 2.1852\n",
      " 141000/ 200000: 1.6911\n",
      " 142000/ 200000: 2.0869\n",
      " 143000/ 200000: 2.3377\n",
      " 144000/ 200000: 2.1629\n",
      " 145000/ 200000: 2.0586\n",
      " 146000/ 200000: 2.0977\n",
      " 147000/ 200000: 2.2760\n",
      " 148000/ 200000: 2.4775\n",
      " 149000/ 200000: 2.4470\n",
      " 150000/ 200000: 2.1685\n",
      " 151000/ 200000: 2.4384\n",
      " 152000/ 200000: 1.9039\n",
      " 153000/ 200000: 2.2755\n",
      " 154000/ 200000: 2.2780\n",
      " 155000/ 200000: 2.0809\n",
      " 156000/ 200000: 1.9429\n",
      " 157000/ 200000: 2.1572\n",
      " 158000/ 200000: 2.0704\n",
      " 159000/ 200000: 2.2378\n",
      " 160000/ 200000: 2.1231\n",
      " 161000/ 200000: 2.0540\n",
      " 162000/ 200000: 1.9258\n",
      " 163000/ 200000: 2.0323\n",
      " 164000/ 200000: 2.1506\n",
      " 165000/ 200000: 2.3690\n",
      " 166000/ 200000: 2.2092\n",
      " 167000/ 200000: 2.3640\n",
      " 168000/ 200000: 1.8544\n",
      " 169000/ 200000: 2.3253\n",
      " 170000/ 200000: 1.8288\n",
      " 171000/ 200000: 2.3161\n",
      " 172000/ 200000: 2.1267\n",
      " 173000/ 200000: 2.2176\n",
      " 174000/ 200000: 2.3718\n",
      " 175000/ 200000: 1.9058\n",
      " 176000/ 200000: 1.8792\n",
      " 177000/ 200000: 2.0864\n",
      " 178000/ 200000: 1.9006\n",
      " 179000/ 200000: 2.1148\n",
      " 180000/ 200000: 2.0834\n",
      " 181000/ 200000: 2.1133\n",
      " 182000/ 200000: 2.2690\n",
      " 183000/ 200000: 2.2654\n",
      " 184000/ 200000: 2.5008\n",
      " 185000/ 200000: 2.2572\n",
      " 186000/ 200000: 1.8644\n",
      " 187000/ 200000: 1.7568\n",
      " 188000/ 200000: 2.1368\n",
      " 189000/ 200000: 2.3998\n",
      " 190000/ 200000: 1.8222\n",
      " 191000/ 200000: 2.2477\n",
      " 192000/ 200000: 2.4226\n",
      " 193000/ 200000: 2.1188\n",
      " 194000/ 200000: 2.0034\n",
      " 195000/ 200000: 2.2530\n",
      " 196000/ 200000: 2.2179\n",
      " 197000/ 200000: 2.1540\n",
      " 198000/ 200000: 1.9826\n",
      " 199000/ 200000: 1.8020\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_NN(Xtr, Ytr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the loss of the training dataset and validation set are too different, that means that our NN hasn't learned much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how the embeddings change after randomly setting them up and training the NN, run below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.1294455528259277\n",
      "loss ->  2.173779249191284\n"
     ]
    }
   ],
   "source": [
    "# training data loss\n",
    "getLoss(Xtr, Ytr)\n",
    "# validation loss\n",
    "getLoss(Xdev, Ydev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the embeddings\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), i2s[i], ha='center', va='center', color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naitabree.\n",
      "jaezi.\n",
      "aje.\n",
      "maxanley.\n",
      "lei.\n",
      "jacaraanly.\n",
      "erilameenah.\n",
      "ell.\n",
      "adalisa.\n",
      "ere.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 1000)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] \n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1) \n",
    "        logits = h @ W2 + b2 \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0: # stop at the end of the word\n",
    "            break\n",
    "    print(''.join(i2s[i] for i in out))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at this point you can start working on the possible configuration tunings and improve your loos and validations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
