{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrej Karpathy's /makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://www.youtube.com/watch?v=TCH_1BHY58I\n",
    "\n",
    "https://github.com/karpathy/makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bigram char level model, we only used two chars which created a 27x27 data space. if we move deeper in this approach to enhance the loss function and the model itself, the only avenue was to explor adding more dimensions, i.e. 27x27x27. however this path suddenly explodes in terms of data and parameters that we want to use for this model.\n",
    "\n",
    "Therefore, we need to explore a better model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)\n",
    "#### Bengio et al. 2003\n",
    "\n",
    "This is another char level model to predict the next char, however the paper is based on word predictions. \n",
    "\n",
    "The proposed approach is to take 'w' number of words, and associate to each word, 'm' number of feature vectors. Meaning that, each word is embedded in a 'm' dimensional feature space. Initially these words are initialized randomly but later we'll tune them using backpropagation. \n",
    "\n",
    "To imagine this approach, think about words that are similar or synonyms. They will end up in the same part of the space. And those that are different will be separated. \n",
    "\n",
    "The modeling approach is similar to the NN approach for Bigram. They use multi-layer NN to predict the next words, given the previous words. To train the NN, they ```maximize the log-likelihood of the training data```.\n",
    "\n",
    "Let's look at an ```example``` for this approach. Assume, we are not given the sentence \"A dog was running in a room\". But now for testing the model we are providing it with \"A dog was running in a ...\" and expecting the model to fill in the blank. Since it hasn't seen this exact sentence, we call it, ```out of distribution```. However, MLP doesn't need to have seen the exact words to predict 'room' for the blank. Because it might have seen \"The dog was running in a room\" and based on the learnings, it has put the embeddings of 'The' and 'A' near by each other in the space. So now that we are asking it to fill the blank based on \"A dog was running in a ...\", it will match it up with \"The dog was running in a room\". This is called ```knowledge transfer```.\n",
    "\n",
    "Let's look at the ```architecture``` of this approach. \n",
    "\n",
    "Assume the NN's input, takes 3 previous-words. And the output is the fourth word. Each of the incoming words, will go through a look-up table, to match up the corresponding embedding ('m' feature vector) for that word. So there will be $3 \\times m$ neurons holding the 3 words. \n",
    "\n",
    "Then we need to build a hidden layer. The size is a ```hyper-parameter```. Meaning that, we need to come up with the right size based on try-error. So all the input neurons goes into the hidden layer. And there will be a ```tanh``` function applied for non-linearity. \n",
    "\n",
    "The output layer is a huge one, because the number of neurons is equivalent to $w$, the number of words in our data set. All the neurons in the hidden layer are connected to the output neurons. That's why there will be lots of params in between these two layers, and therefore, it's going to be computationally expensive. On top of the output layer we have ```softmax``` (exponentiate the logits and normalize, so that it will sum up to 1). This way, we'll get a nice probability distribution for the next word in the sequence. \n",
    "\n",
    "During training, because we have xs and ys, we will get the probability for each x and minimize the NN's loss by improving the parameters. The optimization used here is also ```backpropagation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# read from another package while we are in a separate package\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "file_path = os.path.join(parent_directory, 'opensource/makemore', 'names.txt')\n",
    "\n",
    "words = open(file_path, 'r').read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of chars and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "s2i = {s:i+1 for i,s in enumerate(chars)}\n",
    "s2i['.'] = 0\n",
    "i2s = {i:s for s,i in s2i.items()}\n",
    "print(i2s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words[:5]: # the examples we can generate from the first 5 words\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(i2s[i] for i in context), '--->', i2s[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Embeddings\n",
    "\n",
    "the paper used 70000 words with 30 embeddings. we have 27 chars, so we'll go with 2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3004, -0.6561])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3004, -0.6561],\n",
       "        [-0.3903,  0.2140],\n",
       "        [ 0.6566, -0.8840]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve embeddings with a list of lookups\n",
    "C[[5,6,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-1.3004, -0.6561]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-1.3004, -0.6561],\n",
       "         [ 1.0227,  0.9855]],\n",
       "\n",
       "        [[-1.3004, -0.6561],\n",
       "         [ 1.0227,  0.9855],\n",
       "         [ 1.0227,  0.9855]],\n",
       "\n",
       "        [[ 1.0227,  0.9855],\n",
       "         [ 1.0227,  0.9855],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [ 0.0346,  0.3226]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [ 0.0346,  0.3226],\n",
       "         [-2.1583, -0.1153]],\n",
       "\n",
       "        [[ 0.0346,  0.3226],\n",
       "         [-2.1583, -0.1153],\n",
       "         [ 0.2075,  0.1068]],\n",
       "\n",
       "        [[-2.1583, -0.1153],\n",
       "         [ 0.2075,  0.1068],\n",
       "         [ 1.1989,  0.2576]],\n",
       "\n",
       "        [[ 0.2075,  0.1068],\n",
       "         [ 1.1989,  0.2576],\n",
       "         [ 0.2075,  0.1068]],\n",
       "\n",
       "        [[ 1.1989,  0.2576],\n",
       "         [ 0.2075,  0.1068],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [ 0.6858, -1.1174],\n",
       "         [ 1.1989,  0.2576]],\n",
       "\n",
       "        [[ 0.6858, -1.1174],\n",
       "         [ 1.1989,  0.2576],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [ 0.2075,  0.1068]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [ 0.2075,  0.1068],\n",
       "         [-0.2595,  0.4304]],\n",
       "\n",
       "        [[ 0.2075,  0.1068],\n",
       "         [-0.2595,  0.4304],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.2595,  0.4304],\n",
       "         [ 0.6858, -1.1174],\n",
       "         [ 0.1293,  0.7245]],\n",
       "\n",
       "        [[ 0.6858, -1.1174],\n",
       "         [ 0.1293,  0.7245],\n",
       "         [-1.3004, -0.6561]],\n",
       "\n",
       "        [[ 0.1293,  0.7245],\n",
       "         [-1.3004, -0.6561],\n",
       "         [-2.1583, -0.1153]],\n",
       "\n",
       "        [[-1.3004, -0.6561],\n",
       "         [-2.1583, -0.1153],\n",
       "         [-2.1583, -0.1153]],\n",
       "\n",
       "        [[-2.1583, -0.1153],\n",
       "         [-2.1583, -0.1153],\n",
       "         [ 0.6858, -1.1174]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.5074, -1.8076],\n",
       "         [-0.2595,  0.4304]],\n",
       "\n",
       "        [[-0.5074, -1.8076],\n",
       "         [-0.2595,  0.4304],\n",
       "         [ 0.0346,  0.3226]],\n",
       "\n",
       "        [[-0.2595,  0.4304],\n",
       "         [ 0.0346,  0.3226],\n",
       "         [-1.2582,  0.6796]],\n",
       "\n",
       "        [[ 0.0346,  0.3226],\n",
       "         [-1.2582,  0.6796],\n",
       "         [ 0.6566, -0.8840]],\n",
       "\n",
       "        [[-1.2582,  0.6796],\n",
       "         [ 0.6566, -0.8840],\n",
       "         [ 0.2075,  0.1068]],\n",
       "\n",
       "        [[ 0.6566, -0.8840],\n",
       "         [ 0.2075,  0.1068],\n",
       "         [ 0.6858, -1.1174]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# therefore this works\n",
    "emb = C[X]\n",
    "emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_hyperparameter_size = 100\n",
    "num_of_words = 3\n",
    "num_of_embeddings = 2\n",
    "num_of_inputs = num_of_words * num_of_embeddings\n",
    "\n",
    "w1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size))\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wee want to setup the tensor's shapes in such a way that ```emb @ w1 + b1``` would work.\n",
    "\n",
    "http://blog.ezyang.com/2019/05/pytorch-internals/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6134, -3.1258,  0.7510,  ..., -0.8199, -9.1942, -2.6622],\n",
       "        [-2.8158, -0.8962,  0.6558,  ..., -0.4174, -7.9463, -2.2584],\n",
       "        [-2.4356, -1.1802,  1.7480,  ..., -0.9424,  0.1159, -5.3092],\n",
       "        ...,\n",
       "        [-0.3765, -1.2361,  1.2702,  ...,  0.4268, -1.0929, -5.3421],\n",
       "        [ 0.9624,  2.8507, -0.3402,  ..., -1.6579, -3.0843, -2.4998],\n",
       "        [-0.3146, -2.7932,  0.6544,  ..., -0.6907, -0.2089,  0.6663]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_size = emb.shape[0] # or use -1 for pytorch to figure it out\n",
    "emb.view(x_size, num_of_inputs) @ w1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9893, -0.9962,  0.6357,  ..., -0.6750, -1.0000, -0.9903],\n",
       "        [-0.9929, -0.7144,  0.5756,  ..., -0.3948, -1.0000, -0.9784],\n",
       "        [-0.9848, -0.8275,  0.9411,  ..., -0.7363,  0.1154, -1.0000],\n",
       "        ...,\n",
       "        [-0.3596, -0.8443,  0.8539,  ...,  0.4027, -0.7979, -1.0000],\n",
       "        [ 0.7453,  0.9933, -0.3277,  ..., -0.9299, -0.9958, -0.9866],\n",
       "        [-0.3046, -0.9925,  0.5747,  ..., -0.5984, -0.2060,  0.5826]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ w1 + b1) # added tanh to bring all the values between -1 and 1 for non-linearity\n",
    "h "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output layer\n",
    "w2 = torch.randn((hidden_layer_hyperparameter_size, 27))\n",
    "b2 = torch.randn((27))\n",
    "logits = h @ w2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # get fake counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize to get the probabilities\n",
    "probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proof of normalized probs is to check if every row sums up to =1\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8062e-15, 2.4791e-06, 2.2159e-06, 4.8267e-13, 3.8968e-05, 2.4880e-10,\n",
       "        2.4705e-11, 3.0781e-11, 1.1198e-05, 2.6618e-02, 1.0043e-10, 5.2040e-05,\n",
       "        4.4300e-10, 3.5379e-02, 1.0168e-09, 1.1452e-06, 5.7345e-12, 2.7056e-01,\n",
       "        3.6719e-10, 1.1366e-09, 9.9913e-03, 1.0114e-04, 8.1555e-09, 8.3999e-06,\n",
       "        5.9968e-07, 4.3684e-07, 2.9054e-12, 2.2178e-07, 2.6081e-10, 5.9232e-06,\n",
       "        1.5525e-10, 2.2173e-05])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Small Dataset (over-fitting)\n",
    "\n",
    "we haven't trained the NN yet so the probabilities are far from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4829)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to minimize this loss\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp() # (32, 27)\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of using the above code we can also use pytorch library to arrive at the same loss, which is more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's setup the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  3.985849380493164\n",
      "loss ->  3.6028308868408203\n",
      "loss ->  3.2621421813964844\n",
      "loss ->  2.961381435394287\n",
      "loss ->  2.6982975006103516\n",
      "loss ->  2.469712734222412\n",
      "loss ->  2.271660566329956\n",
      "loss ->  2.101283550262451\n",
      "loss ->  1.9571772813796997\n",
      "loss ->  1.8374857902526855\n",
      "loss ->  1.7380967140197754\n",
      "loss ->  1.653511643409729\n",
      "loss ->  1.5790899991989136\n",
      "loss ->  1.5117664337158203\n",
      "loss ->  1.449604868888855\n",
      "loss ->  1.3913118839263916\n",
      "loss ->  1.3359922170639038\n",
      "loss ->  1.2830528020858765\n",
      "loss ->  1.2321910858154297\n",
      "loss ->  1.18338143825531\n",
      "loss ->  1.136798620223999\n",
      "loss ->  1.092664122581482\n",
      "loss ->  1.051092267036438\n",
      "loss ->  1.0120269060134888\n",
      "loss ->  0.9752703309059143\n",
      "loss ->  0.940556526184082\n",
      "loss ->  0.9076125621795654\n",
      "loss ->  0.8761920928955078\n",
      "loss ->  0.8460890650749207\n",
      "loss ->  0.8171356916427612\n",
      "loss ->  0.7891989946365356\n",
      "loss ->  0.7621745467185974\n",
      "loss ->  0.7359813451766968\n",
      "loss ->  0.7105579972267151\n",
      "loss ->  0.6858609914779663\n",
      "loss ->  0.6618651747703552\n",
      "loss ->  0.6385655403137207\n",
      "loss ->  0.6159816980361938\n",
      "loss ->  0.5941658616065979\n",
      "loss ->  0.5732104182243347\n",
      "loss ->  0.5532563924789429\n",
      "loss ->  0.5344882011413574\n",
      "loss ->  0.5171165466308594\n",
      "loss ->  0.5013313293457031\n",
      "loss ->  0.48724260926246643\n",
      "loss ->  0.4748404622077942\n",
      "loss ->  0.4639977812767029\n",
      "loss ->  0.45451444387435913\n",
      "loss ->  0.44617098569869995\n",
      "loss ->  0.4387663006782532\n",
      "loss ->  0.4321332573890686\n",
      "loss ->  0.42613884806632996\n",
      "loss ->  0.42067989706993103\n",
      "loss ->  0.4156752824783325\n",
      "loss ->  0.4110615849494934\n",
      "loss ->  0.4067871868610382\n",
      "loss ->  0.402810662984848\n",
      "loss ->  0.3990972936153412\n",
      "loss ->  0.3956180214881897\n",
      "loss ->  0.39234787225723267\n",
      "loss ->  0.3892652988433838\n",
      "loss ->  0.38635194301605225\n",
      "loss ->  0.38359174132347107\n",
      "loss ->  0.38096994161605835\n",
      "loss ->  0.37847423553466797\n",
      "loss ->  0.37609291076660156\n",
      "loss ->  0.3738164007663727\n",
      "loss ->  0.3716350197792053\n",
      "loss ->  0.3695409893989563\n",
      "loss ->  0.3675268888473511\n",
      "loss ->  0.3655855357646942\n",
      "loss ->  0.3637113869190216\n",
      "loss ->  0.3618983030319214\n",
      "loss ->  0.36014166474342346\n",
      "loss ->  0.3584362864494324\n",
      "loss ->  0.3567781150341034\n",
      "loss ->  0.3551627993583679\n",
      "loss ->  0.35358694195747375\n",
      "loss ->  0.35204702615737915\n",
      "loss ->  0.3505397439002991\n",
      "loss ->  0.3490622937679291\n",
      "loss ->  0.3476122319698334\n",
      "loss ->  0.34618666768074036\n",
      "loss ->  0.34478360414505005\n",
      "loss ->  0.3434009850025177\n",
      "loss ->  0.3420368432998657\n",
      "loss ->  0.34068992733955383\n",
      "loss ->  0.33935868740081787\n",
      "loss ->  0.33804193139076233\n",
      "loss ->  0.3367388844490051\n",
      "loss ->  0.33544883131980896\n",
      "loss ->  0.33417123556137085\n",
      "loss ->  0.33290591835975647\n",
      "loss ->  0.33165305852890015\n",
      "loss ->  0.33041250705718994\n",
      "loss ->  0.32918494939804077\n",
      "loss ->  0.3279707729816437\n",
      "loss ->  0.3267705738544464\n",
      "loss ->  0.32558542490005493\n",
      "loss ->  0.32441604137420654\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we reached above optimized loss with 5 words and 32 examples. so let's pull in all the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training On Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many chars do we take to predict the next char, 4-th one\n",
    "X, Y = [], []\n",
    "for w in words: # the examples we can generate from the first 5 words\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': # we are padding with dots, because if the word doesn't have enough chars to cover for our block_size, we'll have something to build\n",
    "        ix = s2i[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  10.709587097167969\n",
      "loss ->  10.407631874084473\n",
      "loss ->  10.127808570861816\n",
      "loss ->  9.864364624023438\n",
      "loss ->  9.614503860473633\n",
      "loss ->  9.376439094543457\n",
      "loss ->  9.148944854736328\n",
      "loss ->  8.931110382080078\n",
      "loss ->  8.722230911254883\n",
      "loss ->  8.521748542785645\n",
      "loss ->  8.329227447509766\n",
      "loss ->  8.144325256347656\n",
      "loss ->  7.966791152954102\n",
      "loss ->  7.796450614929199\n",
      "loss ->  7.633185386657715\n",
      "loss ->  7.476908206939697\n",
      "loss ->  7.327521800994873\n",
      "loss ->  7.184885501861572\n",
      "loss ->  7.048791885375977\n",
      "loss ->  6.918952465057373\n",
      "loss ->  6.795018196105957\n",
      "loss ->  6.676602840423584\n",
      "loss ->  6.563319206237793\n",
      "loss ->  6.454790115356445\n",
      "loss ->  6.350668907165527\n",
      "loss ->  6.250643253326416\n",
      "loss ->  6.15443229675293\n",
      "loss ->  6.06178617477417\n",
      "loss ->  5.972482681274414\n",
      "loss ->  5.886328220367432\n",
      "loss ->  5.803147792816162\n",
      "loss ->  5.722784519195557\n",
      "loss ->  5.645095348358154\n",
      "loss ->  5.5699462890625\n",
      "loss ->  5.497213840484619\n",
      "loss ->  5.426781177520752\n",
      "loss ->  5.358536243438721\n",
      "loss ->  5.292376518249512\n",
      "loss ->  5.228204727172852\n",
      "loss ->  5.165928840637207\n",
      "loss ->  5.105469226837158\n",
      "loss ->  5.046748638153076\n",
      "loss ->  4.989699363708496\n",
      "loss ->  4.934262275695801\n",
      "loss ->  4.880381107330322\n",
      "loss ->  4.828006744384766\n",
      "loss ->  4.777096271514893\n",
      "loss ->  4.727611064910889\n",
      "loss ->  4.6795148849487305\n",
      "loss ->  4.6327805519104\n",
      "loss ->  4.587380409240723\n",
      "loss ->  4.543290138244629\n",
      "loss ->  4.500492095947266\n",
      "loss ->  4.458968162536621\n",
      "loss ->  4.418701648712158\n",
      "loss ->  4.379676818847656\n",
      "loss ->  4.341879367828369\n",
      "loss ->  4.305293083190918\n",
      "loss ->  4.269900798797607\n",
      "loss ->  4.235681533813477\n",
      "loss ->  4.202613830566406\n",
      "loss ->  4.170671463012695\n",
      "loss ->  4.139826774597168\n",
      "loss ->  4.110045433044434\n",
      "loss ->  4.08129358291626\n",
      "loss ->  4.053532600402832\n",
      "loss ->  4.026721954345703\n",
      "loss ->  4.000818729400635\n",
      "loss ->  3.975780963897705\n",
      "loss ->  3.9515655040740967\n",
      "loss ->  3.928129196166992\n",
      "loss ->  3.9054319858551025\n",
      "loss ->  3.883434534072876\n",
      "loss ->  3.8620998859405518\n",
      "loss ->  3.841392755508423\n",
      "loss ->  3.8212811946868896\n",
      "loss ->  3.8017354011535645\n",
      "loss ->  3.7827279567718506\n",
      "loss ->  3.7642343044281006\n",
      "loss ->  3.7462308406829834\n",
      "loss ->  3.728696823120117\n",
      "loss ->  3.7116124629974365\n",
      "loss ->  3.694960117340088\n",
      "loss ->  3.6787238121032715\n",
      "loss ->  3.662888288497925\n",
      "loss ->  3.6474387645721436\n",
      "loss ->  3.6323623657226562\n",
      "loss ->  3.6176464557647705\n",
      "loss ->  3.6032798290252686\n",
      "loss ->  3.589250326156616\n",
      "loss ->  3.5755486488342285\n",
      "loss ->  3.5621633529663086\n",
      "loss ->  3.549085855484009\n",
      "loss ->  3.5363059043884277\n",
      "loss ->  3.523815155029297\n",
      "loss ->  3.5116047859191895\n",
      "loss ->  3.4996654987335205\n",
      "loss ->  3.487990617752075\n",
      "loss ->  3.4765710830688477\n",
      "loss ->  3.465399980545044\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X] \n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch\n",
    "\n",
    "The reason why above training takes a lot of time is that, it's doing a forward and backward pass on a large dataset. To optimize the training process we introduce mini-batches. \n",
    "\n",
    "With mini-batch, we do the forward and backward passes on a smaller dataset. Once optimized we move to another batch for training and optimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.343358039855957\n",
      "loss ->  2.5761938095092773\n",
      "loss ->  2.2467153072357178\n",
      "loss ->  2.3134658336639404\n",
      "loss ->  1.9506323337554932\n",
      "loss ->  2.6099228858947754\n",
      "loss ->  2.0967602729797363\n",
      "loss ->  2.2388033866882324\n",
      "loss ->  2.25968599319458\n",
      "loss ->  2.265740156173706\n",
      "loss ->  2.548616886138916\n",
      "loss ->  1.7506353855133057\n",
      "loss ->  2.8751585483551025\n",
      "loss ->  2.7745673656463623\n",
      "loss ->  1.8377327919006348\n",
      "loss ->  2.4854981899261475\n",
      "loss ->  2.0249948501586914\n",
      "loss ->  2.330305337905884\n",
      "loss ->  2.407785654067993\n",
      "loss ->  2.726006269454956\n",
      "loss ->  2.4352259635925293\n",
      "loss ->  2.1189327239990234\n",
      "loss ->  2.480989933013916\n",
      "loss ->  2.2646238803863525\n",
      "loss ->  2.240161418914795\n",
      "loss ->  2.383603811264038\n",
      "loss ->  2.3747398853302\n",
      "loss ->  2.2474868297576904\n",
      "loss ->  2.265019178390503\n",
      "loss ->  2.085399866104126\n",
      "loss ->  2.5529844760894775\n",
      "loss ->  2.0036675930023193\n",
      "loss ->  2.5144033432006836\n",
      "loss ->  2.1372337341308594\n",
      "loss ->  2.318549871444702\n",
      "loss ->  1.9543728828430176\n",
      "loss ->  2.2837953567504883\n",
      "loss ->  2.408473253250122\n",
      "loss ->  2.253357410430908\n",
      "loss ->  2.188284397125244\n",
      "loss ->  2.7726945877075195\n",
      "loss ->  2.064352512359619\n",
      "loss ->  1.9501967430114746\n",
      "loss ->  2.067500591278076\n",
      "loss ->  2.3557944297790527\n",
      "loss ->  2.1351230144500732\n",
      "loss ->  1.8499584197998047\n",
      "loss ->  2.0470945835113525\n",
      "loss ->  2.318967580795288\n",
      "loss ->  2.1073875427246094\n",
      "loss ->  1.9981801509857178\n",
      "loss ->  2.246654510498047\n",
      "loss ->  2.37377667427063\n",
      "loss ->  2.4173169136047363\n",
      "loss ->  2.216341972351074\n",
      "loss ->  2.657839298248291\n",
      "loss ->  2.010936737060547\n",
      "loss ->  1.8638101816177368\n",
      "loss ->  2.0654895305633545\n",
      "loss ->  1.889736533164978\n",
      "loss ->  2.6878201961517334\n",
      "loss ->  2.44472599029541\n",
      "loss ->  2.205423593521118\n",
      "loss ->  1.7654043436050415\n",
      "loss ->  2.0248520374298096\n",
      "loss ->  2.1451590061187744\n",
      "loss ->  2.108337879180908\n",
      "loss ->  2.626938581466675\n",
      "loss ->  2.245450973510742\n",
      "loss ->  2.407459259033203\n",
      "loss ->  2.257350444793701\n",
      "loss ->  2.1574370861053467\n",
      "loss ->  2.4329946041107178\n",
      "loss ->  2.1594862937927246\n",
      "loss ->  2.290759325027466\n",
      "loss ->  2.542874336242676\n",
      "loss ->  2.2015438079833984\n",
      "loss ->  2.214064359664917\n",
      "loss ->  2.489417314529419\n",
      "loss ->  1.833296775817871\n",
      "loss ->  2.576571464538574\n",
      "loss ->  2.103208541870117\n",
      "loss ->  2.0805580615997314\n",
      "loss ->  2.2021260261535645\n",
      "loss ->  2.1120262145996094\n",
      "loss ->  2.4637210369110107\n",
      "loss ->  2.6712918281555176\n",
      "loss ->  2.358374834060669\n",
      "loss ->  2.09616756439209\n",
      "loss ->  2.097447156906128\n",
      "loss ->  2.174809694290161\n",
      "loss ->  2.407951593399048\n",
      "loss ->  2.5889720916748047\n",
      "loss ->  2.422135829925537\n",
      "loss ->  2.15649151802063\n",
      "loss ->  2.5489094257354736\n",
      "loss ->  2.3745245933532715\n",
      "loss ->  2.108576774597168\n",
      "loss ->  2.732464075088501\n",
      "loss ->  2.2874207496643066\n",
      "loss ->  2.477053642272949\n",
      "loss ->  2.4242498874664307\n",
      "loss ->  1.9178242683410645\n",
      "loss ->  2.067081928253174\n",
      "loss ->  1.9536004066467285\n",
      "loss ->  2.127265453338623\n",
      "loss ->  2.320216417312622\n",
      "loss ->  2.2083683013916016\n",
      "loss ->  2.0961804389953613\n",
      "loss ->  2.378934621810913\n",
      "loss ->  2.0615880489349365\n",
      "loss ->  2.174435615539551\n",
      "loss ->  2.0821194648742676\n",
      "loss ->  2.250364065170288\n",
      "loss ->  2.43538236618042\n",
      "loss ->  2.1680362224578857\n",
      "loss ->  2.362178325653076\n",
      "loss ->  2.403843402862549\n",
      "loss ->  2.1590538024902344\n",
      "loss ->  2.4258460998535156\n",
      "loss ->  2.1776247024536133\n",
      "loss ->  2.161721706390381\n",
      "loss ->  2.607544183731079\n",
      "loss ->  2.9225873947143555\n",
      "loss ->  2.2455875873565674\n",
      "loss ->  1.90150785446167\n",
      "loss ->  2.0893349647521973\n",
      "loss ->  2.421881675720215\n",
      "loss ->  2.4391884803771973\n",
      "loss ->  2.2489688396453857\n",
      "loss ->  2.2730743885040283\n",
      "loss ->  2.536628484725952\n",
      "loss ->  2.75490140914917\n",
      "loss ->  2.6329846382141113\n",
      "loss ->  2.457767963409424\n",
      "loss ->  2.2743139266967773\n",
      "loss ->  2.009788990020752\n",
      "loss ->  2.598111152648926\n",
      "loss ->  2.3641703128814697\n",
      "loss ->  2.1274054050445557\n",
      "loss ->  2.3688671588897705\n",
      "loss ->  2.503354549407959\n",
      "loss ->  2.2033824920654297\n",
      "loss ->  1.6531345844268799\n",
      "loss ->  2.151193141937256\n",
      "loss ->  2.493825912475586\n",
      "loss ->  2.1840896606445312\n",
      "loss ->  2.421039342880249\n",
      "loss ->  2.4583518505096436\n",
      "loss ->  2.324972152709961\n",
      "loss ->  1.9612215757369995\n",
      "loss ->  2.253854274749756\n",
      "loss ->  2.2936177253723145\n",
      "loss ->  2.2305827140808105\n",
      "loss ->  2.5608315467834473\n",
      "loss ->  2.310703992843628\n",
      "loss ->  2.476874828338623\n",
      "loss ->  2.013498306274414\n",
      "loss ->  2.254101514816284\n",
      "loss ->  2.038731336593628\n",
      "loss ->  2.150493621826172\n",
      "loss ->  2.0223422050476074\n",
      "loss ->  2.356818437576294\n",
      "loss ->  2.4930219650268555\n",
      "loss ->  2.066920042037964\n",
      "loss ->  2.633148193359375\n",
      "loss ->  2.2007057666778564\n",
      "loss ->  2.482239246368408\n",
      "loss ->  2.3519294261932373\n",
      "loss ->  2.6560544967651367\n",
      "loss ->  2.4889376163482666\n",
      "loss ->  2.070700168609619\n",
      "loss ->  2.138404369354248\n",
      "loss ->  2.1483194828033447\n",
      "loss ->  2.308387279510498\n",
      "loss ->  2.3651375770568848\n",
      "loss ->  2.2469754219055176\n",
      "loss ->  2.0886569023132324\n",
      "loss ->  1.8656612634658813\n",
      "loss ->  2.3596253395080566\n",
      "loss ->  2.4691967964172363\n",
      "loss ->  2.3049416542053223\n",
      "loss ->  2.12550687789917\n",
      "loss ->  2.4218382835388184\n",
      "loss ->  2.2981832027435303\n",
      "loss ->  2.5511932373046875\n",
      "loss ->  2.5125441551208496\n",
      "loss ->  2.274562120437622\n",
      "loss ->  2.5225889682769775\n",
      "loss ->  2.2544937133789062\n",
      "loss ->  2.128692865371704\n",
      "loss ->  2.674926280975342\n",
      "loss ->  2.393096685409546\n",
      "loss ->  2.2960476875305176\n",
      "loss ->  2.033576011657715\n",
      "loss ->  2.0880799293518066\n",
      "loss ->  2.371431589126587\n",
      "loss ->  2.3354756832122803\n",
      "loss ->  2.437124013900757\n",
      "loss ->  2.4357683658599854\n",
      "loss ->  2.381152868270874\n",
      "loss ->  2.2720911502838135\n",
      "loss ->  2.7472660541534424\n",
      "loss ->  2.5345144271850586\n",
      "loss ->  2.3728859424591064\n",
      "loss ->  1.9753222465515137\n",
      "loss ->  2.3865132331848145\n",
      "loss ->  2.1812429428100586\n",
      "loss ->  2.6341938972473145\n",
      "loss ->  1.832127332687378\n",
      "loss ->  2.0135810375213623\n",
      "loss ->  2.4703245162963867\n",
      "loss ->  2.170994520187378\n",
      "loss ->  2.386183500289917\n",
      "loss ->  2.3263909816741943\n",
      "loss ->  1.9876317977905273\n",
      "loss ->  2.556980848312378\n",
      "loss ->  2.304990291595459\n",
      "loss ->  1.9099458456039429\n",
      "loss ->  2.4159092903137207\n",
      "loss ->  1.9932069778442383\n",
      "loss ->  2.045377254486084\n",
      "loss ->  2.5014007091522217\n",
      "loss ->  2.219069004058838\n",
      "loss ->  2.7661890983581543\n",
      "loss ->  2.311392307281494\n",
      "loss ->  1.9629335403442383\n",
      "loss ->  2.19954776763916\n",
      "loss ->  2.7830235958099365\n",
      "loss ->  2.4033985137939453\n",
      "loss ->  2.1433098316192627\n",
      "loss ->  1.830693244934082\n",
      "loss ->  2.3074452877044678\n",
      "loss ->  2.1637659072875977\n",
      "loss ->  2.211592435836792\n",
      "loss ->  2.264641761779785\n",
      "loss ->  2.3304078578948975\n",
      "loss ->  1.9627925157546997\n",
      "loss ->  2.0278632640838623\n",
      "loss ->  2.2896413803100586\n",
      "loss ->  1.9712228775024414\n",
      "loss ->  2.393674850463867\n",
      "loss ->  2.2582640647888184\n",
      "loss ->  1.8903050422668457\n",
      "loss ->  2.217829942703247\n",
      "loss ->  2.5806353092193604\n",
      "loss ->  2.0114200115203857\n",
      "loss ->  2.223677635192871\n",
      "loss ->  2.3764488697052\n",
      "loss ->  2.417811632156372\n",
      "loss ->  2.2099971771240234\n",
      "loss ->  2.145968198776245\n",
      "loss ->  2.3156516551971436\n",
      "loss ->  2.1628851890563965\n",
      "loss ->  2.5108847618103027\n",
      "loss ->  2.7430644035339355\n",
      "loss ->  2.025067090988159\n",
      "loss ->  2.1684296131134033\n",
      "loss ->  2.4481542110443115\n",
      "loss ->  2.346421957015991\n",
      "loss ->  2.103935956954956\n",
      "loss ->  2.109171152114868\n",
      "loss ->  2.2738823890686035\n",
      "loss ->  2.4597866535186768\n",
      "loss ->  1.786718726158142\n",
      "loss ->  2.2193729877471924\n",
      "loss ->  2.6812872886657715\n",
      "loss ->  2.0124495029449463\n",
      "loss ->  2.301891565322876\n",
      "loss ->  1.8008129596710205\n",
      "loss ->  2.1841211318969727\n",
      "loss ->  2.8824191093444824\n",
      "loss ->  1.9642690420150757\n",
      "loss ->  2.5201568603515625\n",
      "loss ->  2.0467095375061035\n",
      "loss ->  1.982741355895996\n",
      "loss ->  2.278606414794922\n",
      "loss ->  2.0817127227783203\n",
      "loss ->  2.4179043769836426\n",
      "loss ->  2.07487416267395\n",
      "loss ->  2.47684907913208\n",
      "loss ->  2.2796123027801514\n",
      "loss ->  2.3477516174316406\n",
      "loss ->  2.58328914642334\n",
      "loss ->  2.3096466064453125\n",
      "loss ->  1.8862711191177368\n",
      "loss ->  2.056058168411255\n",
      "loss ->  2.420189142227173\n",
      "loss ->  2.3049473762512207\n",
      "loss ->  2.1219522953033447\n",
      "loss ->  2.4133460521698\n",
      "loss ->  2.1005890369415283\n",
      "loss ->  2.187974691390991\n",
      "loss ->  2.3267605304718018\n",
      "loss ->  2.67492938041687\n",
      "loss ->  2.0936801433563232\n",
      "loss ->  2.178645372390747\n",
      "loss ->  2.4579527378082275\n",
      "loss ->  2.052077293395996\n",
      "loss ->  2.521968364715576\n",
      "loss ->  2.4199342727661133\n",
      "loss ->  2.02553129196167\n",
      "loss ->  2.1664488315582275\n",
      "loss ->  2.292412281036377\n",
      "loss ->  2.716824531555176\n",
      "loss ->  2.169747829437256\n",
      "loss ->  2.360445737838745\n",
      "loss ->  2.1821651458740234\n",
      "loss ->  2.3527259826660156\n",
      "loss ->  2.1029000282287598\n",
      "loss ->  2.484222888946533\n",
      "loss ->  2.3232216835021973\n",
      "loss ->  2.190067768096924\n",
      "loss ->  1.921205997467041\n",
      "loss ->  2.1980297565460205\n",
      "loss ->  2.247495651245117\n",
      "loss ->  2.237830877304077\n",
      "loss ->  2.223558187484741\n",
      "loss ->  2.0856473445892334\n",
      "loss ->  2.4019582271575928\n",
      "loss ->  2.318032741546631\n",
      "loss ->  2.0140955448150635\n",
      "loss ->  2.311720609664917\n",
      "loss ->  2.24611234664917\n",
      "loss ->  2.0297927856445312\n",
      "loss ->  2.210078001022339\n",
      "loss ->  2.3147284984588623\n",
      "loss ->  2.280799150466919\n",
      "loss ->  1.9894828796386719\n",
      "loss ->  2.0919084548950195\n",
      "loss ->  2.669405698776245\n",
      "loss ->  2.4052624702453613\n",
      "loss ->  2.501556873321533\n",
      "loss ->  2.0921237468719482\n",
      "loss ->  2.1050662994384766\n",
      "loss ->  2.0339624881744385\n",
      "loss ->  2.306086778640747\n",
      "loss ->  2.3852689266204834\n",
      "loss ->  2.1168007850646973\n",
      "loss ->  2.050494909286499\n",
      "loss ->  1.893727421760559\n",
      "loss ->  1.7355570793151855\n",
      "loss ->  2.26933217048645\n",
      "loss ->  2.2974348068237305\n",
      "loss ->  2.1823439598083496\n",
      "loss ->  2.18002986907959\n",
      "loss ->  2.0563931465148926\n",
      "loss ->  1.837651252746582\n",
      "loss ->  2.520845890045166\n",
      "loss ->  2.01224422454834\n",
      "loss ->  1.858860731124878\n",
      "loss ->  2.212320327758789\n",
      "loss ->  2.402212381362915\n",
      "loss ->  2.2809200286865234\n",
      "loss ->  1.9803441762924194\n",
      "loss ->  2.2565248012542725\n",
      "loss ->  1.6810508966445923\n",
      "loss ->  2.4192862510681152\n",
      "loss ->  2.2743871212005615\n",
      "loss ->  1.9036951065063477\n",
      "loss ->  2.815326690673828\n",
      "loss ->  2.600156307220459\n",
      "loss ->  2.5076427459716797\n",
      "loss ->  2.3513031005859375\n",
      "loss ->  2.273348093032837\n",
      "loss ->  2.239957571029663\n",
      "loss ->  2.3333492279052734\n",
      "loss ->  2.4001431465148926\n",
      "loss ->  1.9568997621536255\n",
      "loss ->  2.3881123065948486\n",
      "loss ->  2.050393581390381\n",
      "loss ->  2.007385730743408\n",
      "loss ->  2.454739570617676\n",
      "loss ->  2.421787738800049\n",
      "loss ->  1.7424745559692383\n",
      "loss ->  2.4887607097625732\n",
      "loss ->  2.258441686630249\n",
      "loss ->  2.4510700702667236\n",
      "loss ->  2.1443986892700195\n",
      "loss ->  2.027362585067749\n",
      "loss ->  2.397801160812378\n",
      "loss ->  2.123474597930908\n",
      "loss ->  2.16475772857666\n",
      "loss ->  2.7544972896575928\n",
      "loss ->  2.064441204071045\n",
      "loss ->  2.7463247776031494\n",
      "loss ->  2.254718065261841\n",
      "loss ->  2.0882599353790283\n",
      "loss ->  1.9852708578109741\n",
      "loss ->  2.563920736312866\n",
      "loss ->  1.8968679904937744\n",
      "loss ->  2.622037172317505\n",
      "loss ->  2.143179178237915\n",
      "loss ->  2.318066120147705\n",
      "loss ->  2.232956886291504\n",
      "loss ->  2.3556220531463623\n",
      "loss ->  2.404536008834839\n",
      "loss ->  2.247065544128418\n",
      "loss ->  2.013068437576294\n",
      "loss ->  2.3228561878204346\n",
      "loss ->  2.179378032684326\n",
      "loss ->  2.0939297676086426\n",
      "loss ->  2.2252275943756104\n",
      "loss ->  2.366861581802368\n",
      "loss ->  2.0347695350646973\n",
      "loss ->  2.44159197807312\n",
      "loss ->  2.122250556945801\n",
      "loss ->  2.486401319503784\n",
      "loss ->  2.2268080711364746\n",
      "loss ->  2.370014190673828\n",
      "loss ->  2.084172487258911\n",
      "loss ->  2.3896067142486572\n",
      "loss ->  2.3272368907928467\n",
      "loss ->  2.062159538269043\n",
      "loss ->  2.237030267715454\n",
      "loss ->  2.0693795680999756\n",
      "loss ->  2.393751621246338\n",
      "loss ->  2.382199764251709\n",
      "loss ->  2.386221170425415\n",
      "loss ->  2.6666111946105957\n",
      "loss ->  2.0323424339294434\n",
      "loss ->  2.5098047256469727\n",
      "loss ->  2.4712045192718506\n",
      "loss ->  2.275644540786743\n",
      "loss ->  2.1060004234313965\n",
      "loss ->  2.1900296211242676\n",
      "loss ->  1.9156112670898438\n",
      "loss ->  1.9631246328353882\n",
      "loss ->  2.4653480052948\n",
      "loss ->  2.02785062789917\n",
      "loss ->  2.3700482845306396\n",
      "loss ->  2.3545637130737305\n",
      "loss ->  2.4897797107696533\n",
      "loss ->  2.026851177215576\n",
      "loss ->  2.2389373779296875\n",
      "loss ->  2.6656131744384766\n",
      "loss ->  2.330152750015259\n",
      "loss ->  2.2463321685791016\n",
      "loss ->  2.421483278274536\n",
      "loss ->  2.3265268802642822\n",
      "loss ->  2.111504554748535\n",
      "loss ->  1.9568119049072266\n",
      "loss ->  2.670301914215088\n",
      "loss ->  2.1689538955688477\n",
      "loss ->  2.136714458465576\n",
      "loss ->  2.540635108947754\n",
      "loss ->  1.879705786705017\n",
      "loss ->  1.9970060586929321\n",
      "loss ->  2.434561252593994\n",
      "loss ->  2.3163976669311523\n",
      "loss ->  1.9229646921157837\n",
      "loss ->  2.437311887741089\n",
      "loss ->  2.4048900604248047\n",
      "loss ->  2.5067403316497803\n",
      "loss ->  2.35945200920105\n",
      "loss ->  2.24971079826355\n",
      "loss ->  2.274214744567871\n",
      "loss ->  1.9686576128005981\n",
      "loss ->  2.6096208095550537\n",
      "loss ->  2.0152525901794434\n",
      "loss ->  2.420117139816284\n",
      "loss ->  2.341247797012329\n",
      "loss ->  2.81559419631958\n",
      "loss ->  2.113342761993408\n",
      "loss ->  2.256051778793335\n",
      "loss ->  1.9697190523147583\n",
      "loss ->  2.1926498413085938\n",
      "loss ->  1.928857684135437\n",
      "loss ->  2.322993516921997\n",
      "loss ->  2.0063328742980957\n",
      "loss ->  2.5342671871185303\n",
      "loss ->  2.6103439331054688\n",
      "loss ->  2.177572250366211\n",
      "loss ->  1.9914884567260742\n",
      "loss ->  2.2957770824432373\n",
      "loss ->  2.4114317893981934\n",
      "loss ->  2.1716952323913574\n",
      "loss ->  2.2442212104797363\n",
      "loss ->  2.6485321521759033\n",
      "loss ->  2.385497808456421\n",
      "loss ->  2.605822801589966\n",
      "loss ->  2.3202083110809326\n",
      "loss ->  2.3338654041290283\n",
      "loss ->  2.539698362350464\n",
      "loss ->  2.505464553833008\n",
      "loss ->  1.788786768913269\n",
      "loss ->  2.535850763320923\n",
      "loss ->  2.2046632766723633\n",
      "loss ->  2.180894613265991\n",
      "loss ->  2.445519208908081\n",
      "loss ->  2.167968273162842\n",
      "loss ->  2.585332155227661\n",
      "loss ->  2.2787344455718994\n",
      "loss ->  2.2139713764190674\n",
      "loss ->  2.1420483589172363\n",
      "loss ->  2.342740058898926\n",
      "loss ->  2.4181275367736816\n",
      "loss ->  2.10455584526062\n",
      "loss ->  1.8599364757537842\n",
      "loss ->  1.7911769151687622\n",
      "loss ->  2.6302525997161865\n",
      "loss ->  2.4885106086730957\n",
      "loss ->  1.8257609605789185\n",
      "loss ->  2.532867193222046\n",
      "loss ->  1.9319915771484375\n",
      "loss ->  2.3325135707855225\n",
      "loss ->  2.4576022624969482\n",
      "loss ->  2.3315627574920654\n",
      "loss ->  2.1868107318878174\n",
      "loss ->  2.3465399742126465\n",
      "loss ->  2.232900619506836\n",
      "loss ->  2.24879789352417\n",
      "loss ->  2.112447738647461\n",
      "loss ->  2.066208600997925\n",
      "loss ->  2.352360248565674\n",
      "loss ->  2.5443203449249268\n",
      "loss ->  2.182337760925293\n",
      "loss ->  2.20019793510437\n",
      "loss ->  2.4645748138427734\n",
      "loss ->  2.481203556060791\n",
      "loss ->  2.7576959133148193\n",
      "loss ->  2.094200611114502\n",
      "loss ->  2.395897626876831\n",
      "loss ->  1.767885684967041\n",
      "loss ->  2.4857754707336426\n",
      "loss ->  2.152003288269043\n",
      "loss ->  2.3106062412261963\n",
      "loss ->  2.116457939147949\n",
      "loss ->  2.182846784591675\n",
      "loss ->  2.283587694168091\n",
      "loss ->  2.6314330101013184\n",
      "loss ->  2.5590901374816895\n",
      "loss ->  1.974458932876587\n",
      "loss ->  2.2128190994262695\n",
      "loss ->  2.2657151222229004\n",
      "loss ->  2.1888418197631836\n",
      "loss ->  2.2329230308532715\n",
      "loss ->  2.615015745162964\n",
      "loss ->  2.059593915939331\n",
      "loss ->  2.7677693367004395\n",
      "loss ->  2.292304277420044\n",
      "loss ->  2.4059054851531982\n",
      "loss ->  2.0548808574676514\n",
      "loss ->  2.3456904888153076\n",
      "loss ->  2.0820860862731934\n",
      "loss ->  1.9403860569000244\n",
      "loss ->  2.1996076107025146\n",
      "loss ->  1.9409074783325195\n",
      "loss ->  2.6355412006378174\n",
      "loss ->  2.107988119125366\n",
      "loss ->  2.1129791736602783\n",
      "loss ->  2.022289514541626\n",
      "loss ->  2.1959445476531982\n",
      "loss ->  2.2484800815582275\n",
      "loss ->  1.9545801877975464\n",
      "loss ->  2.210439920425415\n",
      "loss ->  1.7644951343536377\n",
      "loss ->  2.4996986389160156\n",
      "loss ->  2.1522679328918457\n",
      "loss ->  1.8117746114730835\n",
      "loss ->  2.172793388366699\n",
      "loss ->  2.633253574371338\n",
      "loss ->  2.611541986465454\n",
      "loss ->  2.166868209838867\n",
      "loss ->  2.1803054809570312\n",
      "loss ->  2.0440921783447266\n",
      "loss ->  2.2699081897735596\n",
      "loss ->  2.521052360534668\n",
      "loss ->  2.328066825866699\n",
      "loss ->  2.1193487644195557\n",
      "loss ->  2.6342878341674805\n",
      "loss ->  1.6316399574279785\n",
      "loss ->  2.180222988128662\n",
      "loss ->  2.191598653793335\n",
      "loss ->  2.4341979026794434\n",
      "loss ->  2.0661351680755615\n",
      "loss ->  2.299121379852295\n",
      "loss ->  2.279327630996704\n",
      "loss ->  2.090693235397339\n",
      "loss ->  2.4482944011688232\n",
      "loss ->  2.432541608810425\n",
      "loss ->  2.5250277519226074\n",
      "loss ->  2.3581435680389404\n",
      "loss ->  2.1659820079803467\n",
      "loss ->  2.1052322387695312\n",
      "loss ->  2.322524309158325\n",
      "loss ->  2.620543956756592\n",
      "loss ->  2.677008628845215\n",
      "loss ->  2.126685619354248\n",
      "loss ->  2.32055401802063\n",
      "loss ->  2.386241912841797\n",
      "loss ->  2.165086269378662\n",
      "loss ->  2.3075661659240723\n",
      "loss ->  2.347409963607788\n",
      "loss ->  2.229356050491333\n",
      "loss ->  2.3049325942993164\n",
      "loss ->  2.1692757606506348\n",
      "loss ->  2.1597352027893066\n",
      "loss ->  2.043609142303467\n",
      "loss ->  2.1702523231506348\n",
      "loss ->  2.2094786167144775\n",
      "loss ->  1.9608393907546997\n",
      "loss ->  2.094496726989746\n",
      "loss ->  2.434596538543701\n",
      "loss ->  2.3205220699310303\n",
      "loss ->  2.526477098464966\n",
      "loss ->  2.4701874256134033\n",
      "loss ->  2.0853748321533203\n",
      "loss ->  2.035569667816162\n",
      "loss ->  2.3454017639160156\n",
      "loss ->  1.9420034885406494\n",
      "loss ->  2.361579656600952\n",
      "loss ->  2.247062921524048\n",
      "loss ->  2.0978903770446777\n",
      "loss ->  2.4708592891693115\n",
      "loss ->  2.34033203125\n",
      "loss ->  2.6228644847869873\n",
      "loss ->  2.4288976192474365\n",
      "loss ->  2.071375608444214\n",
      "loss ->  2.1956191062927246\n",
      "loss ->  2.2671639919281006\n",
      "loss ->  2.3200442790985107\n",
      "loss ->  2.0653302669525146\n",
      "loss ->  2.140897274017334\n",
      "loss ->  2.141608238220215\n",
      "loss ->  2.0906929969787598\n",
      "loss ->  2.5156617164611816\n",
      "loss ->  2.2065322399139404\n",
      "loss ->  2.0228731632232666\n",
      "loss ->  1.9982000589370728\n",
      "loss ->  2.270568370819092\n",
      "loss ->  2.0698702335357666\n",
      "loss ->  2.2241547107696533\n",
      "loss ->  2.3142356872558594\n",
      "loss ->  2.085033893585205\n",
      "loss ->  2.524371862411499\n",
      "loss ->  2.573375940322876\n",
      "loss ->  2.4132118225097656\n",
      "loss ->  2.1959099769592285\n",
      "loss ->  2.219761610031128\n",
      "loss ->  2.0965521335601807\n",
      "loss ->  2.022449016571045\n",
      "loss ->  2.5142135620117188\n",
      "loss ->  2.2741904258728027\n",
      "loss ->  2.47884202003479\n",
      "loss ->  2.745847225189209\n",
      "loss ->  2.2132205963134766\n",
      "loss ->  2.3573837280273438\n",
      "loss ->  2.068995475769043\n",
      "loss ->  2.1300272941589355\n",
      "loss ->  2.450355291366577\n",
      "loss ->  2.2123770713806152\n",
      "loss ->  2.438887119293213\n",
      "loss ->  2.095123767852783\n",
      "loss ->  2.305109977722168\n",
      "loss ->  2.2637839317321777\n",
      "loss ->  2.1674675941467285\n",
      "loss ->  2.276305913925171\n",
      "loss ->  2.5134549140930176\n",
      "loss ->  2.1880745887756348\n",
      "loss ->  1.9683407545089722\n",
      "loss ->  2.1832642555236816\n",
      "loss ->  2.2705862522125244\n",
      "loss ->  2.4546008110046387\n",
      "loss ->  2.5972506999969482\n",
      "loss ->  1.9570708274841309\n",
      "loss ->  2.1203384399414062\n",
      "loss ->  2.211341619491577\n",
      "loss ->  1.9866491556167603\n",
      "loss ->  2.3827500343322754\n",
      "loss ->  2.545340061187744\n",
      "loss ->  2.337232828140259\n",
      "loss ->  2.2945356369018555\n",
      "loss ->  2.346519947052002\n",
      "loss ->  2.413895845413208\n",
      "loss ->  2.423513412475586\n",
      "loss ->  2.024341344833374\n",
      "loss ->  2.5166208744049072\n",
      "loss ->  1.9602705240249634\n",
      "loss ->  1.9387143850326538\n",
      "loss ->  2.1415176391601562\n",
      "loss ->  2.306138038635254\n",
      "loss ->  2.0134799480438232\n",
      "loss ->  2.1546616554260254\n",
      "loss ->  2.2224903106689453\n",
      "loss ->  2.1909186840057373\n",
      "loss ->  2.246744155883789\n",
      "loss ->  2.475954532623291\n",
      "loss ->  2.6185920238494873\n",
      "loss ->  1.9374125003814697\n",
      "loss ->  2.117680549621582\n",
      "loss ->  2.027513265609741\n",
      "loss ->  2.2981512546539307\n",
      "loss ->  2.053354024887085\n",
      "loss ->  2.2871084213256836\n",
      "loss ->  2.0163004398345947\n",
      "loss ->  2.690777540206909\n",
      "loss ->  2.2928779125213623\n",
      "loss ->  2.2004148960113525\n",
      "loss ->  2.0505189895629883\n",
      "loss ->  2.1647965908050537\n",
      "loss ->  2.200509548187256\n",
      "loss ->  2.447833299636841\n",
      "loss ->  2.0513596534729004\n",
      "loss ->  2.5557637214660645\n",
      "loss ->  2.124329090118408\n",
      "loss ->  2.387410879135132\n",
      "loss ->  2.3414676189422607\n",
      "loss ->  2.0700631141662598\n",
      "loss ->  2.009676933288574\n",
      "loss ->  2.2044899463653564\n",
      "loss ->  2.021334409713745\n",
      "loss ->  1.7629234790802002\n",
      "loss ->  2.12073016166687\n",
      "loss ->  2.224644184112549\n",
      "loss ->  2.1649394035339355\n",
      "loss ->  2.4965147972106934\n",
      "loss ->  2.231304883956909\n",
      "loss ->  2.9218013286590576\n",
      "loss ->  2.392728090286255\n",
      "loss ->  2.2812929153442383\n",
      "loss ->  1.875292420387268\n",
      "loss ->  2.2389442920684814\n",
      "loss ->  2.2221574783325195\n",
      "loss ->  2.1375317573547363\n",
      "loss ->  2.3982157707214355\n",
      "loss ->  1.8502099514007568\n",
      "loss ->  2.4864659309387207\n",
      "loss ->  2.190847158432007\n",
      "loss ->  2.3213720321655273\n",
      "loss ->  2.4597556591033936\n",
      "loss ->  1.823543667793274\n",
      "loss ->  2.4562182426452637\n",
      "loss ->  2.2775371074676514\n",
      "loss ->  1.8792222738265991\n",
      "loss ->  2.3168187141418457\n",
      "loss ->  2.157989501953125\n",
      "loss ->  2.1404972076416016\n",
      "loss ->  2.0319554805755615\n",
      "loss ->  2.3165106773376465\n",
      "loss ->  1.8560360670089722\n",
      "loss ->  2.5540380477905273\n",
      "loss ->  2.602809190750122\n",
      "loss ->  2.6162633895874023\n",
      "loss ->  2.184331178665161\n",
      "loss ->  2.3661561012268066\n",
      "loss ->  2.327969789505005\n",
      "loss ->  1.9456138610839844\n",
      "loss ->  2.047422409057617\n",
      "loss ->  2.1095240116119385\n",
      "loss ->  2.443514347076416\n",
      "loss ->  2.026459217071533\n",
      "loss ->  2.03983473777771\n",
      "loss ->  2.511045217514038\n",
      "loss ->  2.1440980434417725\n",
      "loss ->  2.110368490219116\n",
      "loss ->  2.3275861740112305\n",
      "loss ->  2.3014841079711914\n",
      "loss ->  2.188298463821411\n",
      "loss ->  1.8248494863510132\n",
      "loss ->  2.5232582092285156\n",
      "loss ->  1.9463938474655151\n",
      "loss ->  2.351836919784546\n",
      "loss ->  2.3902411460876465\n",
      "loss ->  2.348330020904541\n",
      "loss ->  2.5703318119049072\n",
      "loss ->  1.647119164466858\n",
      "loss ->  1.929934024810791\n",
      "loss ->  2.556410789489746\n",
      "loss ->  2.3838071823120117\n",
      "loss ->  2.4594709873199463\n",
      "loss ->  2.1904666423797607\n",
      "loss ->  2.1347575187683105\n",
      "loss ->  2.2229368686676025\n",
      "loss ->  2.21911883354187\n",
      "loss ->  1.9248183965682983\n",
      "loss ->  2.1467130184173584\n",
      "loss ->  2.1936442852020264\n",
      "loss ->  2.4989171028137207\n",
      "loss ->  2.5402519702911377\n",
      "loss ->  2.1937801837921143\n",
      "loss ->  2.442800760269165\n",
      "loss ->  2.150968313217163\n",
      "loss ->  1.9491026401519775\n",
      "loss ->  2.208557367324829\n",
      "loss ->  2.5755739212036133\n",
      "loss ->  2.385608434677124\n",
      "loss ->  2.1361072063446045\n",
      "loss ->  2.395526170730591\n",
      "loss ->  2.458111047744751\n",
      "loss ->  2.369684934616089\n",
      "loss ->  2.354043960571289\n",
      "loss ->  2.2126805782318115\n",
      "loss ->  2.4851462841033936\n",
      "loss ->  2.211418628692627\n",
      "loss ->  2.0111491680145264\n",
      "loss ->  2.1021857261657715\n",
      "loss ->  2.222435712814331\n",
      "loss ->  2.0197930335998535\n",
      "loss ->  2.2735722064971924\n",
      "loss ->  2.4018709659576416\n",
      "loss ->  2.412536859512329\n",
      "loss ->  2.4795899391174316\n",
      "loss ->  2.225055694580078\n",
      "loss ->  2.539522886276245\n",
      "loss ->  1.943423867225647\n",
      "loss ->  2.3310952186584473\n",
      "loss ->  2.1509389877319336\n",
      "loss ->  2.199946165084839\n",
      "loss ->  2.179572343826294\n",
      "loss ->  1.9737240076065063\n",
      "loss ->  2.2310211658477783\n",
      "loss ->  2.7407941818237305\n",
      "loss ->  2.3745458126068115\n",
      "loss ->  2.672236680984497\n",
      "loss ->  1.7765295505523682\n",
      "loss ->  2.256863832473755\n",
      "loss ->  1.5120114088058472\n",
      "loss ->  2.088773250579834\n",
      "loss ->  2.18387770652771\n",
      "loss ->  2.095773220062256\n",
      "loss ->  2.4940686225891113\n",
      "loss ->  2.243558883666992\n",
      "loss ->  2.620270013809204\n",
      "loss ->  2.526362180709839\n",
      "loss ->  2.0264885425567627\n",
      "loss ->  2.2690231800079346\n",
      "loss ->  2.1686222553253174\n",
      "loss ->  1.9438878297805786\n",
      "loss ->  2.4267656803131104\n",
      "loss ->  2.268980026245117\n",
      "loss ->  2.1202352046966553\n",
      "loss ->  2.5303962230682373\n",
      "loss ->  2.051469087600708\n",
      "loss ->  2.1914286613464355\n",
      "loss ->  2.1843934059143066\n",
      "loss ->  2.5541906356811523\n",
      "loss ->  1.7236287593841553\n",
      "loss ->  2.2565908432006836\n",
      "loss ->  2.022739887237549\n",
      "loss ->  1.8677005767822266\n",
      "loss ->  1.9862271547317505\n",
      "loss ->  2.3347091674804688\n",
      "loss ->  1.9610356092453003\n",
      "loss ->  2.0222179889678955\n",
      "loss ->  2.3060848712921143\n",
      "loss ->  2.3406283855438232\n",
      "loss ->  2.6648664474487305\n",
      "loss ->  2.182239294052124\n",
      "loss ->  2.108651638031006\n",
      "loss ->  2.3409583568573\n",
      "loss ->  2.240288019180298\n",
      "loss ->  2.155435800552368\n",
      "loss ->  2.0774714946746826\n",
      "loss ->  2.392448663711548\n",
      "loss ->  2.46734356880188\n",
      "loss ->  2.3692626953125\n",
      "loss ->  2.0296552181243896\n",
      "loss ->  2.3377909660339355\n",
      "loss ->  2.4597668647766113\n",
      "loss ->  2.3319759368896484\n",
      "loss ->  2.2302045822143555\n",
      "loss ->  2.6121349334716797\n",
      "loss ->  2.1443440914154053\n",
      "loss ->  2.36681866645813\n",
      "loss ->  2.444711923599243\n",
      "loss ->  2.1948482990264893\n",
      "loss ->  2.5794498920440674\n",
      "loss ->  2.0512888431549072\n",
      "loss ->  2.133660078048706\n",
      "loss ->  2.1607625484466553\n",
      "loss ->  2.6117095947265625\n",
      "loss ->  2.0827441215515137\n",
      "loss ->  2.131575107574463\n",
      "loss ->  2.1093499660491943\n",
      "loss ->  2.2899229526519775\n",
      "loss ->  2.1879472732543945\n",
      "loss ->  2.202338695526123\n",
      "loss ->  2.154435634613037\n",
      "loss ->  2.076260805130005\n",
      "loss ->  2.2401275634765625\n",
      "loss ->  2.0260865688323975\n",
      "loss ->  2.3018240928649902\n",
      "loss ->  2.4702308177948\n",
      "loss ->  1.9537328481674194\n",
      "loss ->  2.1880767345428467\n",
      "loss ->  2.6752099990844727\n",
      "loss ->  2.4616222381591797\n",
      "loss ->  2.1798269748687744\n",
      "loss ->  2.5345864295959473\n",
      "loss ->  2.1378047466278076\n",
      "loss ->  2.052794933319092\n",
      "loss ->  2.0710527896881104\n",
      "loss ->  2.3437986373901367\n",
      "loss ->  2.195276975631714\n",
      "loss ->  2.3956410884857178\n",
      "loss ->  2.4170706272125244\n",
      "loss ->  2.2718138694763184\n",
      "loss ->  2.517932176589966\n",
      "loss ->  2.5887107849121094\n",
      "loss ->  2.1036999225616455\n",
      "loss ->  2.1250014305114746\n",
      "loss ->  2.215327024459839\n",
      "loss ->  2.3333864212036133\n",
      "loss ->  2.6708805561065674\n",
      "loss ->  2.13342547416687\n",
      "loss ->  2.4333157539367676\n",
      "loss ->  2.360999822616577\n",
      "loss ->  2.1633358001708984\n",
      "loss ->  2.303208589553833\n",
      "loss ->  2.499476909637451\n",
      "loss ->  2.2134146690368652\n",
      "loss ->  2.4605391025543213\n",
      "loss ->  2.3254756927490234\n",
      "loss ->  1.9723858833312988\n",
      "loss ->  2.044440507888794\n",
      "loss ->  2.3274388313293457\n",
      "loss ->  2.5223307609558105\n",
      "loss ->  2.1546199321746826\n",
      "loss ->  2.5223875045776367\n",
      "loss ->  2.048246145248413\n",
      "loss ->  1.8472992181777954\n",
      "loss ->  2.421041488647461\n",
      "loss ->  1.8174583911895752\n",
      "loss ->  2.2073493003845215\n",
      "loss ->  2.4869279861450195\n",
      "loss ->  2.137758731842041\n",
      "loss ->  2.0978212356567383\n",
      "loss ->  2.0618505477905273\n",
      "loss ->  2.2439961433410645\n",
      "loss ->  2.3949246406555176\n",
      "loss ->  2.0927681922912598\n",
      "loss ->  2.233952522277832\n",
      "loss ->  2.190901756286621\n",
      "loss ->  2.0729126930236816\n",
      "loss ->  2.5184035301208496\n",
      "loss ->  2.3040716648101807\n",
      "loss ->  2.1485726833343506\n",
      "loss ->  2.2611401081085205\n",
      "loss ->  2.2487339973449707\n",
      "loss ->  2.5301311016082764\n",
      "loss ->  2.240570068359375\n",
      "loss ->  2.5547595024108887\n",
      "loss ->  2.7836203575134277\n",
      "loss ->  2.315025806427002\n",
      "loss ->  2.2662763595581055\n",
      "loss ->  2.5037472248077393\n",
      "loss ->  2.0816757678985596\n",
      "loss ->  2.0506999492645264\n",
      "loss ->  2.161170721054077\n",
      "loss ->  2.3318066596984863\n",
      "loss ->  2.0870704650878906\n",
      "loss ->  2.03350830078125\n",
      "loss ->  2.0549936294555664\n",
      "loss ->  2.2115321159362793\n",
      "loss ->  2.3671317100524902\n",
      "loss ->  2.2894585132598877\n",
      "loss ->  1.6143909692764282\n",
      "loss ->  2.187385082244873\n",
      "loss ->  2.35629940032959\n",
      "loss ->  2.4450266361236572\n",
      "loss ->  1.8927983045578003\n",
      "loss ->  2.112379789352417\n",
      "loss ->  2.422433614730835\n",
      "loss ->  2.074765682220459\n",
      "loss ->  2.2429399490356445\n",
      "loss ->  2.631411552429199\n",
      "loss ->  2.348040819168091\n",
      "loss ->  2.3652725219726562\n",
      "loss ->  1.8447858095169067\n",
      "loss ->  2.210114002227783\n",
      "loss ->  2.070669412612915\n",
      "loss ->  2.005289316177368\n",
      "loss ->  2.6675026416778564\n",
      "loss ->  1.9461596012115479\n",
      "loss ->  2.229097604751587\n",
      "loss ->  2.121100425720215\n",
      "loss ->  2.041454553604126\n",
      "loss ->  2.3143301010131836\n",
      "loss ->  1.97366464138031\n",
      "loss ->  2.3371741771698\n",
      "loss ->  2.655643939971924\n",
      "loss ->  2.394928216934204\n",
      "loss ->  2.259596586227417\n",
      "loss ->  2.0528323650360107\n",
      "loss ->  2.1749696731567383\n",
      "loss ->  2.283238410949707\n",
      "loss ->  2.4098129272460938\n",
      "loss ->  2.4019463062286377\n",
      "loss ->  2.1245293617248535\n",
      "loss ->  2.24696683883667\n",
      "loss ->  2.2872002124786377\n",
      "loss ->  2.298194646835327\n",
      "loss ->  2.1948084831237793\n",
      "loss ->  2.050591230392456\n",
      "loss ->  1.9488075971603394\n",
      "loss ->  2.5446391105651855\n",
      "loss ->  2.0526788234710693\n",
      "loss ->  2.4259238243103027\n",
      "loss ->  2.31276798248291\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you can see that thee training is much faster and therefore, you can afford to increase the iterations for further minimizing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "\n",
    "How do you determine the learning rate? How do you know if it's too small (moving too slowly towards an optimized loss), or it's too big (over-stepping and missing the optimized loss)?\n",
    "\n",
    "One way is to find the min and max range, first. You can provide -0.0001 or lower and find the value that demonstrates a reasonable decrease in the loss. Then find a big number with the same analogy. Based in this, we can see that, the optimized learning rate should be between -0.001 and -1.\n",
    "\n",
    "We can use pytorch's library to create a linear array of learning rates between these two numbers for, say 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # linear rate exponential\n",
    "lrs = 10**lre # learning rates: 10^-3 = 0.001 and 10^0 = 1\n",
    "lrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's reset everything and iterate through possible learning rates to find the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.1068224906921387\n",
      "loss ->  2.4579553604125977\n",
      "loss ->  2.2688448429107666\n",
      "loss ->  1.6855255365371704\n",
      "loss ->  2.457881450653076\n",
      "loss ->  2.037208318710327\n",
      "loss ->  2.0690746307373047\n",
      "loss ->  2.3032195568084717\n",
      "loss ->  2.7215311527252197\n",
      "loss ->  1.904735803604126\n",
      "loss ->  2.4351987838745117\n",
      "loss ->  2.4516847133636475\n",
      "loss ->  2.0160207748413086\n",
      "loss ->  2.134155750274658\n",
      "loss ->  2.0672080516815186\n",
      "loss ->  2.3296079635620117\n",
      "loss ->  2.107666015625\n",
      "loss ->  2.2208847999572754\n",
      "loss ->  2.4583420753479004\n",
      "loss ->  1.92301344871521\n",
      "loss ->  2.5815346240997314\n",
      "loss ->  2.258862257003784\n",
      "loss ->  2.206108808517456\n",
      "loss ->  2.1114749908447266\n",
      "loss ->  2.3371829986572266\n",
      "loss ->  2.4083943367004395\n",
      "loss ->  2.5233583450317383\n",
      "loss ->  2.1591427326202393\n",
      "loss ->  2.0235626697540283\n",
      "loss ->  2.5124928951263428\n",
      "loss ->  2.2697577476501465\n",
      "loss ->  2.4515655040740967\n",
      "loss ->  2.1295182704925537\n",
      "loss ->  2.02724289894104\n",
      "loss ->  2.2297043800354004\n",
      "loss ->  2.3201208114624023\n",
      "loss ->  2.305053949356079\n",
      "loss ->  2.25532865524292\n",
      "loss ->  2.3320798873901367\n",
      "loss ->  1.98809015750885\n",
      "loss ->  1.679693341255188\n",
      "loss ->  2.550058126449585\n",
      "loss ->  2.323410749435425\n",
      "loss ->  1.9688670635223389\n",
      "loss ->  2.132004499435425\n",
      "loss ->  1.7279733419418335\n",
      "loss ->  2.5108437538146973\n",
      "loss ->  2.1413755416870117\n",
      "loss ->  2.149564266204834\n",
      "loss ->  2.3270959854125977\n",
      "loss ->  2.4050981998443604\n",
      "loss ->  2.684177875518799\n",
      "loss ->  2.0367653369903564\n",
      "loss ->  2.3162918090820312\n",
      "loss ->  1.9145715236663818\n",
      "loss ->  2.081918239593506\n",
      "loss ->  2.531005620956421\n",
      "loss ->  2.0812594890594482\n",
      "loss ->  2.223797559738159\n",
      "loss ->  2.1508069038391113\n",
      "loss ->  2.1638033390045166\n",
      "loss ->  2.3947582244873047\n",
      "loss ->  2.4961228370666504\n",
      "loss ->  2.450350522994995\n",
      "loss ->  2.0445594787597656\n",
      "loss ->  1.9459861516952515\n",
      "loss ->  2.4214093685150146\n",
      "loss ->  2.5893189907073975\n",
      "loss ->  2.2577290534973145\n",
      "loss ->  2.2716245651245117\n",
      "loss ->  2.4245176315307617\n",
      "loss ->  1.9861674308776855\n",
      "loss ->  2.0060203075408936\n",
      "loss ->  1.9537532329559326\n",
      "loss ->  2.4591472148895264\n",
      "loss ->  2.2941946983337402\n",
      "loss ->  2.298308849334717\n",
      "loss ->  2.524923086166382\n",
      "loss ->  2.3560612201690674\n",
      "loss ->  1.8584009408950806\n",
      "loss ->  2.3884084224700928\n",
      "loss ->  1.9999890327453613\n",
      "loss ->  2.393688201904297\n",
      "loss ->  2.125598907470703\n",
      "loss ->  2.1923344135284424\n",
      "loss ->  2.060067653656006\n",
      "loss ->  2.274845600128174\n",
      "loss ->  2.024566173553467\n",
      "loss ->  2.325674295425415\n",
      "loss ->  2.1578073501586914\n",
      "loss ->  2.4081056118011475\n",
      "loss ->  2.507935047149658\n",
      "loss ->  2.341031312942505\n",
      "loss ->  2.373394012451172\n",
      "loss ->  2.432105541229248\n",
      "loss ->  2.120992422103882\n",
      "loss ->  2.3874073028564453\n",
      "loss ->  2.122610092163086\n",
      "loss ->  2.1350882053375244\n",
      "loss ->  2.016254425048828\n",
      "loss ->  1.925497055053711\n",
      "loss ->  2.3466567993164062\n",
      "loss ->  1.9551327228546143\n",
      "loss ->  2.392260789871216\n",
      "loss ->  1.9899348020553589\n",
      "loss ->  2.2850282192230225\n",
      "loss ->  2.1125080585479736\n",
      "loss ->  2.5194859504699707\n",
      "loss ->  2.125960111618042\n",
      "loss ->  2.3216805458068848\n",
      "loss ->  2.407837390899658\n",
      "loss ->  2.220294237136841\n",
      "loss ->  2.437314748764038\n",
      "loss ->  1.9866273403167725\n",
      "loss ->  2.050701141357422\n",
      "loss ->  2.1196656227111816\n",
      "loss ->  2.355224847793579\n",
      "loss ->  2.15118670463562\n",
      "loss ->  2.457263469696045\n",
      "loss ->  2.327824354171753\n",
      "loss ->  2.496117115020752\n",
      "loss ->  2.471710205078125\n",
      "loss ->  2.2220561504364014\n",
      "loss ->  2.1693413257598877\n",
      "loss ->  2.446251153945923\n",
      "loss ->  1.8519617319107056\n",
      "loss ->  1.989878535270691\n",
      "loss ->  2.1766629219055176\n",
      "loss ->  1.8340206146240234\n",
      "loss ->  2.7586870193481445\n",
      "loss ->  2.2033543586730957\n",
      "loss ->  1.920119047164917\n",
      "loss ->  2.16873836517334\n",
      "loss ->  2.43648624420166\n",
      "loss ->  2.022571563720703\n",
      "loss ->  2.216494560241699\n",
      "loss ->  1.9135075807571411\n",
      "loss ->  2.0436463356018066\n",
      "loss ->  2.3585493564605713\n",
      "loss ->  2.4969215393066406\n",
      "loss ->  2.2763264179229736\n",
      "loss ->  2.042729139328003\n",
      "loss ->  2.21820330619812\n",
      "loss ->  2.524864912033081\n",
      "loss ->  2.3697664737701416\n",
      "loss ->  2.2445366382598877\n",
      "loss ->  2.1235969066619873\n",
      "loss ->  2.0292751789093018\n",
      "loss ->  2.154888153076172\n",
      "loss ->  2.384348154067993\n",
      "loss ->  2.439849853515625\n",
      "loss ->  2.5172629356384277\n",
      "loss ->  2.593456745147705\n",
      "loss ->  2.223477363586426\n",
      "loss ->  2.521599054336548\n",
      "loss ->  2.0799145698547363\n",
      "loss ->  2.7825615406036377\n",
      "loss ->  2.068286180496216\n",
      "loss ->  2.2871477603912354\n",
      "loss ->  2.360527515411377\n",
      "loss ->  2.2700302600860596\n",
      "loss ->  2.151937484741211\n",
      "loss ->  2.218186140060425\n",
      "loss ->  2.241055488586426\n",
      "loss ->  2.1482813358306885\n",
      "loss ->  2.36391544342041\n",
      "loss ->  2.049402952194214\n",
      "loss ->  2.1660735607147217\n",
      "loss ->  2.0945520401000977\n",
      "loss ->  2.632178783416748\n",
      "loss ->  2.0343143939971924\n",
      "loss ->  2.1935908794403076\n",
      "loss ->  2.342994213104248\n",
      "loss ->  2.117520332336426\n",
      "loss ->  2.193302631378174\n",
      "loss ->  2.2862765789031982\n",
      "loss ->  2.0266895294189453\n",
      "loss ->  2.3785033226013184\n",
      "loss ->  2.1215147972106934\n",
      "loss ->  1.972949504852295\n",
      "loss ->  2.303964138031006\n",
      "loss ->  2.355250835418701\n",
      "loss ->  2.388334274291992\n",
      "loss ->  2.119680643081665\n",
      "loss ->  2.3815503120422363\n",
      "loss ->  1.998003602027893\n",
      "loss ->  2.38047194480896\n",
      "loss ->  1.962835669517517\n",
      "loss ->  2.4228365421295166\n",
      "loss ->  2.194307327270508\n",
      "loss ->  2.179105758666992\n",
      "loss ->  2.2963502407073975\n",
      "loss ->  2.3942484855651855\n",
      "loss ->  2.503833293914795\n",
      "loss ->  2.1139633655548096\n",
      "loss ->  1.9922842979431152\n",
      "loss ->  2.2550864219665527\n",
      "loss ->  1.7816307544708252\n",
      "loss ->  2.330233573913574\n",
      "loss ->  2.5178003311157227\n",
      "loss ->  2.523339033126831\n",
      "loss ->  2.321695327758789\n",
      "loss ->  2.057532548904419\n",
      "loss ->  2.7293143272399902\n",
      "loss ->  2.2536258697509766\n",
      "loss ->  2.384413719177246\n",
      "loss ->  2.3841006755828857\n",
      "loss ->  2.2713711261749268\n",
      "loss ->  2.208019256591797\n",
      "loss ->  2.2565841674804688\n",
      "loss ->  2.4325356483459473\n",
      "loss ->  2.8386993408203125\n",
      "loss ->  1.8661527633666992\n",
      "loss ->  2.2084052562713623\n",
      "loss ->  2.1055679321289062\n",
      "loss ->  2.381723403930664\n",
      "loss ->  2.2409439086914062\n",
      "loss ->  2.2786452770233154\n",
      "loss ->  2.1846556663513184\n",
      "loss ->  1.9464234113693237\n",
      "loss ->  1.8939043283462524\n",
      "loss ->  2.3892505168914795\n",
      "loss ->  2.141073703765869\n",
      "loss ->  2.349623680114746\n",
      "loss ->  2.0650272369384766\n",
      "loss ->  2.0656185150146484\n",
      "loss ->  2.2833783626556396\n",
      "loss ->  2.3273632526397705\n",
      "loss ->  2.4413838386535645\n",
      "loss ->  2.0750784873962402\n",
      "loss ->  1.8021628856658936\n",
      "loss ->  2.2015020847320557\n",
      "loss ->  1.975887656211853\n",
      "loss ->  2.1302058696746826\n",
      "loss ->  2.1550216674804688\n",
      "loss ->  2.2540395259857178\n",
      "loss ->  2.0602636337280273\n",
      "loss ->  2.3149399757385254\n",
      "loss ->  2.1985111236572266\n",
      "loss ->  2.5198535919189453\n",
      "loss ->  2.593228816986084\n",
      "loss ->  2.7542803287506104\n",
      "loss ->  2.20676326751709\n",
      "loss ->  1.964085340499878\n",
      "loss ->  2.006342649459839\n",
      "loss ->  2.28137469291687\n",
      "loss ->  2.3803648948669434\n",
      "loss ->  1.925527572631836\n",
      "loss ->  2.1693389415740967\n",
      "loss ->  2.086179494857788\n",
      "loss ->  2.0447604656219482\n",
      "loss ->  1.8888221979141235\n",
      "loss ->  2.0649261474609375\n",
      "loss ->  2.3078572750091553\n",
      "loss ->  2.387385368347168\n",
      "loss ->  2.0029163360595703\n",
      "loss ->  2.1607282161712646\n",
      "loss ->  2.112348794937134\n",
      "loss ->  2.6690738201141357\n",
      "loss ->  2.13800048828125\n",
      "loss ->  2.3015100955963135\n",
      "loss ->  2.0665688514709473\n",
      "loss ->  2.1513519287109375\n",
      "loss ->  2.0823118686676025\n",
      "loss ->  2.578002452850342\n",
      "loss ->  2.389392137527466\n",
      "loss ->  2.0417215824127197\n",
      "loss ->  1.977142095565796\n",
      "loss ->  2.3416874408721924\n",
      "loss ->  2.145031690597534\n",
      "loss ->  1.9102749824523926\n",
      "loss ->  2.217933416366577\n",
      "loss ->  2.186006784439087\n",
      "loss ->  1.9945282936096191\n",
      "loss ->  2.456665515899658\n",
      "loss ->  2.2692484855651855\n",
      "loss ->  2.064972400665283\n",
      "loss ->  2.6183488368988037\n",
      "loss ->  2.1866514682769775\n",
      "loss ->  2.339751720428467\n",
      "loss ->  1.9712638854980469\n",
      "loss ->  2.041104555130005\n",
      "loss ->  2.105499029159546\n",
      "loss ->  2.242332935333252\n",
      "loss ->  2.217013120651245\n",
      "loss ->  2.023399591445923\n",
      "loss ->  2.146754026412964\n",
      "loss ->  1.9805479049682617\n",
      "loss ->  2.3172388076782227\n",
      "loss ->  2.2006382942199707\n",
      "loss ->  2.1510252952575684\n",
      "loss ->  2.0374484062194824\n",
      "loss ->  2.256312370300293\n",
      "loss ->  2.3959262371063232\n",
      "loss ->  2.119377374649048\n",
      "loss ->  2.3395071029663086\n",
      "loss ->  2.0850613117218018\n",
      "loss ->  2.251786708831787\n",
      "loss ->  2.8385884761810303\n",
      "loss ->  2.291532039642334\n",
      "loss ->  2.448215961456299\n",
      "loss ->  2.2623846530914307\n",
      "loss ->  2.487738847732544\n",
      "loss ->  2.1377406120300293\n",
      "loss ->  2.2634551525115967\n",
      "loss ->  2.300931215286255\n",
      "loss ->  2.3964178562164307\n",
      "loss ->  2.1043968200683594\n",
      "loss ->  2.2078139781951904\n",
      "loss ->  2.678173780441284\n",
      "loss ->  1.9984638690948486\n",
      "loss ->  2.3023757934570312\n",
      "loss ->  2.22855806350708\n",
      "loss ->  2.320772171020508\n",
      "loss ->  2.457869052886963\n",
      "loss ->  2.262122392654419\n",
      "loss ->  2.236307382583618\n",
      "loss ->  1.8303972482681274\n",
      "loss ->  2.3202600479125977\n",
      "loss ->  2.268388509750366\n",
      "loss ->  2.407315731048584\n",
      "loss ->  2.7971839904785156\n",
      "loss ->  2.2102558612823486\n",
      "loss ->  2.544964075088501\n",
      "loss ->  2.5650033950805664\n",
      "loss ->  2.603963851928711\n",
      "loss ->  2.2013304233551025\n",
      "loss ->  2.2454187870025635\n",
      "loss ->  2.3394784927368164\n",
      "loss ->  2.187192440032959\n",
      "loss ->  2.381772518157959\n",
      "loss ->  2.269688129425049\n",
      "loss ->  1.6280407905578613\n",
      "loss ->  2.091974973678589\n",
      "loss ->  2.263787269592285\n",
      "loss ->  2.6473259925842285\n",
      "loss ->  2.4105570316314697\n",
      "loss ->  2.103377103805542\n",
      "loss ->  2.275311231613159\n",
      "loss ->  2.2644224166870117\n",
      "loss ->  1.8681389093399048\n",
      "loss ->  2.179594039916992\n",
      "loss ->  2.6626698970794678\n",
      "loss ->  2.1048779487609863\n",
      "loss ->  2.4348530769348145\n",
      "loss ->  2.115185499191284\n",
      "loss ->  2.1660308837890625\n",
      "loss ->  2.1174826622009277\n",
      "loss ->  2.113349199295044\n",
      "loss ->  2.3367760181427\n",
      "loss ->  2.2578256130218506\n",
      "loss ->  2.2880003452301025\n",
      "loss ->  2.5818240642547607\n",
      "loss ->  2.153961181640625\n",
      "loss ->  2.1317172050476074\n",
      "loss ->  2.0554001331329346\n",
      "loss ->  1.7771673202514648\n",
      "loss ->  2.0585880279541016\n",
      "loss ->  2.190671443939209\n",
      "loss ->  2.1819896697998047\n",
      "loss ->  2.329652786254883\n",
      "loss ->  2.0933566093444824\n",
      "loss ->  2.190802812576294\n",
      "loss ->  2.180433750152588\n",
      "loss ->  2.3128416538238525\n",
      "loss ->  2.3570146560668945\n",
      "loss ->  1.867213487625122\n",
      "loss ->  2.1165566444396973\n",
      "loss ->  2.104471445083618\n",
      "loss ->  2.3411521911621094\n",
      "loss ->  1.8607079982757568\n",
      "loss ->  1.9704736471176147\n",
      "loss ->  2.1220009326934814\n",
      "loss ->  2.285536766052246\n",
      "loss ->  2.3522493839263916\n",
      "loss ->  2.0693612098693848\n",
      "loss ->  2.4711806774139404\n",
      "loss ->  2.2824177742004395\n",
      "loss ->  2.1101114749908447\n",
      "loss ->  2.36657452583313\n",
      "loss ->  2.0656909942626953\n",
      "loss ->  2.3350651264190674\n",
      "loss ->  2.444281816482544\n",
      "loss ->  1.9831417798995972\n",
      "loss ->  1.9750996828079224\n",
      "loss ->  2.4303343296051025\n",
      "loss ->  2.1373062133789062\n",
      "loss ->  2.1096322536468506\n",
      "loss ->  2.0044689178466797\n",
      "loss ->  1.8041082620620728\n",
      "loss ->  2.1544361114501953\n",
      "loss ->  2.054990768432617\n",
      "loss ->  1.5341901779174805\n",
      "loss ->  2.191917896270752\n",
      "loss ->  2.6376800537109375\n",
      "loss ->  2.2113263607025146\n",
      "loss ->  2.483499050140381\n",
      "loss ->  1.7506422996520996\n",
      "loss ->  1.9253853559494019\n",
      "loss ->  2.3327059745788574\n",
      "loss ->  2.2547171115875244\n",
      "loss ->  2.2940430641174316\n",
      "loss ->  2.1775052547454834\n",
      "loss ->  2.320375442504883\n",
      "loss ->  1.911961317062378\n",
      "loss ->  2.0031909942626953\n",
      "loss ->  2.1843903064727783\n",
      "loss ->  2.286087989807129\n",
      "loss ->  2.1482081413269043\n",
      "loss ->  2.008949041366577\n",
      "loss ->  2.275700569152832\n",
      "loss ->  2.537970781326294\n",
      "loss ->  2.2027053833007812\n",
      "loss ->  2.1159627437591553\n",
      "loss ->  2.5617072582244873\n",
      "loss ->  2.0996007919311523\n",
      "loss ->  2.3205015659332275\n",
      "loss ->  2.3011281490325928\n",
      "loss ->  2.216592311859131\n",
      "loss ->  2.162881374359131\n",
      "loss ->  2.204822301864624\n",
      "loss ->  1.8118085861206055\n",
      "loss ->  2.0494680404663086\n",
      "loss ->  2.0322978496551514\n",
      "loss ->  2.095050573348999\n",
      "loss ->  2.168212413787842\n",
      "loss ->  1.781205654144287\n",
      "loss ->  2.0812978744506836\n",
      "loss ->  2.370192289352417\n",
      "loss ->  2.506523609161377\n",
      "loss ->  2.369290351867676\n",
      "loss ->  2.024465799331665\n",
      "loss ->  1.6734228134155273\n",
      "loss ->  2.0209572315216064\n",
      "loss ->  2.2183194160461426\n",
      "loss ->  2.2881174087524414\n",
      "loss ->  2.32045841217041\n",
      "loss ->  2.017211675643921\n",
      "loss ->  2.5002951622009277\n",
      "loss ->  2.4040114879608154\n",
      "loss ->  2.374218463897705\n",
      "loss ->  2.479177236557007\n",
      "loss ->  2.234135389328003\n",
      "loss ->  2.1599295139312744\n",
      "loss ->  1.995849370956421\n",
      "loss ->  2.1062276363372803\n",
      "loss ->  2.9215075969696045\n",
      "loss ->  2.3549964427948\n",
      "loss ->  2.306729793548584\n",
      "loss ->  2.2377758026123047\n",
      "loss ->  2.299341917037964\n",
      "loss ->  2.1722679138183594\n",
      "loss ->  2.3709378242492676\n",
      "loss ->  2.1963510513305664\n",
      "loss ->  2.2129836082458496\n",
      "loss ->  2.069824695587158\n",
      "loss ->  2.3076117038726807\n",
      "loss ->  1.8467707633972168\n",
      "loss ->  2.2327542304992676\n",
      "loss ->  2.1700944900512695\n",
      "loss ->  2.1362736225128174\n",
      "loss ->  2.2756943702697754\n",
      "loss ->  2.594559669494629\n",
      "loss ->  2.390878915786743\n",
      "loss ->  2.148402214050293\n",
      "loss ->  2.280318021774292\n",
      "loss ->  2.2751080989837646\n",
      "loss ->  2.634483814239502\n",
      "loss ->  2.201749563217163\n",
      "loss ->  1.8787473440170288\n",
      "loss ->  2.1359610557556152\n",
      "loss ->  2.2071642875671387\n",
      "loss ->  2.166931629180908\n",
      "loss ->  2.36883544921875\n",
      "loss ->  2.162565231323242\n",
      "loss ->  2.205256700515747\n",
      "loss ->  2.0528905391693115\n",
      "loss ->  1.7928950786590576\n",
      "loss ->  2.182473659515381\n",
      "loss ->  1.8580266237258911\n",
      "loss ->  2.480508804321289\n",
      "loss ->  2.049039125442505\n",
      "loss ->  1.9743239879608154\n",
      "loss ->  2.193093776702881\n",
      "loss ->  2.14896297454834\n",
      "loss ->  2.2178196907043457\n",
      "loss ->  2.162167549133301\n",
      "loss ->  2.107956886291504\n",
      "loss ->  2.4871912002563477\n",
      "loss ->  2.5531821250915527\n",
      "loss ->  1.9336638450622559\n",
      "loss ->  2.6247975826263428\n",
      "loss ->  2.4084103107452393\n",
      "loss ->  2.139263153076172\n",
      "loss ->  2.340557336807251\n",
      "loss ->  2.1218984127044678\n",
      "loss ->  1.7815194129943848\n",
      "loss ->  2.0741662979125977\n",
      "loss ->  2.3212571144104004\n",
      "loss ->  2.103733777999878\n",
      "loss ->  1.9019525051116943\n",
      "loss ->  2.5208184719085693\n",
      "loss ->  2.4769232273101807\n",
      "loss ->  2.3643250465393066\n",
      "loss ->  2.254206657409668\n",
      "loss ->  2.4124035835266113\n",
      "loss ->  2.2154195308685303\n",
      "loss ->  2.2290289402008057\n",
      "loss ->  2.4783198833465576\n",
      "loss ->  2.13323712348938\n",
      "loss ->  2.079242467880249\n",
      "loss ->  1.9775469303131104\n",
      "loss ->  2.2068424224853516\n",
      "loss ->  2.493565320968628\n",
      "loss ->  2.3930346965789795\n",
      "loss ->  2.191279649734497\n",
      "loss ->  2.0630619525909424\n",
      "loss ->  2.3581597805023193\n",
      "loss ->  2.8716580867767334\n",
      "loss ->  1.9999597072601318\n",
      "loss ->  2.195981740951538\n",
      "loss ->  2.4563076496124268\n",
      "loss ->  2.3248181343078613\n",
      "loss ->  2.17368745803833\n",
      "loss ->  1.9282352924346924\n",
      "loss ->  2.1452479362487793\n",
      "loss ->  2.451263904571533\n",
      "loss ->  2.819321393966675\n",
      "loss ->  2.476766347885132\n",
      "loss ->  1.8205831050872803\n",
      "loss ->  2.140413999557495\n",
      "loss ->  2.255988597869873\n",
      "loss ->  1.8732972145080566\n",
      "loss ->  2.6805405616760254\n",
      "loss ->  2.355273485183716\n",
      "loss ->  2.041965961456299\n",
      "loss ->  2.1312973499298096\n",
      "loss ->  2.191244125366211\n",
      "loss ->  2.11110520362854\n",
      "loss ->  1.9606566429138184\n",
      "loss ->  2.238201141357422\n",
      "loss ->  1.863490104675293\n",
      "loss ->  1.9865140914916992\n",
      "loss ->  1.9623833894729614\n",
      "loss ->  2.1720645427703857\n",
      "loss ->  2.03018856048584\n",
      "loss ->  1.9256443977355957\n",
      "loss ->  2.269798517227173\n",
      "loss ->  2.5320539474487305\n",
      "loss ->  2.0043742656707764\n",
      "loss ->  2.060476064682007\n",
      "loss ->  1.944606065750122\n",
      "loss ->  2.37591814994812\n",
      "loss ->  2.0342464447021484\n",
      "loss ->  2.477217674255371\n",
      "loss ->  2.421233654022217\n",
      "loss ->  2.3515024185180664\n",
      "loss ->  2.719083786010742\n",
      "loss ->  2.2561047077178955\n",
      "loss ->  2.5520217418670654\n",
      "loss ->  1.8452279567718506\n",
      "loss ->  2.011857509613037\n",
      "loss ->  1.9215975999832153\n",
      "loss ->  2.170308828353882\n",
      "loss ->  2.0148115158081055\n",
      "loss ->  2.092222213745117\n",
      "loss ->  2.2442517280578613\n",
      "loss ->  2.066636323928833\n",
      "loss ->  2.048593759536743\n",
      "loss ->  2.0936801433563232\n",
      "loss ->  2.244741678237915\n",
      "loss ->  2.554741144180298\n",
      "loss ->  2.3165154457092285\n",
      "loss ->  2.1062541007995605\n",
      "loss ->  2.4560227394104004\n",
      "loss ->  2.1782617568969727\n",
      "loss ->  2.534752130508423\n",
      "loss ->  2.2338449954986572\n",
      "loss ->  1.7798409461975098\n",
      "loss ->  2.245727062225342\n",
      "loss ->  1.9094847440719604\n",
      "loss ->  2.4772987365722656\n",
      "loss ->  2.3541972637176514\n",
      "loss ->  2.2624287605285645\n",
      "loss ->  1.6548981666564941\n",
      "loss ->  2.3377811908721924\n",
      "loss ->  2.1263177394866943\n",
      "loss ->  2.4696567058563232\n",
      "loss ->  2.296560764312744\n",
      "loss ->  2.1296939849853516\n",
      "loss ->  2.696660280227661\n",
      "loss ->  2.1918866634368896\n",
      "loss ->  1.9318575859069824\n",
      "loss ->  2.3141067028045654\n",
      "loss ->  2.32765793800354\n",
      "loss ->  2.342902421951294\n",
      "loss ->  2.859423875808716\n",
      "loss ->  2.3613767623901367\n",
      "loss ->  2.025503158569336\n",
      "loss ->  2.415475845336914\n",
      "loss ->  2.2429890632629395\n",
      "loss ->  2.340432643890381\n",
      "loss ->  2.6169979572296143\n",
      "loss ->  2.699983596801758\n",
      "loss ->  2.169067621231079\n",
      "loss ->  1.8705642223358154\n",
      "loss ->  2.1636857986450195\n",
      "loss ->  1.7865397930145264\n",
      "loss ->  2.3939578533172607\n",
      "loss ->  1.807344675064087\n",
      "loss ->  2.07785701751709\n",
      "loss ->  1.971902847290039\n",
      "loss ->  2.1718788146972656\n",
      "loss ->  2.216747522354126\n",
      "loss ->  2.126905679702759\n",
      "loss ->  2.3867053985595703\n",
      "loss ->  2.1993231773376465\n",
      "loss ->  2.609076738357544\n",
      "loss ->  2.033522844314575\n",
      "loss ->  1.9686115980148315\n",
      "loss ->  1.9150006771087646\n",
      "loss ->  2.3021085262298584\n",
      "loss ->  2.246805429458618\n",
      "loss ->  2.2683377265930176\n",
      "loss ->  2.208509922027588\n",
      "loss ->  2.5502710342407227\n",
      "loss ->  2.1395440101623535\n",
      "loss ->  2.2449686527252197\n",
      "loss ->  2.287367582321167\n",
      "loss ->  2.4535164833068848\n",
      "loss ->  2.0336663722991943\n",
      "loss ->  2.617933750152588\n",
      "loss ->  2.198181390762329\n",
      "loss ->  1.9970893859863281\n",
      "loss ->  2.231093168258667\n",
      "loss ->  2.7280426025390625\n",
      "loss ->  1.87772536277771\n",
      "loss ->  1.9463087320327759\n",
      "loss ->  2.09527850151062\n",
      "loss ->  3.036982536315918\n",
      "loss ->  2.176652669906616\n",
      "loss ->  2.0624866485595703\n",
      "loss ->  2.518322706222534\n",
      "loss ->  2.528181314468384\n",
      "loss ->  2.2723188400268555\n",
      "loss ->  2.292649745941162\n",
      "loss ->  2.1795146465301514\n",
      "loss ->  2.249110698699951\n",
      "loss ->  2.057671546936035\n",
      "loss ->  2.5391557216644287\n",
      "loss ->  2.33564829826355\n",
      "loss ->  2.1808409690856934\n",
      "loss ->  2.0607635974884033\n",
      "loss ->  2.620025157928467\n",
      "loss ->  2.155428647994995\n",
      "loss ->  2.4163553714752197\n",
      "loss ->  2.026294469833374\n",
      "loss ->  2.3581125736236572\n",
      "loss ->  2.5142407417297363\n",
      "loss ->  1.8888176679611206\n",
      "loss ->  2.3372652530670166\n",
      "loss ->  2.4920711517333984\n",
      "loss ->  2.117049217224121\n",
      "loss ->  2.2680771350860596\n",
      "loss ->  2.4155285358428955\n",
      "loss ->  2.15916109085083\n",
      "loss ->  2.0278050899505615\n",
      "loss ->  1.971203327178955\n",
      "loss ->  2.157759189605713\n",
      "loss ->  2.0789506435394287\n",
      "loss ->  2.7799603939056396\n",
      "loss ->  2.0465011596679688\n",
      "loss ->  2.359477996826172\n",
      "loss ->  2.1423346996307373\n",
      "loss ->  2.196098566055298\n",
      "loss ->  2.309539318084717\n",
      "loss ->  2.1733856201171875\n",
      "loss ->  2.1107611656188965\n",
      "loss ->  2.2871651649475098\n",
      "loss ->  2.769951105117798\n",
      "loss ->  2.256760358810425\n",
      "loss ->  2.194110155105591\n",
      "loss ->  2.1912648677825928\n",
      "loss ->  2.141146659851074\n",
      "loss ->  2.368720769882202\n",
      "loss ->  2.4514904022216797\n",
      "loss ->  2.4111242294311523\n",
      "loss ->  2.5747668743133545\n",
      "loss ->  2.6545863151550293\n",
      "loss ->  2.2843713760375977\n",
      "loss ->  2.5169637203216553\n",
      "loss ->  2.372462034225464\n",
      "loss ->  2.3255884647369385\n",
      "loss ->  2.4866693019866943\n",
      "loss ->  2.205345630645752\n",
      "loss ->  2.013874053955078\n",
      "loss ->  2.0006000995635986\n",
      "loss ->  2.363704204559326\n",
      "loss ->  2.348674774169922\n",
      "loss ->  2.1134676933288574\n",
      "loss ->  2.2428603172302246\n",
      "loss ->  2.3558011054992676\n",
      "loss ->  2.236210584640503\n",
      "loss ->  2.256099224090576\n",
      "loss ->  2.4143500328063965\n",
      "loss ->  2.1758952140808105\n",
      "loss ->  1.9583287239074707\n",
      "loss ->  2.6583590507507324\n",
      "loss ->  2.2353832721710205\n",
      "loss ->  2.4209749698638916\n",
      "loss ->  2.157228946685791\n",
      "loss ->  2.1084156036376953\n",
      "loss ->  2.447906970977783\n",
      "loss ->  2.203178644180298\n",
      "loss ->  2.3796310424804688\n",
      "loss ->  2.275420665740967\n",
      "loss ->  2.0561294555664062\n",
      "loss ->  2.5380876064300537\n",
      "loss ->  2.4767022132873535\n",
      "loss ->  2.4646666049957275\n",
      "loss ->  2.264370918273926\n",
      "loss ->  2.5052671432495117\n",
      "loss ->  2.1829919815063477\n",
      "loss ->  2.2831931114196777\n",
      "loss ->  2.486391067504883\n",
      "loss ->  2.171103000640869\n",
      "loss ->  2.2427897453308105\n",
      "loss ->  2.74898099899292\n",
      "loss ->  2.486990213394165\n",
      "loss ->  2.642106533050537\n",
      "loss ->  2.436876058578491\n",
      "loss ->  2.6357200145721436\n",
      "loss ->  1.9744324684143066\n",
      "loss ->  2.353139877319336\n",
      "loss ->  2.2129087448120117\n",
      "loss ->  2.507063388824463\n",
      "loss ->  2.7564001083374023\n",
      "loss ->  2.2318551540374756\n",
      "loss ->  2.3341708183288574\n",
      "loss ->  1.869185209274292\n",
      "loss ->  2.3964877128601074\n",
      "loss ->  2.1605348587036133\n",
      "loss ->  2.034325361251831\n",
      "loss ->  2.2426278591156006\n",
      "loss ->  2.090815782546997\n",
      "loss ->  2.2454724311828613\n",
      "loss ->  2.578200578689575\n",
      "loss ->  2.116523027420044\n",
      "loss ->  2.127056121826172\n",
      "loss ->  2.277745246887207\n",
      "loss ->  2.774120330810547\n",
      "loss ->  2.149531602859497\n",
      "loss ->  2.138434648513794\n",
      "loss ->  2.4044413566589355\n",
      "loss ->  2.589954137802124\n",
      "loss ->  2.6254186630249023\n",
      "loss ->  2.287681818008423\n",
      "loss ->  1.908499836921692\n",
      "loss ->  2.2133116722106934\n",
      "loss ->  1.9248332977294922\n",
      "loss ->  2.2118427753448486\n",
      "loss ->  2.392296314239502\n",
      "loss ->  2.3461341857910156\n",
      "loss ->  2.1676394939422607\n",
      "loss ->  2.416872024536133\n",
      "loss ->  2.38295578956604\n",
      "loss ->  2.2654805183410645\n",
      "loss ->  2.4800031185150146\n",
      "loss ->  2.593386650085449\n",
      "loss ->  2.4022586345672607\n",
      "loss ->  2.641544818878174\n",
      "loss ->  2.5848536491394043\n",
      "loss ->  2.871591567993164\n",
      "loss ->  2.284468650817871\n",
      "loss ->  2.392693519592285\n",
      "loss ->  2.3952651023864746\n",
      "loss ->  2.435706853866577\n",
      "loss ->  2.2856523990631104\n",
      "loss ->  2.372668504714966\n",
      "loss ->  2.2956860065460205\n",
      "loss ->  2.68769907951355\n",
      "loss ->  2.6729066371917725\n",
      "loss ->  2.5095431804656982\n",
      "loss ->  2.5480968952178955\n",
      "loss ->  2.2140920162200928\n",
      "loss ->  2.5769407749176025\n",
      "loss ->  2.2936792373657227\n",
      "loss ->  1.863577961921692\n",
      "loss ->  2.964151620864868\n",
      "loss ->  2.3122940063476562\n",
      "loss ->  2.351106882095337\n",
      "loss ->  2.6012959480285645\n",
      "loss ->  2.592390537261963\n",
      "loss ->  2.4241652488708496\n",
      "loss ->  2.3079144954681396\n",
      "loss ->  2.6776671409606934\n",
      "loss ->  2.285778284072876\n",
      "loss ->  2.707049608230591\n",
      "loss ->  2.6919031143188477\n",
      "loss ->  2.5972821712493896\n",
      "loss ->  2.3363022804260254\n",
      "loss ->  2.501572847366333\n",
      "loss ->  2.2160043716430664\n",
      "loss ->  2.0173985958099365\n",
      "loss ->  2.159189462661743\n",
      "loss ->  2.0037004947662354\n",
      "loss ->  2.5844435691833496\n",
      "loss ->  2.5314512252807617\n",
      "loss ->  2.171713352203369\n",
      "loss ->  2.836453914642334\n",
      "loss ->  2.5013656616210938\n",
      "loss ->  2.3566067218780518\n",
      "loss ->  2.3522775173187256\n",
      "loss ->  2.8108084201812744\n",
      "loss ->  2.2069272994995117\n",
      "loss ->  2.5418853759765625\n",
      "loss ->  2.575061082839966\n",
      "loss ->  2.0870230197906494\n",
      "loss ->  2.1742913722991943\n",
      "loss ->  2.7097079753875732\n",
      "loss ->  2.906149387359619\n",
      "loss ->  2.8437259197235107\n",
      "loss ->  2.2793450355529785\n",
      "loss ->  2.7892098426818848\n",
      "loss ->  2.6648104190826416\n",
      "loss ->  2.1842031478881836\n",
      "loss ->  2.201387882232666\n",
      "loss ->  2.674668788909912\n",
      "loss ->  2.0869688987731934\n",
      "loss ->  2.727083683013916\n",
      "loss ->  2.7869911193847656\n",
      "loss ->  2.5690054893493652\n",
      "loss ->  2.314312696456909\n",
      "loss ->  2.5777266025543213\n",
      "loss ->  2.2180070877075195\n",
      "loss ->  2.6478447914123535\n",
      "loss ->  2.7273921966552734\n",
      "loss ->  2.1627156734466553\n",
      "loss ->  2.688610315322876\n",
      "loss ->  2.604763984680176\n",
      "loss ->  2.4697463512420654\n",
      "loss ->  3.0759661197662354\n",
      "loss ->  2.3421120643615723\n",
      "loss ->  2.6608223915100098\n",
      "loss ->  2.420253276824951\n",
      "loss ->  2.554320812225342\n",
      "loss ->  2.6769773960113525\n",
      "loss ->  2.309581756591797\n",
      "loss ->  2.4097232818603516\n",
      "loss ->  2.6710867881774902\n",
      "loss ->  2.486708402633667\n",
      "loss ->  2.644110918045044\n",
      "loss ->  2.2513206005096436\n",
      "loss ->  2.5296595096588135\n",
      "loss ->  2.069018840789795\n",
      "loss ->  2.560084819793701\n",
      "loss ->  2.499539852142334\n",
      "loss ->  1.8891841173171997\n",
      "loss ->  2.59513521194458\n",
      "loss ->  2.0724315643310547\n",
      "loss ->  2.320296287536621\n",
      "loss ->  1.9554624557495117\n",
      "loss ->  2.803359270095825\n",
      "loss ->  2.798705577850342\n",
      "loss ->  2.327664852142334\n",
      "loss ->  2.222026824951172\n",
      "loss ->  2.9294943809509277\n",
      "loss ->  2.8287858963012695\n",
      "loss ->  2.580934524536133\n",
      "loss ->  2.3993725776672363\n",
      "loss ->  2.179469585418701\n",
      "loss ->  2.385972261428833\n",
      "loss ->  2.6068222522735596\n",
      "loss ->  3.2777674198150635\n",
      "loss ->  2.1473686695098877\n",
      "loss ->  2.931737184524536\n",
      "loss ->  2.6821556091308594\n",
      "loss ->  2.733855724334717\n",
      "loss ->  2.4597127437591553\n",
      "loss ->  2.5060107707977295\n",
      "loss ->  2.4230265617370605\n",
      "loss ->  2.943105936050415\n",
      "loss ->  3.215481758117676\n",
      "loss ->  2.704972267150879\n",
      "loss ->  2.7687642574310303\n",
      "loss ->  2.589259624481201\n",
      "loss ->  2.3764209747314453\n",
      "loss ->  2.381862163543701\n",
      "loss ->  2.3650927543640137\n",
      "loss ->  2.8847639560699463\n",
      "loss ->  3.217731237411499\n",
      "loss ->  3.11220383644104\n",
      "loss ->  3.0948455333709717\n",
      "loss ->  2.938462495803833\n",
      "loss ->  2.9194552898406982\n",
      "loss ->  3.5794928073883057\n",
      "loss ->  2.920747756958008\n",
      "loss ->  3.23681378364563\n",
      "loss ->  2.3530099391937256\n",
      "loss ->  2.6010348796844482\n",
      "loss ->  2.7787630558013916\n",
      "loss ->  2.5581018924713135\n",
      "loss ->  3.109177827835083\n",
      "loss ->  2.854604959487915\n",
      "loss ->  3.032884359359741\n",
      "loss ->  2.7611172199249268\n",
      "loss ->  2.9932444095611572\n",
      "loss ->  3.306326150894165\n",
      "loss ->  2.611215829849243\n",
      "loss ->  2.8416037559509277\n",
      "loss ->  2.720734119415283\n",
      "loss ->  2.570624828338623\n",
      "loss ->  3.266300916671753\n",
      "loss ->  2.4450573921203613\n",
      "loss ->  2.4830667972564697\n",
      "loss ->  2.8511404991149902\n",
      "loss ->  3.292229652404785\n",
      "loss ->  2.91721773147583\n",
      "loss ->  3.2491860389709473\n",
      "loss ->  3.763362407684326\n",
      "loss ->  3.3721120357513428\n",
      "loss ->  3.41288161277771\n",
      "loss ->  3.34539794921875\n",
      "loss ->  3.286129951477051\n",
      "loss ->  2.5574588775634766\n",
      "loss ->  2.4851207733154297\n",
      "loss ->  3.5084712505340576\n",
      "loss ->  3.2424867153167725\n",
      "loss ->  3.6242058277130127\n",
      "loss ->  3.9044084548950195\n",
      "loss ->  3.6532633304595947\n",
      "loss ->  3.5712480545043945\n",
      "loss ->  3.2855923175811768\n",
      "loss ->  3.2176051139831543\n",
      "loss ->  2.550347089767456\n",
      "loss ->  3.097764015197754\n",
      "loss ->  2.5133144855499268\n",
      "loss ->  3.751800775527954\n",
      "loss ->  3.578003406524658\n",
      "loss ->  3.4884002208709717\n",
      "loss ->  3.955937385559082\n",
      "loss ->  4.368524074554443\n",
      "loss ->  4.700832366943359\n",
      "loss ->  3.6191723346710205\n",
      "loss ->  4.054832458496094\n",
      "loss ->  3.08304762840271\n",
      "loss ->  3.5434460639953613\n",
      "loss ->  3.3447418212890625\n",
      "loss ->  3.67344331741333\n",
      "loss ->  3.3890633583068848\n",
      "loss ->  4.027383327484131\n",
      "loss ->  5.631860256195068\n",
      "loss ->  4.000256538391113\n",
      "loss ->  3.690200090408325\n",
      "loss ->  3.0829317569732666\n",
      "loss ->  4.334791660308838\n",
      "loss ->  3.3656694889068604\n",
      "loss ->  3.9509775638580322\n",
      "loss ->  4.1104044914245605\n",
      "loss ->  3.030792713165283\n",
      "loss ->  2.5194783210754395\n",
      "loss ->  4.442095756530762\n",
      "loss ->  5.438503742218018\n",
      "loss ->  3.2157342433929443\n",
      "loss ->  4.460186958312988\n",
      "loss ->  4.024781227111816\n",
      "loss ->  3.084798574447632\n",
      "loss ->  3.6677911281585693\n",
      "loss ->  3.4482967853546143\n",
      "loss ->  3.831866502761841\n",
      "loss ->  4.825303554534912\n",
      "loss ->  4.508642673492432\n",
      "loss ->  3.8020222187042236\n",
      "loss ->  3.4484822750091553\n",
      "loss ->  4.150307655334473\n",
      "loss ->  4.13936710357666\n",
      "loss ->  4.880756378173828\n",
      "loss ->  8.468413352966309\n",
      "loss ->  4.553840637207031\n",
      "loss ->  4.826687335968018\n",
      "loss ->  4.30883264541626\n",
      "loss ->  6.843733310699463\n",
      "loss ->  4.996654510498047\n",
      "loss ->  5.063161849975586\n",
      "loss ->  4.3394012451171875\n",
      "loss ->  6.842232704162598\n",
      "loss ->  6.223195552825928\n",
      "loss ->  7.042021751403809\n",
      "loss ->  5.593358039855957\n",
      "loss ->  4.785949230194092\n",
      "loss ->  5.853872299194336\n",
      "loss ->  5.146058559417725\n",
      "loss ->  5.8263468742370605\n",
      "loss ->  6.158658504486084\n",
      "loss ->  5.3669843673706055\n",
      "loss ->  5.044759750366211\n",
      "loss ->  4.6250224113464355\n",
      "loss ->  4.097059726715088\n",
      "loss ->  4.757780075073242\n",
      "loss ->  6.151412010192871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff185546d00>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# track the learning rates in respect to its loss\n",
    "lr_i = []\n",
    "loss_i = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data.add_(-lr * p.grad)\n",
    "\n",
    "    lr_i.append(lr)\n",
    "    loss_i.append(loss.item())\n",
    "\n",
    "plt.plot(lr_i, loss_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the learning rate was stable between 0.0 and 0.1, and after that it became unstable. So we have narrowed down the learning rate's range in respect to minimized loss function. So we can choose 0.1 safely now and also increase the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 \n",
    "    loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "    # print('loss -> ', loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data.add_(-0.1 * p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.355053186416626\n"
     ]
    }
   ],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print('loss -> ', loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Decay\n",
    "\n",
    "One more thing to remember is that, once we found our optimized learning rate, we can run the training with it a few times. But then you realize the loss is not moving lower by much. In that case, on a trained NN, you can further reduce the learning rate, by a factor of 10, and continue thee training with same amount of iterations. You can continue this until you reach a plateau. This approach is called learning decay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split - Train/Validate/Test\n",
    "\n",
    "If you increase your parameters to exceed your input size, you will end up over-fitting the data. Meaning that, your NN will memorize the data and outputs exactly what it saw, rather then being creative and output something new. \n",
    "\n",
    "To make sure we are not over-fitting or under-fitting we can split the data into training (80%), validation/dev (10%), and testing sets (10%). \n",
    "\n",
    "Validation is used to find the best hyper-params and settings of the NN. You can try multiple variations to evaluate which one is best for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182431, 3]) torch.Size([182431])\n",
      "torch.Size([22818, 3]) torch.Size([22818])\n",
      "torch.Size([22897, 3]) torch.Size([22897])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3 \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "# split the dataset randomly for train, dev and test\n",
    "import random\n",
    "random.seed\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words)) # 80% of the words\n",
    "n2 = int(0.9 * len(words)) # 90% of the words\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train on 80% of the words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # validate on 10% of the words\n",
    "Xtst, Ytst = build_dataset(words[n2:]) # test on 10% of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g, requires_grad=True)\n",
    "W1 = torch.randn((num_of_inputs, hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((hidden_layer_hyperparameter_size), generator=g, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_hyperparameter_size, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((27), generator=g, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(X,Y):\n",
    "    \n",
    "    for i in range(10000):\n",
    "\n",
    "        # mini-batch\n",
    "        ix = torch.randint(0, X.shape[0], (32,)) # 32 random integers between 0 and X.shape[0]. this will make the batch size 32\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[X[ix]] # instead of sending the entire X for mapping, we can only iterate over the batch size\n",
    "        h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1)\n",
    "        logits = h @ W2 + b2 \n",
    "        loss = F.cross_entropy(logits, Y[ix]) # labels are also only for the batch size too\n",
    "        # print('loss -> ', loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        for p in parameters:\n",
    "            p.data.add_(-0.1 * p.grad)\n",
    "\n",
    "def getLoss(X,Y):\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, num_of_inputs) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print('loss -> ', loss.item())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss ->  2.4326486587524414\n",
      "Training Loss:  None\n",
      "loss ->  2.443953037261963\n",
      "Training Loss:  None\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_NN(Xtr, Ytr)\n",
    "print('Training Loss: ', getLoss(Xtr, Ytr))\n",
    "# validate\n",
    "print('Training Loss: ', getLoss(Xdev, Ydev))\n",
    "# Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the loss of the training dataset and validation set are too different, that means that our NN hasn't learned much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
