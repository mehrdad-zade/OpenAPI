{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrej Karpathy's /nanoGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatgPT is another language model that completes a given sentence. The generative AI comes from \"Attention Is All You Need\" paper. \n",
    "\n",
    "In this module we'll build a char level language model based on the transformer's architecture. The dataset is the entire work of shakespeare. The model will generate, shakespeare-like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny-shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in char:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in char: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size:  65\n"
     ]
    }
   ],
   "source": [
    "# here is our vocabulary; all possible characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text\n",
    "s2i = {ch:i for i,ch in enumerate(chars)}\n",
    "i2s = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [s2i[ch] for ch in s] # encoder: take a string and return a list of integers\n",
    "decode = lambda x: ''.join([i2s[i] for i in x]) # decoder: take a list of integers and return a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are many tokenizing algorithms. Google uses ```SentencePiece``` which is a sub-word-unit tokenizer; it's not char level, nor word level.  OpenAi uses ```tiktoken```.\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.gen_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world')) == \"hello world\"\n",
    "\n",
    "with these tokenizers you won't get a long list of int. it helps to get a smaller list with larger word vocab sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split the data into train and validation sets to understand if our model is overfitting\n",
    "n = int(0.9*len(data)) # first 90% of the data\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training the NN we need to train on chunks of data, otherwise it's not optimal to feed in the entire data. that's why we use ```block_size```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think about the training process as follows: \n",
    "\n",
    "when a token of value '18' is provided, there is a high chance to generate '47'.\n",
    "\n",
    "when a token of value '18 47' is provided, there is a high chance to generate '56'.\n",
    "\n",
    "when a token of value '18 47 56' is provided, there is a high chance to generate '57'.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # this is for reproducibility to get the same results every time\n",
    "batch_size = 4 # how many independent sequences to process in parallel\n",
    "block_size = 8 # the max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random start indices for the examples\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # batch_size x block_size\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # batch_size x block_size\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets: ')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24], target is 43\n",
      "when input is [24, 43], target is 58\n",
      "when input is [24, 43, 58], target is 5\n",
      "when input is [24, 43, 58, 5], target is 57\n",
      "when input is [24, 43, 58, 5, 57], target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1], target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], target is 39\n",
      "when input is [44], target is 53\n",
      "when input is [44, 53], target is 56\n",
      "when input is [44, 53, 56], target is 1\n",
      "when input is [44, 53, 56, 1], target is 58\n",
      "when input is [44, 53, 56, 1, 58], target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46], target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], target is 1\n",
      "when input is [52], target is 58\n",
      "when input is [52, 58], target is 1\n",
      "when input is [52, 58, 1], target is 58\n",
      "when input is [52, 58, 1, 58], target is 46\n",
      "when input is [52, 58, 1, 58, 46], target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39], target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], target is 46\n",
      "when input is [25], target is 17\n",
      "when input is [25, 17], target is 27\n",
      "when input is [25, 17, 27], target is 10\n",
      "when input is [25, 17, 27, 10], target is 0\n",
      "when input is [25, 17, 27, 10, 0], target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21], target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], target is 39\n"
     ]
    }
   ],
   "source": [
    "# visualize the way the input and target are generated\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()}, target is {target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in bigram model for instance, the prediction was only based on the last charr. However, thee history or chars/tokens that came before also has a impact on choosing the next char. That's when transformer model comes into the picture.\n",
    "\n",
    "the easiest way to get info about the previous tokens is to sum their channel values and average it. obviously this will loose a lott of info on the spacial information about the tokens, but we'll address that later.\n",
    "\n",
    "the technique for averaging over previous tokens is called ```bag of words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider this toy example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: use for loops for averaging\n",
    "\n",
    "# we want x[b, t] = mean _ {i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C)) # x bag of words. a word is on each of the 8 locations and we are averaging\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, c)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be efficient about this averaging we can use matrix multiplications. we'll build a matrix 'a' such that, multiplying another matrix by it, would generate a new matrix where each row represents the average of elements in previous rows, column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3, 3)))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version2: use matrix multiplication\n",
    "\n",
    "wei = torch.tril(torch.ones((T, T))) # weight matrix\n",
    "weil = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = weil @ x\n",
    "\n",
    "#validate to make sure xbow and xbow2 are the same\n",
    "torch.allclose(xbow, xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: use softmax\n",
    "\n",
    "import torch.nn.functional as F\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # make all the zeros -inf\n",
    "wei = F.softmax(wei, 1) # exponentiate and divide by sum\n",
    "xbow3 = wei @ x\n",
    "\n",
    "# validate \n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1316, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1215, 0.0814, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1467, 0.0783, 0.2508, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1593, 0.0674, 0.3324, 0.5611, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1262, 0.1577, 0.1347, 0.1402, 0.2156, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0913, 0.1651, 0.0511, 0.0325, 0.2371, 0.2694, 0.0000, 0.0000],\n",
       "        [0.1351, 0.0838, 0.1934, 0.2463, 0.4575, 0.0160, 0.6153, 0.0000],\n",
       "        [0.0882, 0.3662, 0.0376, 0.0200, 0.0898, 0.7147, 0.3847, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key =   nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # transpose the last two columns. (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # make all the zeros -inf. if you are for instance trying to do sentiment analysis, you can remove this line so all the nodes get to talk to each other\n",
    "wei = F.softmax(wei, 1) # exponentiate and divide by sum\n",
    "out = wei @ x\n",
    "\n",
    "wei[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
